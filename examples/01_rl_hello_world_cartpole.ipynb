{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning \"Hello World\"\n",
    "\n",
    "## Copyright\n",
    "\n",
    "*Copyright Geoscience DS&ML Special Interest Group, 2022.*\n",
    "*License: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)*\n",
    "*Author(s): [Altay Sansal](https://github.com/tasansal)*\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook assumes familiarity with Reinforcement Learning concepts (agents, spaces, actions, ...) and knowledge of [OpenAI's Gym](https://www.gymlibrary.ml/) APIs and environments. If you are not familiar with RL or Gym at all, we refer the reader to the following resources:\n",
    "1. [OpenAI Gym Introduction](https://www.gymlibrary.ml/content/api/)\n",
    "2. [OpenAI Gym Introduction](https://www.gymlibrary.ml/content/api/)\n",
    "3. [OpenAI Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)\n",
    "    * [Part 1: Key Concepts in RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
    "    * [Part 2: Kinds of RL Algorithms](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)\n",
    "    * [Part 3: Intro to Policy Optimization](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)\n",
    "    * [Key Papers](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)\n",
    "    * [RL Algorithms](https://spinningup.openai.com/en/latest/user/algorithms.html)\n",
    "4. [David Silver's RL Course](https://www.davidsilver.uk/teaching/)\n",
    "5. [Berkeley's Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)\n",
    "6. [PyTorch DQN from Scratch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "7. [OpenAI Gym Tutorials](https://www.gymlibrary.ml/content/tutorials/)\n",
    "8. [More Resources](https://github.com/dennybritz/reinforcement-learning#resources)\n",
    "9. [If you like MATLAB...](https://www.mathworks.com/products/reinforcement-learning.html)\n",
    "\n",
    "First two resources are bare-minimum, but for the curious minded, we included more detailed resources!\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Here we will be using an environment from `Gym`. The environment we choose is the `CartPole-v1`, which is a part of the classic control system environments.\n",
    "\n",
    "The problem we are trying to solve is trying to keep a pole upright, which is attached to a frictionless cart with an un-actuated joint. The goal is to balance the pole by moving the cart to the left or to the right.\n",
    "\n",
    "Untrained environment looks like this:\n",
    "<img src=\"https://www.gymlibrary.ml/_images/cart_pole.gif\" width=\"350\"/>\n",
    "\n",
    "More information about the environment can be found [here](https://www.gymlibrary.ml/environments/classic_control/cart_pole/) and the source code for the environment can be found [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py).\n",
    "\n",
    "The environment has the following action and observation spaces:\n",
    "\n",
    "| Property          | Details             |\n",
    "|-------------------|---------------------|\n",
    "| Action Space      | `Discrete(2)`       |\n",
    "| Observation Space | `Box(4, 'float32')` |\n",
    "\n",
    "Action Space:\n",
    "\n",
    "| Num | Action                 |\n",
    "|:---:|------------------------|\n",
    "|  0  | Push cart to the left  |\n",
    "|  1  | Push cart to the right |\n",
    "\n",
    "Observation Space:\n",
    "\n",
    "| Num | Action                |     Min      |     Max     |\n",
    "|:---:|-----------------------|:------------:|:-----------:|\n",
    "|  0  | Cart Position         |    `-4.8`    |    `4.8`    |\n",
    "|  1  | Cart Velocity         |    `-inf`    |    `inf`    |\n",
    "|  2  | Pole Angle            | `-0.418 rad` | `0.418 rad` |\n",
    "|  3  | Pole Angular Velocity |    `-inf`    |    `inf`    |\n",
    "\n",
    "Let's take a more detailed look!\n",
    "\n",
    "We first import `gym` and make the environment. Then we will look at its pre-configured action space and observation space using built in attributes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(2)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here let's see what the observation state looks like if we reset the environment five times."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset 1 [ 0.01589157 -0.03395329  0.04970384  0.01610303]\n",
      "reset 2 [-0.0248685   0.0477733   0.03452688 -0.03332323]\n",
      "reset 3 [ 0.02041535 -0.0374088  -0.03411317  0.00380573]\n",
      "reset 4 [ 0.02248388 -0.00815395  0.01375304 -0.04466613]\n",
      "reset 5 [ 0.04557251 -0.01278952 -0.04608022  0.00124771]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    print(f\"reset {idx + 1}\", env.reset())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, we can see the cart and pole parameters are all different once we reset the environment.\n",
    "\n",
    "Reminder: Values are `[cart_position, cart_velocity, pole_angle(rad), pole_angular_velocity]`.\n",
    "\n",
    "Now let's reset the environment and take 10 steps, then we will visualize it. Since there is no trained model, the actions we sample are going to be randomly selected from the action space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`. Alternatively, please install imageio-ffmpeg with `pip install imageio-ffmpeg`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mDependencyNotInstalled\u001B[0m                    Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m      9\u001B[0m     env\u001B[38;5;241m.\u001B[39mrender()\n\u001B[1;32m---> 10\u001B[0m     \u001B[43mvideo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcapture_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     action \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39msample()\n\u001B[0;32m     13\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\wildcatter\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:151\u001B[0m, in \u001B[0;36mVideoRecorder.capture_frame\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encode_ansi_frame(frame)\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 151\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_encode_image_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\wildcatter\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:208\u001B[0m, in \u001B[0;36mVideoRecorder._encode_image_frame\u001B[1;34m(self, frame)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_encode_image_frame\u001B[39m(\u001B[38;5;28mself\u001B[39m, frame):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder:\n\u001B[1;32m--> 208\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m \u001B[43mImageEncoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mframes_per_sec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_frames_per_sec\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_version\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mversion_info\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\wildcatter\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:336\u001B[0m, in \u001B[0;36mImageEncoder.__init__\u001B[1;34m(self, output_path, frame_shape, frames_per_sec, output_frames_per_sec)\u001B[0m\n\u001B[0;32m    334\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m imageio_ffmpeg\u001B[38;5;241m.\u001B[39mget_ffmpeg_exe()\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 336\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mDependencyNotInstalled(\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;124;03m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`. Alternatively, please install imageio-ffmpeg with `pip install imageio-ffmpeg`\"\"\"\u001B[39;00m\n\u001B[0;32m    338\u001B[0m     )\n\u001B[0;32m    340\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart()\n",
      "\u001B[1;31mDependencyNotInstalled\u001B[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`. Alternatively, please install imageio-ffmpeg with `pip install imageio-ffmpeg`"
     ]
    }
   ],
   "source": [
    "from gym import wrappers\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env_w = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "env_w.reset()\n",
    "for idx in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, the returned variables are **observation** which is the current state, the **reward** is the current reward for the action we have taken (not cumulative), **done** states if the environment finished, or failed etc., and finally **info** is a diagnostic variable (if we included things in the `step` implementation). In the cartpole environment, the info is empty."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 146         |\n",
      "|    ep_rew_mean          | 146         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1599        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006854622 |\n",
      "|    clip_fraction        | 0.074       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.43        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 16.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 312          |\n",
      "|    ep_rew_mean          | 312          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1548         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061348686 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.534       |\n",
      "|    explained_variance   | -0.0748      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0487       |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    value_loss           | 0.23         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 452          |\n",
      "|    ep_rew_mean          | 452          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1532         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027107985 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | -0.0147      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0087      |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 0.00321      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 499          |\n",
      "|    ep_rew_mean          | 499          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1523         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019317234 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.379       |\n",
      "|    explained_variance   | -0.0835      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00593      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.000862    |\n",
      "|    value_loss           | 8.09e-05     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(100_000), log_interval=10)\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"dqn_lunar\", env=env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "500.0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "mean_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     11\u001B[0m action, _states \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(obs)\n\u001B[0;32m     12\u001B[0m obs, rewards, dones, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m---> 13\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrgb_array\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\wildcatter\\lib\\site-packages\\gym\\core.py:295\u001B[0m, in \u001B[0;36mWrapper.render\u001B[1;34m(self, mode, **kwargs)\u001B[0m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mrender(mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\wildcatter\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:229\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcarttrans\u001B[38;5;241m.\u001B[39mset_translation(cartx, carty)\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoletrans\u001B[38;5;241m.\u001B[39mset_rotation(\u001B[38;5;241m-\u001B[39mx[\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m--> 229\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mviewer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreturn_rgb_array\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrgb_array\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\wildcatter\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:145\u001B[0m, in \u001B[0;36mViewer.render\u001B[1;34m(self, return_rgb_array)\u001B[0m\n\u001B[0;32m    143\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mreshape(buffer\u001B[38;5;241m.\u001B[39mheight, buffer\u001B[38;5;241m.\u001B[39mwidth, \u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m    144\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :, \u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m--> 145\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39monetime_geoms \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr \u001B[38;5;28;01mif\u001B[39;00m return_rgb_array \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39misopen\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\wildcatter\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py:383\u001B[0m, in \u001B[0;36mWin32Window.flip\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    381\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_always_dwm \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dwm_composition_enabled():\n\u001B[0;32m    382\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interval:\n\u001B[1;32m--> 383\u001B[0m             \u001B[43m_dwmapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDwmFlush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstyle \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moverlay\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtransparent\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    386\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_transparency()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Enjoy trained agent\n",
    "# import imageio\n",
    "import numpy as np\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "img = env.render(mode=\"rgb_array\")\n",
    "images = []\n",
    "for i in range(1000):\n",
    "    images.append(img)\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "\n",
    "#\n",
    "# imageio.mimsave(\n",
    "#     \"test.gif\",\n",
    "#     [img for i, img in enumerate(images) if i % 10 == 0],\n",
    "#     fps=29,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}