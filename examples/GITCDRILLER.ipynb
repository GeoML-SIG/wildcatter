{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecb48a0-f05f-4787-954c-618c9b4a5205",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69dd798e-c44e-4ddf-ac2a-a68708c3f641",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gym Agent and Environment Class\n",
    "\n",
    "#### Things to Consider \n",
    "1. This is limited to drilling interactions, production is not considered in reward\n",
    "2. SHL is selected by user but should be something we included in the optimization \n",
    "3. The action and observations space logic must be preserved for the RL Q-Learning Flow, but the logic is not really dependant on these\n",
    "4. Collision logic just prevents agents from wrapping around environment on Eastern and Western boundaries. It also prevents from hitting boundaries North and South. Logic just causes a bounce back effect that forces the agent to perform the opposite action ex: if it is going east and hits a boundary, it will go west for that action instead. \n",
    "5. Realistically all that can be optimized now is drilling direction based on available drill pipe\n",
    "\n",
    "\n",
    "All models are created using tiled, here is the download link https://www.mapeditor.org/\n",
    "\n",
    "location = [row,col]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from wildcatter.environment import SimpleDriller"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the Env Object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Drill Campaign: 1\n",
      "Final Trajectory:[[1, 18], [2, 18], [3, 18], [3, 19], [2, 19], [2, 20], [1, 20], [1, 19]]\n",
      "Campaign:1 Score:0.0\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=50, delim=\",\")\n",
    "\n",
    "env = SimpleDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "for episode in range(1, episodes + 1):\n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print(f\"Final Trajectory:{env.trajectory}\")\n",
    "    print(\"Campaign:{} Score:{}\".format(episode, score))\n",
    "    print(\"------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Actual Execution of Agent/Env Interactions\n",
    "\n",
    "There are some basic print statements to help users understand what is going on since we don't have a UI or pygame visual to analyze."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x12e4e00a0>]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1440 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(env.trajectory, columns=[\"rows\", \"colums\"])\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.grid()\n",
    "plt.plot(df[\"colums\"], df[\"rows\"], \"-\", c=\"m\", linewidth=6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PPO TESTS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 22:09:41,083\tWARNING trainer.py:2540 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-06-26 22:09:41,085\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-06-26 22:09:41,085\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=25709)\u001B[0m 2022-06-26 22:09:42,274\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-06-26 22:09:42,307\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-06-26 22:09:42,307\tWARNING trainer.py:2540 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-06-26 22:09:42,331\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=25710)\u001B[0m 2022-06-26 22:09:43,550\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-09-45\n",
      "done: false\n",
      "episode_len_mean: 37.66981132075472\n",
      "episode_media: {}\n",
      "episode_reward_max: 91.0\n",
      "episode_reward_mean: 7.066037735849057\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 106\n",
      "episodes_total: 106\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3861144195320785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001821326983072562\n",
      "        policy_loss: -0.00512676188141428\n",
      "        total_loss: 3.3689793202184863\n",
      "        vf_explained_var: -0.06347525863237279\n",
      "        vf_loss: 3.374069649883498\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 22.2\n",
      "  ram_util_percent: 52.25\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.016046505932568372\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.022202961089103947\n",
      "  mean_inference_ms: 0.24435109598759974\n",
      "  mean_raw_obs_processing_ms: 0.03444472124385285\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 37.66981132075472\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 91.0\n",
      "  episode_reward_mean: 7.066037735849057\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 106\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 40\n",
      "    - 33\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 11\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 8\n",
      "    - 24\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 13\n",
      "    - 25\n",
      "    - 50\n",
      "    - 23\n",
      "    - 24\n",
      "    - 21\n",
      "    - 37\n",
      "    - 19\n",
      "    - 50\n",
      "    - 15\n",
      "    - 20\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 41\n",
      "    - 23\n",
      "    - 35\n",
      "    - 44\n",
      "    - 43\n",
      "    - 50\n",
      "    - 10\n",
      "    - 36\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 27\n",
      "    - 18\n",
      "    - 27\n",
      "    - 7\n",
      "    - 50\n",
      "    - 24\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 26\n",
      "    - 13\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    episode_reward:\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 28.0\n",
      "    - 2.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 3.0\n",
      "    - 3.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 14.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 8.0\n",
      "    - 15.0\n",
      "    - 8.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 91.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.016046505932568372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.022202961089103947\n",
      "    mean_inference_ms: 0.24435109598759974\n",
      "    mean_raw_obs_processing_ms: 0.03444472124385285\n",
      "time_since_restore: 2.7368252277374268\n",
      "time_this_iter_s: 2.7368252277374268\n",
      "time_total_s: 2.7368252277374268\n",
      "timers:\n",
      "  learn_throughput: 2814.997\n",
      "  learn_time_ms: 1420.961\n",
      "  load_throughput: 9592461.978\n",
      "  load_time_ms: 0.417\n",
      "  training_iteration_time_ms: 2734.155\n",
      "  update_time_ms: 0.67\n",
      "timestamp: 1656299385\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "checkpoint saved at /Users/tasansal/ray_results/PPOTrainer_SimpleDriller_2022-06-26_22-09-41ksweze8k/checkpoint_000001/checkpoint-1\n",
      "agent_timesteps_total: 8000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-09-47\n",
      "done: false\n",
      "episode_len_mean: 33.05785123966942\n",
      "episode_media: {}\n",
      "episode_reward_max: 90.0\n",
      "episode_reward_mean: 5.322314049586777\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 121\n",
      "episodes_total: 227\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.10000000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3849546697831923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005465983435085515\n",
      "        policy_loss: -0.005722594032845189\n",
      "        total_loss: 2.5430435978276753\n",
      "        vf_explained_var: -0.06429520287821369\n",
      "        vf_loss: 2.548711525524656\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 8000\n",
      "num_agent_steps_trained: 8000\n",
      "num_env_steps_sampled: 8000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 8000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.1\n",
      "  ram_util_percent: 52.175\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01565817966563688\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.021470902696458596\n",
      "  mean_inference_ms: 0.23911190307106084\n",
      "  mean_raw_obs_processing_ms: 0.03385522964223892\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 33.05785123966942\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 90.0\n",
      "  episode_reward_mean: 5.322314049586777\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 121\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 42\n",
      "    - 10\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 6\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 17\n",
      "    - 50\n",
      "    - 25\n",
      "    - 15\n",
      "    - 12\n",
      "    - 20\n",
      "    - 50\n",
      "    - 34\n",
      "    - 32\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 12\n",
      "    - 30\n",
      "    - 38\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 6\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 44\n",
      "    - 19\n",
      "    - 9\n",
      "    - 37\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 20\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 36\n",
      "    - 37\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 12\n",
      "    - 20\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 33\n",
      "    - 29\n",
      "    - 35\n",
      "    - 20\n",
      "    - 42\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 21\n",
      "    - 50\n",
      "    - 36\n",
      "    - 9\n",
      "    - 25\n",
      "    - 24\n",
      "    - 6\n",
      "    - 17\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 12\n",
      "    - 50\n",
      "    - 29\n",
      "    - 4\n",
      "    - 50\n",
      "    - 19\n",
      "    - 33\n",
      "    - 4\n",
      "    - 19\n",
      "    - 20\n",
      "    - 50\n",
      "    - 27\n",
      "    - 18\n",
      "    - 33\n",
      "    - 50\n",
      "    - 33\n",
      "    - 16\n",
      "    - 7\n",
      "    - 23\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 36\n",
      "    - 34\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 13.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 59.0\n",
      "    - 45.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 90.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 6.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01565817966563688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021470902696458596\n",
      "    mean_inference_ms: 0.23911190307106084\n",
      "    mean_raw_obs_processing_ms: 0.03385522964223892\n",
      "time_since_restore: 5.4194371700286865\n",
      "time_this_iter_s: 2.6826119422912598\n",
      "time_total_s: 5.4194371700286865\n",
      "timers:\n",
      "  learn_throughput: 2802.884\n",
      "  learn_time_ms: 1427.101\n",
      "  load_throughput: 13888423.841\n",
      "  load_time_ms: 0.288\n",
      "  training_iteration_time_ms: 2706.948\n",
      "  update_time_ms: 0.639\n",
      "timestamp: 1656299387\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-09-50\n",
      "done: false\n",
      "episode_len_mean: 34.69565217391305\n",
      "episode_media: {}\n",
      "episode_reward_max: 70.0\n",
      "episode_reward_mean: 10.756521739130434\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 115\n",
      "episodes_total: 342\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.05000000000000001\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3822000394585312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0010757570497952002\n",
      "        policy_loss: -0.007955327977536506\n",
      "        total_loss: 3.574670809699643\n",
      "        vf_explained_var: -0.015417352735355336\n",
      "        vf_loss: 3.5825723516103882\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 12000\n",
      "num_agent_steps_trained: 12000\n",
      "num_env_steps_sampled: 12000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 12000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.05\n",
      "  ram_util_percent: 52.3\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015479566136873436\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.021205724413102287\n",
      "  mean_inference_ms: 0.23694604985386747\n",
      "  mean_raw_obs_processing_ms: 0.0333887131529424\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 34.69565217391305\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 70.0\n",
      "  episode_reward_mean: 10.756521739130434\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 115\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 20\n",
      "    - 35\n",
      "    - 50\n",
      "    - 39\n",
      "    - 18\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 42\n",
      "    - 32\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 26\n",
      "    - 50\n",
      "    - 23\n",
      "    - 12\n",
      "    - 36\n",
      "    - 50\n",
      "    - 15\n",
      "    - 6\n",
      "    - 29\n",
      "    - 50\n",
      "    - 39\n",
      "    - 14\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 28\n",
      "    - 50\n",
      "    - 10\n",
      "    - 19\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 14\n",
      "    - 24\n",
      "    - 50\n",
      "    - 41\n",
      "    - 25\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 10\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 8\n",
      "    - 20\n",
      "    - 34\n",
      "    - 26\n",
      "    - 18\n",
      "    - 50\n",
      "    - 14\n",
      "    - 29\n",
      "    - 32\n",
      "    - 18\n",
      "    - 22\n",
      "    - 23\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 41\n",
      "    - 8\n",
      "    - 16\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 22\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 14\n",
      "    - 37\n",
      "    - 50\n",
      "    - 36\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 32\n",
      "    - 8\n",
      "    - 44\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 35\n",
      "    - 27\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 24\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 53.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 62.0\n",
      "    - 12.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015479566136873436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.021205724413102287\n",
      "    mean_inference_ms: 0.23694604985386747\n",
      "    mean_raw_obs_processing_ms: 0.0333887131529424\n",
      "time_since_restore: 8.076902151107788\n",
      "time_this_iter_s: 2.6574649810791016\n",
      "time_total_s: 8.076902151107788\n",
      "timers:\n",
      "  learn_throughput: 2808.191\n",
      "  learn_time_ms: 1424.405\n",
      "  load_throughput: 16262244.911\n",
      "  load_time_ms: 0.246\n",
      "  training_iteration_time_ms: 2689.587\n",
      "  update_time_ms: 0.668\n",
      "timestamp: 1656299390\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 16000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-09-53\n",
      "done: false\n",
      "episode_len_mean: 33.83898305084746\n",
      "episode_media: {}\n",
      "episode_reward_max: 88.0\n",
      "episode_reward_mean: 11.847457627118644\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 118\n",
      "episodes_total: 460\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.025000000000000005\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3770116438147841\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0018305505721038708\n",
      "        policy_loss: -0.008840322077915233\n",
      "        total_loss: 3.9371448524536623\n",
      "        vf_explained_var: 0.0025465789020702404\n",
      "        vf_loss: 3.9459394350808155\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 16000\n",
      "num_agent_steps_trained: 16000\n",
      "num_env_steps_sampled: 16000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 16000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.049999999999999\n",
      "  ram_util_percent: 52.27499999999999\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015400624351198685\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02100598654905847\n",
      "  mean_inference_ms: 0.235994192490197\n",
      "  mean_raw_obs_processing_ms: 0.03317220308923564\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 33.83898305084746\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 88.0\n",
      "  episode_reward_mean: 11.847457627118644\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 118\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 32\n",
      "    - 26\n",
      "    - 50\n",
      "    - 12\n",
      "    - 11\n",
      "    - 22\n",
      "    - 50\n",
      "    - 25\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 21\n",
      "    - 50\n",
      "    - 11\n",
      "    - 15\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 13\n",
      "    - 9\n",
      "    - 32\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 26\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 16\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 30\n",
      "    - 31\n",
      "    - 27\n",
      "    - 23\n",
      "    - 27\n",
      "    - 13\n",
      "    - 42\n",
      "    - 12\n",
      "    - 50\n",
      "    - 25\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 38\n",
      "    - 7\n",
      "    - 25\n",
      "    - 6\n",
      "    - 50\n",
      "    - 43\n",
      "    - 33\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 32\n",
      "    - 32\n",
      "    - 20\n",
      "    - 31\n",
      "    - 20\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 10\n",
      "    - 50\n",
      "    - 10\n",
      "    - 18\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 36\n",
      "    - 24\n",
      "    - 50\n",
      "    - 20\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 23\n",
      "    - 29\n",
      "    - 22\n",
      "    - 20\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 27\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 30\n",
      "    - 11\n",
      "    - 38\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 5.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015400624351198685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02100598654905847\n",
      "    mean_inference_ms: 0.235994192490197\n",
      "    mean_raw_obs_processing_ms: 0.03317220308923564\n",
      "time_since_restore: 10.7299063205719\n",
      "time_this_iter_s: 2.6530041694641113\n",
      "time_total_s: 10.7299063205719\n",
      "timers:\n",
      "  learn_throughput: 2814.129\n",
      "  learn_time_ms: 1421.399\n",
      "  load_throughput: 17739588.686\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 2679.679\n",
      "  update_time_ms: 0.673\n",
      "timestamp: 1656299393\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 20000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-09-55\n",
      "done: false\n",
      "episode_len_mean: 37.129629629629626\n",
      "episode_media: {}\n",
      "episode_reward_max: 99.0\n",
      "episode_reward_mean: 11.675925925925926\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 108\n",
      "episodes_total: 568\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.012500000000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3700187276768427\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.002618948236403246\n",
      "        policy_loss: -0.006278819867199467\n",
      "        total_loss: 3.8396304672764194\n",
      "        vf_explained_var: -0.06478504512899666\n",
      "        vf_loss: 3.845876565106934\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 20000\n",
      "num_agent_steps_trained: 20000\n",
      "num_env_steps_sampled: 20000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 20000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.825\n",
      "  ram_util_percent: 52.275000000000006\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015354759662987306\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020926192154747735\n",
      "  mean_inference_ms: 0.23536268254946632\n",
      "  mean_raw_obs_processing_ms: 0.03297574150223154\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 37.129629629629626\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 99.0\n",
      "  episode_reward_mean: 11.675925925925926\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 108\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 42\n",
      "    - 11\n",
      "    - 20\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 26\n",
      "    - 16\n",
      "    - 50\n",
      "    - 19\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 32\n",
      "    - 50\n",
      "    - 31\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 47\n",
      "    - 23\n",
      "    - 14\n",
      "    - 25\n",
      "    - 46\n",
      "    - 50\n",
      "    - 25\n",
      "    - 23\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 14\n",
      "    - 32\n",
      "    - 50\n",
      "    - 8\n",
      "    - 37\n",
      "    - 26\n",
      "    - 18\n",
      "    - 48\n",
      "    - 35\n",
      "    - 18\n",
      "    - 37\n",
      "    - 6\n",
      "    - 20\n",
      "    - 50\n",
      "    - 42\n",
      "    - 10\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 27\n",
      "    - 34\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 4\n",
      "    - 41\n",
      "    - 17\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 36\n",
      "    - 33\n",
      "    - 23\n",
      "    - 40\n",
      "    - 39\n",
      "    - 50\n",
      "    - 4\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 22\n",
      "    - 34\n",
      "    - 41\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 54.0\n",
      "    - 1.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 18.0\n",
      "    - 58.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 99.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 4.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 1.0\n",
      "    - 27.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015354759662987306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020926192154747735\n",
      "    mean_inference_ms: 0.23536268254946632\n",
      "    mean_raw_obs_processing_ms: 0.03297574150223154\n",
      "time_since_restore: 13.368529319763184\n",
      "time_this_iter_s: 2.638622999191284\n",
      "time_total_s: 13.368529319763184\n",
      "timers:\n",
      "  learn_throughput: 2822.258\n",
      "  learn_time_ms: 1417.305\n",
      "  load_throughput: 18620661.487\n",
      "  load_time_ms: 0.215\n",
      "  training_iteration_time_ms: 2670.962\n",
      "  update_time_ms: 0.655\n",
      "timestamp: 1656299395\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 24000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-09-58\n",
      "done: false\n",
      "episode_len_mean: 36.69724770642202\n",
      "episode_media: {}\n",
      "episode_reward_max: 105.0\n",
      "episode_reward_mean: 16.394495412844037\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 109\n",
      "episodes_total: 677\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.006250000000000001\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.360500031901944\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0040245813268777585\n",
      "        policy_loss: -0.005327677777818134\n",
      "        total_loss: 4.740973823942164\n",
      "        vf_explained_var: 0.0028612935414878272\n",
      "        vf_loss: 4.746276357481556\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 24000\n",
      "num_agent_steps_trained: 24000\n",
      "num_env_steps_sampled: 24000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 24000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.766666666666666\n",
      "  ram_util_percent: 52.29999999999999\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015323685882518775\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020861670373007738\n",
      "  mean_inference_ms: 0.23491225348507913\n",
      "  mean_raw_obs_processing_ms: 0.03286833703719071\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 36.69724770642202\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 105.0\n",
      "  episode_reward_mean: 16.394495412844037\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 109\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 34\n",
      "    - 17\n",
      "    - 50\n",
      "    - 11\n",
      "    - 16\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 32\n",
      "    - 17\n",
      "    - 34\n",
      "    - 8\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 9\n",
      "    - 27\n",
      "    - 17\n",
      "    - 50\n",
      "    - 47\n",
      "    - 13\n",
      "    - 39\n",
      "    - 50\n",
      "    - 8\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 18\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 23\n",
      "    - 22\n",
      "    - 45\n",
      "    - 50\n",
      "    - 31\n",
      "    - 21\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 42\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 18\n",
      "    - 32\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 12\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 46\n",
      "    - 17\n",
      "    - 28\n",
      "    - 50\n",
      "    - 6\n",
      "    - 36\n",
      "    - 47\n",
      "    - 50\n",
      "    - 13\n",
      "    - 34\n",
      "    - 31\n",
      "    - 23\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 33\n",
      "    - 28\n",
      "    - 27\n",
      "    - 13\n",
      "    - 27\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 30.0\n",
      "    - 23.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 79.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 4.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 2.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 22.0\n",
      "    - 14.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 96.0\n",
      "    - 30.0\n",
      "    - 25.0\n",
      "    - 34.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 1.0\n",
      "    - 28.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 105.0\n",
      "    - 31.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 19.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 38.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015323685882518775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020861670373007738\n",
      "    mean_inference_ms: 0.23491225348507913\n",
      "    mean_raw_obs_processing_ms: 0.03286833703719071\n",
      "time_since_restore: 16.011991024017334\n",
      "time_this_iter_s: 2.6434617042541504\n",
      "time_total_s: 16.011991024017334\n",
      "timers:\n",
      "  learn_throughput: 2825.958\n",
      "  learn_time_ms: 1415.449\n",
      "  load_throughput: 19350883.506\n",
      "  load_time_ms: 0.207\n",
      "  training_iteration_time_ms: 2665.94\n",
      "  update_time_ms: 0.651\n",
      "timestamp: 1656299398\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 28000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-01\n",
      "done: false\n",
      "episode_len_mean: 35.097345132743364\n",
      "episode_media: {}\n",
      "episode_reward_max: 104.0\n",
      "episode_reward_mean: 15.283185840707965\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 113\n",
      "episodes_total: 790\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3500944242682509\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005357054120144977\n",
      "        policy_loss: -0.0073893990017153246\n",
      "        total_loss: 4.393068933422848\n",
      "        vf_explained_var: -0.030813508392662132\n",
      "        vf_loss: 4.400441580594227\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 28000\n",
      "num_agent_steps_trained: 28000\n",
      "num_env_steps_sampled: 28000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 28000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.875\n",
      "  ram_util_percent: 52.3\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015312133314422126\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020804879818723514\n",
      "  mean_inference_ms: 0.23470665360300857\n",
      "  mean_raw_obs_processing_ms: 0.03283054742390444\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 35.097345132743364\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 104.0\n",
      "  episode_reward_mean: 15.283185840707965\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 113\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 15\n",
      "    - 30\n",
      "    - 18\n",
      "    - 27\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 11\n",
      "    - 9\n",
      "    - 50\n",
      "    - 30\n",
      "    - 27\n",
      "    - 22\n",
      "    - 26\n",
      "    - 50\n",
      "    - 47\n",
      "    - 43\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 13\n",
      "    - 37\n",
      "    - 24\n",
      "    - 14\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 35\n",
      "    - 20\n",
      "    - 50\n",
      "    - 27\n",
      "    - 14\n",
      "    - 29\n",
      "    - 50\n",
      "    - 31\n",
      "    - 18\n",
      "    - 9\n",
      "    - 28\n",
      "    - 50\n",
      "    - 35\n",
      "    - 25\n",
      "    - 8\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 30\n",
      "    - 37\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 17\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 22\n",
      "    - 50\n",
      "    - 31\n",
      "    - 37\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 44\n",
      "    - 18\n",
      "    - 33\n",
      "    - 46\n",
      "    - 50\n",
      "    - 40\n",
      "    - 29\n",
      "    - 23\n",
      "    - 6\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 33\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 22\n",
      "    - 50\n",
      "    - 16\n",
      "    - 28\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 76.0\n",
      "    - 0.0\n",
      "    - 11.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 79.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 13.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 55.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 104.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 90.0\n",
      "    - 0.0\n",
      "    - 93.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 21.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015312133314422126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020804879818723514\n",
      "    mean_inference_ms: 0.23470665360300857\n",
      "    mean_raw_obs_processing_ms: 0.03283054742390444\n",
      "time_since_restore: 18.65822696685791\n",
      "time_this_iter_s: 2.646235942840576\n",
      "time_total_s: 18.65822696685791\n",
      "timers:\n",
      "  learn_throughput: 2829.432\n",
      "  learn_time_ms: 1413.712\n",
      "  load_throughput: 20137262.003\n",
      "  load_time_ms: 0.199\n",
      "  training_iteration_time_ms: 2662.703\n",
      "  update_time_ms: 0.686\n",
      "timestamp: 1656299401\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 32000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-03\n",
      "done: false\n",
      "episode_len_mean: 37.48598130841121\n",
      "episode_media: {}\n",
      "episode_reward_max: 91.0\n",
      "episode_reward_mean: 18.542056074766354\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 107\n",
      "episodes_total: 897\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.339402315949881\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005879306972842984\n",
      "        policy_loss: -0.002620581050793971\n",
      "        total_loss: 5.198254671917167\n",
      "        vf_explained_var: -0.031341690145513065\n",
      "        vf_loss: 5.200856867392537\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 32000\n",
      "num_agent_steps_trained: 32000\n",
      "num_env_steps_sampled: 32000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 32000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.625\n",
      "  ram_util_percent: 52.3\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015277213772544007\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020749844378566168\n",
      "  mean_inference_ms: 0.23435367680665822\n",
      "  mean_raw_obs_processing_ms: 0.03270518649001036\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 37.48598130841121\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 91.0\n",
      "  episode_reward_mean: 18.542056074766354\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 107\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 10\n",
      "    - 8\n",
      "    - 17\n",
      "    - 50\n",
      "    - 16\n",
      "    - 49\n",
      "    - 42\n",
      "    - 22\n",
      "    - 14\n",
      "    - 15\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 23\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 8\n",
      "    - 14\n",
      "    - 18\n",
      "    - 47\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 13\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 39\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 10\n",
      "    - 18\n",
      "    - 8\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 43\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 23\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 27\n",
      "    - 43\n",
      "    - 20\n",
      "    - 31\n",
      "    - 24\n",
      "    - 50\n",
      "    - 10\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    episode_reward:\n",
      "    - 19.0\n",
      "    - 4.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 37.0\n",
      "    - 45.0\n",
      "    - 47.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 13.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 2.0\n",
      "    - 32.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 55.0\n",
      "    - 66.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 31.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 57.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 85.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 91.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 10.0\n",
      "    - 76.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 18.0\n",
      "    - 2.0\n",
      "    - 37.0\n",
      "    - 8.0\n",
      "    - 5.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015277213772544007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020749844378566168\n",
      "    mean_inference_ms: 0.23435367680665822\n",
      "    mean_raw_obs_processing_ms: 0.03270518649001036\n",
      "time_since_restore: 21.288392066955566\n",
      "time_this_iter_s: 2.6301651000976562\n",
      "time_total_s: 21.288392066955566\n",
      "timers:\n",
      "  learn_throughput: 2833.161\n",
      "  learn_time_ms: 1411.85\n",
      "  load_throughput: 20519450.848\n",
      "  load_time_ms: 0.195\n",
      "  training_iteration_time_ms: 2658.306\n",
      "  update_time_ms: 0.697\n",
      "timestamp: 1656299403\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 36000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-06\n",
      "done: false\n",
      "episode_len_mean: 38.142857142857146\n",
      "episode_media: {}\n",
      "episode_reward_max: 112.0\n",
      "episode_reward_mean: 16.961904761904762\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 105\n",
      "episodes_total: 1002\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3224107526963758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00840912184312499\n",
      "        policy_loss: -0.0015048979300885431\n",
      "        total_loss: 5.3942679019384485\n",
      "        vf_explained_var: 0.0025450472549725604\n",
      "        vf_loss: 5.395746495390451\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 36000\n",
      "num_agent_steps_trained: 36000\n",
      "num_env_steps_sampled: 36000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 36000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.925\n",
      "  ram_util_percent: 52.3\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015259961254619694\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020733581894334996\n",
      "  mean_inference_ms: 0.23421724386796144\n",
      "  mean_raw_obs_processing_ms: 0.0326511664594671\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 38.142857142857146\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 112.0\n",
      "  episode_reward_mean: 16.961904761904762\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 105\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 19\n",
      "    - 50\n",
      "    - 10\n",
      "    - 7\n",
      "    - 27\n",
      "    - 31\n",
      "    - 27\n",
      "    - 20\n",
      "    - 33\n",
      "    - 50\n",
      "    - 6\n",
      "    - 47\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 9\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 40\n",
      "    - 23\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 26\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 14\n",
      "    - 50\n",
      "    - 11\n",
      "    - 20\n",
      "    - 25\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 24\n",
      "    - 21\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    episode_reward:\n",
      "    - 38.0\n",
      "    - 67.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 25.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 112.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 59.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 19.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 4.0\n",
      "    - 28.0\n",
      "    - 51.0\n",
      "    - 21.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 23.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 10.0\n",
      "    - 32.0\n",
      "    - 5.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 92.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 76.0\n",
      "    - 64.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015259961254619694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020733581894334996\n",
      "    mean_inference_ms: 0.23421724386796144\n",
      "    mean_raw_obs_processing_ms: 0.0326511664594671\n",
      "time_since_restore: 23.95613694190979\n",
      "time_this_iter_s: 2.6677448749542236\n",
      "time_total_s: 23.95613694190979\n",
      "timers:\n",
      "  learn_throughput: 2829.276\n",
      "  learn_time_ms: 1413.789\n",
      "  load_throughput: 20852775.031\n",
      "  load_time_ms: 0.192\n",
      "  training_iteration_time_ms: 2659.069\n",
      "  update_time_ms: 0.703\n",
      "timestamp: 1656299406\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 40000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-09\n",
      "done: false\n",
      "episode_len_mean: 37.51401869158879\n",
      "episode_media: {}\n",
      "episode_reward_max: 109.0\n",
      "episode_reward_mean: 20.093457943925234\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 107\n",
      "episodes_total: 1109\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3141487620210135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008871003316705345\n",
      "        policy_loss: -0.011200400910550547\n",
      "        total_loss: 5.944940067491224\n",
      "        vf_explained_var: 0.002500403952854936\n",
      "        vf_loss: 5.956112749906637\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 40000\n",
      "num_agent_steps_trained: 40000\n",
      "num_env_steps_sampled: 40000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 40000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.2\n",
      "  ram_util_percent: 52.224999999999994\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015255707579402191\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020705080965734035\n",
      "  mean_inference_ms: 0.2341759013836392\n",
      "  mean_raw_obs_processing_ms: 0.03262697549191824\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 37.51401869158879\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 109.0\n",
      "  episode_reward_mean: 20.093457943925234\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 107\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 23\n",
      "    - 8\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 27\n",
      "    - 20\n",
      "    - 50\n",
      "    - 4\n",
      "    - 10\n",
      "    - 23\n",
      "    - 28\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 38\n",
      "    - 26\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 33\n",
      "    - 50\n",
      "    - 24\n",
      "    - 34\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 27\n",
      "    - 6\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 36\n",
      "    - 38\n",
      "    - 31\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 11\n",
      "    - 50\n",
      "    - 26\n",
      "    - 33\n",
      "    - 50\n",
      "    - 10\n",
      "    - 25\n",
      "    - 15\n",
      "    - 15\n",
      "    - 50\n",
      "    - 38\n",
      "    - 10\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 14\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 14\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 56.0\n",
      "    - 12.0\n",
      "    - 9.0\n",
      "    - 48.0\n",
      "    - 26.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 85.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 109.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 37.0\n",
      "    - 39.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 6.0\n",
      "    - 33.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 8.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 30.0\n",
      "    - 7.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 25.0\n",
      "    - 21.0\n",
      "    - 4.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 37.0\n",
      "    - 77.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 54.0\n",
      "    - 40.0\n",
      "    - 2.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 10.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015255707579402191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020705080965734035\n",
      "    mean_inference_ms: 0.2341759013836392\n",
      "    mean_raw_obs_processing_ms: 0.03262697549191824\n",
      "time_since_restore: 26.59868025779724\n",
      "time_this_iter_s: 2.642543315887451\n",
      "time_total_s: 26.59868025779724\n",
      "timers:\n",
      "  learn_throughput: 2832.492\n",
      "  learn_time_ms: 1412.184\n",
      "  load_throughput: 21328777.015\n",
      "  load_time_ms: 0.188\n",
      "  training_iteration_time_ms: 2657.136\n",
      "  update_time_ms: 0.702\n",
      "timestamp: 1656299409\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 44000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_trained: 44000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-11\n",
      "done: false\n",
      "episode_len_mean: 40.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 86.0\n",
      "episode_reward_mean: 22.07\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 100\n",
      "episodes_total: 1209\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.3042962162725387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009475111877245288\n",
      "        policy_loss: -0.005244609292957091\n",
      "        total_loss: 6.48776893384995\n",
      "        vf_explained_var: 0.002509633379597818\n",
      "        vf_loss: 6.492983946364413\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_trained: 44000\n",
      "iterations_since_restore: 11\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 44000\n",
      "num_agent_steps_trained: 44000\n",
      "num_env_steps_sampled: 44000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 44000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.366666666666667\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015238117557778008\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02067819654593422\n",
      "  mean_inference_ms: 0.2339772074247652\n",
      "  mean_raw_obs_processing_ms: 0.032548354334761984\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 40.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 86.0\n",
      "  episode_reward_mean: 22.07\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 26\n",
      "    - 50\n",
      "    - 44\n",
      "    - 22\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 22\n",
      "    - 25\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 18\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 47\n",
      "    - 50\n",
      "    - 46\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 12\n",
      "    - 50\n",
      "    - 49\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 26\n",
      "    - 50\n",
      "    - 39\n",
      "    - 47\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 34\n",
      "    - 39\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 41\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 13\n",
      "    - 22\n",
      "    - 32\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 47\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 44.0\n",
      "    - 4.0\n",
      "    - 3.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 47.0\n",
      "    - 65.0\n",
      "    - 82.0\n",
      "    - 32.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 33.0\n",
      "    - 2.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 86.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 66.0\n",
      "    - 32.0\n",
      "    - 46.0\n",
      "    - 36.0\n",
      "    - 46.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 14.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 65.0\n",
      "    - 31.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 37.0\n",
      "    - 44.0\n",
      "    - 1.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 85.0\n",
      "    - 15.0\n",
      "    - 6.0\n",
      "    - 21.0\n",
      "    - 18.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 78.0\n",
      "    - 56.0\n",
      "    - 34.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015238117557778008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02067819654593422\n",
      "    mean_inference_ms: 0.2339772074247652\n",
      "    mean_raw_obs_processing_ms: 0.032548354334761984\n",
      "time_since_restore: 29.250234365463257\n",
      "time_this_iter_s: 2.6515541076660156\n",
      "time_total_s: 29.250234365463257\n",
      "timers:\n",
      "  learn_throughput: 2833.023\n",
      "  learn_time_ms: 1411.919\n",
      "  load_throughput: 22761112.468\n",
      "  load_time_ms: 0.176\n",
      "  training_iteration_time_ms: 2648.635\n",
      "  update_time_ms: 0.695\n",
      "timestamp: 1656299411\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 48000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_trained: 48000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-14\n",
      "done: false\n",
      "episode_len_mean: 40.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 92.0\n",
      "episode_reward_mean: 21.78\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 100\n",
      "episodes_total: 1309\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.295717538300381\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010163972732080247\n",
      "        policy_loss: -0.018307869272526874\n",
      "        total_loss: 5.7676586788187745\n",
      "        vf_explained_var: 0.003195482236082836\n",
      "        vf_loss: 5.785934784784112\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_trained: 48000\n",
      "iterations_since_restore: 12\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 48000\n",
      "num_agent_steps_trained: 48000\n",
      "num_env_steps_sampled: 48000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 48000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.65\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01523177878702336\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020659160361692602\n",
      "  mean_inference_ms: 0.23387625839250248\n",
      "  mean_raw_obs_processing_ms: 0.032469187251518344\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 40.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 92.0\n",
      "  episode_reward_mean: 21.78\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 5\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 42\n",
      "    - 15\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 39\n",
      "    - 40\n",
      "    - 27\n",
      "    - 23\n",
      "    - 50\n",
      "    - 39\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 30\n",
      "    - 49\n",
      "    - 25\n",
      "    - 32\n",
      "    - 38\n",
      "    - 28\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 23\n",
      "    - 50\n",
      "    - 24\n",
      "    - 27\n",
      "    - 50\n",
      "    - 33\n",
      "    - 19\n",
      "    - 33\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 1.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 40.0\n",
      "    - 37.0\n",
      "    - 26.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 5.0\n",
      "    - 50.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 55.0\n",
      "    - 18.0\n",
      "    - 28.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 1.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 38.0\n",
      "    - 10.0\n",
      "    - 56.0\n",
      "    - 54.0\n",
      "    - 28.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 88.0\n",
      "    - 28.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 79.0\n",
      "    - 21.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 92.0\n",
      "    - 56.0\n",
      "    - 2.0\n",
      "    - 72.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 81.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 19.0\n",
      "    - 16.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 1.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 35.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 14.0\n",
      "    - 15.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01523177878702336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020659160361692602\n",
      "    mean_inference_ms: 0.23387625839250248\n",
      "    mean_raw_obs_processing_ms: 0.032469187251518344\n",
      "time_since_restore: 31.89512324333191\n",
      "time_this_iter_s: 2.6448888778686523\n",
      "time_total_s: 31.89512324333191\n",
      "timers:\n",
      "  learn_throughput: 2837.635\n",
      "  learn_time_ms: 1409.625\n",
      "  load_throughput: 22592534.339\n",
      "  load_time_ms: 0.177\n",
      "  training_iteration_time_ms: 2644.906\n",
      "  update_time_ms: 0.708\n",
      "timestamp: 1656299414\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 48000\n",
      "training_iteration: 12\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 52000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_env_steps_sampled: 52000\n",
      "  num_env_steps_trained: 52000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-17\n",
      "done: false\n",
      "episode_len_mean: 39.584158415841586\n",
      "episode_media: {}\n",
      "episode_reward_max: 98.0\n",
      "episode_reward_mean: 21.752475247524753\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 101\n",
      "episodes_total: 1410\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2874857109080078\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011366653263128253\n",
      "        policy_loss: -0.0005412034039455716\n",
      "        total_loss: 5.8153944880090735\n",
      "        vf_explained_var: -0.009628359220361198\n",
      "        vf_loss: 5.8159001730863125\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_env_steps_sampled: 52000\n",
      "  num_env_steps_trained: 52000\n",
      "iterations_since_restore: 13\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 52000\n",
      "num_agent_steps_trained: 52000\n",
      "num_env_steps_sampled: 52000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 52000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.675\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015221954100393465\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020649086472868974\n",
      "  mean_inference_ms: 0.233802682438602\n",
      "  mean_raw_obs_processing_ms: 0.03243272564635411\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 39.584158415841586\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 98.0\n",
      "  episode_reward_mean: 21.752475247524753\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 101\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 34\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 16\n",
      "    - 6\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 24\n",
      "    - 16\n",
      "    - 18\n",
      "    - 6\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 22\n",
      "    - 50\n",
      "    - 36\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 26\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 8\n",
      "    - 33\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 17\n",
      "    - 16\n",
      "    - 20\n",
      "    - 50\n",
      "    - 6\n",
      "    - 36\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 38.0\n",
      "    - 20.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 59.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 42.0\n",
      "    - 38.0\n",
      "    - 8.0\n",
      "    - 14.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 19.0\n",
      "    - 16.0\n",
      "    - 23.0\n",
      "    - 25.0\n",
      "    - 71.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 39.0\n",
      "    - 43.0\n",
      "    - 47.0\n",
      "    - 39.0\n",
      "    - 19.0\n",
      "    - 86.0\n",
      "    - 64.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 1.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 86.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 8.0\n",
      "    - 98.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015221954100393465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020649086472868974\n",
      "    mean_inference_ms: 0.233802682438602\n",
      "    mean_raw_obs_processing_ms: 0.03243272564635411\n",
      "time_since_restore: 34.552202463150024\n",
      "time_this_iter_s: 2.6570792198181152\n",
      "time_total_s: 34.552202463150024\n",
      "timers:\n",
      "  learn_throughput: 2837.327\n",
      "  learn_time_ms: 1409.778\n",
      "  load_throughput: 22504649.229\n",
      "  load_time_ms: 0.178\n",
      "  training_iteration_time_ms: 2644.879\n",
      "  update_time_ms: 0.706\n",
      "timestamp: 1656299417\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 56000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 56000\n",
      "  num_env_steps_trained: 56000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-19\n",
      "done: false\n",
      "episode_len_mean: 36.24545454545454\n",
      "episode_media: {}\n",
      "episode_reward_max: 74.0\n",
      "episode_reward_mean: 18.1\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 110\n",
      "episodes_total: 1520\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.281827026285151\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010893869476132495\n",
      "        policy_loss: -0.013066159056559687\n",
      "        total_loss: 5.713854487480655\n",
      "        vf_explained_var: 0.003396694121822234\n",
      "        vf_loss: 5.726886594359593\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 56000\n",
      "  num_env_steps_trained: 56000\n",
      "iterations_since_restore: 14\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 56000\n",
      "num_agent_steps_trained: 56000\n",
      "num_env_steps_sampled: 56000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 56000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.85\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015217424177684541\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020638484767167842\n",
      "  mean_inference_ms: 0.23375620397252125\n",
      "  mean_raw_obs_processing_ms: 0.03241835333437127\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 36.24545454545454\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 74.0\n",
      "  episode_reward_mean: 18.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 110\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 30\n",
      "    - 32\n",
      "    - 6\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 16\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 46\n",
      "    - 6\n",
      "    - 28\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 22\n",
      "    - 34\n",
      "    - 12\n",
      "    - 50\n",
      "    - 14\n",
      "    - 30\n",
      "    - 11\n",
      "    - 24\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 20\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 15\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 16\n",
      "    - 6\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 22\n",
      "    - 6\n",
      "    - 10\n",
      "    - 10\n",
      "    - 24\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 15.0\n",
      "    - 73.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 19.0\n",
      "    - 0.0\n",
      "    - 20.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 25.0\n",
      "    - 44.0\n",
      "    - 4.0\n",
      "    - 26.0\n",
      "    - 64.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 64.0\n",
      "    - 40.0\n",
      "    - 17.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 41.0\n",
      "    - 61.0\n",
      "    - 16.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 31.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 29.0\n",
      "    - 2.0\n",
      "    - 39.0\n",
      "    - 38.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015217424177684541\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020638484767167842\n",
      "    mean_inference_ms: 0.23375620397252125\n",
      "    mean_raw_obs_processing_ms: 0.03241835333437127\n",
      "time_since_restore: 37.19466757774353\n",
      "time_this_iter_s: 2.642465114593506\n",
      "time_total_s: 37.19466757774353\n",
      "timers:\n",
      "  learn_throughput: 2839.161\n",
      "  learn_time_ms: 1408.867\n",
      "  load_throughput: 22319031.529\n",
      "  load_time_ms: 0.179\n",
      "  training_iteration_time_ms: 2643.861\n",
      "  update_time_ms: 0.698\n",
      "timestamp: 1656299419\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 56000\n",
      "training_iteration: 14\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 60000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 60000\n",
      "  num_env_steps_trained: 60000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-22\n",
      "done: false\n",
      "episode_len_mean: 38.12380952380953\n",
      "episode_media: {}\n",
      "episode_reward_max: 111.0\n",
      "episode_reward_mean: 21.561904761904763\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 105\n",
      "episodes_total: 1625\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2784111648477534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011348552077067316\n",
      "        policy_loss: -0.011297606873095677\n",
      "        total_loss: 5.825353525274544\n",
      "        vf_explained_var: 0.0023263423673568233\n",
      "        vf_loss: 5.836615668060959\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 60000\n",
      "  num_env_steps_trained: 60000\n",
      "iterations_since_restore: 15\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 60000\n",
      "num_agent_steps_trained: 60000\n",
      "num_env_steps_sampled: 60000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 60000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.05\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015210291558159497\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020621722961365763\n",
      "  mean_inference_ms: 0.2336822782575511\n",
      "  mean_raw_obs_processing_ms: 0.03239717132415265\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 38.12380952380953\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 111.0\n",
      "  episode_reward_mean: 21.561904761904763\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 105\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 30\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 43\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 10\n",
      "    - 23\n",
      "    - 50\n",
      "    - 41\n",
      "    - 34\n",
      "    - 15\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 32\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 15\n",
      "    - 50\n",
      "    - 21\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 27\n",
      "    - 16\n",
      "    - 50\n",
      "    - 25\n",
      "    - 16\n",
      "    - 50\n",
      "    - 38\n",
      "    - 16\n",
      "    - 28\n",
      "    - 19\n",
      "    - 50\n",
      "    - 19\n",
      "    - 24\n",
      "    - 8\n",
      "    - 50\n",
      "    - 44\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 7.0\n",
      "    - 47.0\n",
      "    - 57.0\n",
      "    - 35.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 33.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 46.0\n",
      "    - 111.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 98.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 45.0\n",
      "    - 18.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 28.0\n",
      "    - 67.0\n",
      "    - 9.0\n",
      "    - 53.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 22.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 4.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 45.0\n",
      "    - 28.0\n",
      "    - 1.0\n",
      "    - 7.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 69.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015210291558159497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020621722961365763\n",
      "    mean_inference_ms: 0.2336822782575511\n",
      "    mean_raw_obs_processing_ms: 0.03239717132415265\n",
      "time_since_restore: 39.83455944061279\n",
      "time_this_iter_s: 2.6398918628692627\n",
      "time_total_s: 39.83455944061279\n",
      "timers:\n",
      "  learn_throughput: 2838.392\n",
      "  learn_time_ms: 1409.249\n",
      "  load_throughput: 22384544.363\n",
      "  load_time_ms: 0.179\n",
      "  training_iteration_time_ms: 2643.989\n",
      "  update_time_ms: 0.712\n",
      "timestamp: 1656299422\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 64000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 64000\n",
      "  num_env_steps_trained: 64000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-25\n",
      "done: false\n",
      "episode_len_mean: 39.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 90.0\n",
      "episode_reward_mean: 24.78\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 100\n",
      "episodes_total: 1725\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2630790156702842\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014287438766369896\n",
      "        policy_loss: -0.017198976721133918\n",
      "        total_loss: 6.197906902656761\n",
      "        vf_explained_var: -0.03081157418989366\n",
      "        vf_loss: 6.215061229683699\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 64000\n",
      "  num_env_steps_trained: 64000\n",
      "iterations_since_restore: 16\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 64000\n",
      "num_agent_steps_trained: 64000\n",
      "num_env_steps_sampled: 64000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 64000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.966666666666669\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015203335257582802\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020608535265691955\n",
      "  mean_inference_ms: 0.23360049802249946\n",
      "  mean_raw_obs_processing_ms: 0.032359053478734794\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 39.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 90.0\n",
      "  episode_reward_mean: 24.78\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 15\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 33\n",
      "    - 16\n",
      "    - 41\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 13\n",
      "    - 47\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 24\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 13\n",
      "    - 46\n",
      "    - 37\n",
      "    - 49\n",
      "    - 50\n",
      "    - 21\n",
      "    - 18\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 40.0\n",
      "    - 23.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 58.0\n",
      "    - 41.0\n",
      "    - 33.0\n",
      "    - 70.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 26.0\n",
      "    - 57.0\n",
      "    - 77.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 86.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 27.0\n",
      "    - 57.0\n",
      "    - 43.0\n",
      "    - 6.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 23.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 62.0\n",
      "    - 69.0\n",
      "    - 73.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 3.0\n",
      "    - 45.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 90.0\n",
      "    - 14.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 34.0\n",
      "    - 51.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015203335257582802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020608535265691955\n",
      "    mean_inference_ms: 0.23360049802249946\n",
      "    mean_raw_obs_processing_ms: 0.032359053478734794\n",
      "time_since_restore: 42.47452640533447\n",
      "time_this_iter_s: 2.6399669647216797\n",
      "time_total_s: 42.47452640533447\n",
      "timers:\n",
      "  learn_throughput: 2838.476\n",
      "  learn_time_ms: 1409.207\n",
      "  load_throughput: 22402478.302\n",
      "  load_time_ms: 0.179\n",
      "  training_iteration_time_ms: 2643.64\n",
      "  update_time_ms: 0.72\n",
      "timestamp: 1656299425\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 64000\n",
      "training_iteration: 16\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 68000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_env_steps_sampled: 68000\n",
      "  num_env_steps_trained: 68000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-27\n",
      "done: false\n",
      "episode_len_mean: 41.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 106.0\n",
      "episode_reward_mean: 23.93\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 96\n",
      "episodes_total: 1821\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2663237469170683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011182174027486835\n",
      "        policy_loss: -0.002815889977719835\n",
      "        total_loss: 6.538489882151286\n",
      "        vf_explained_var: 0.008615353607362316\n",
      "        vf_loss: 6.541270832685373\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_env_steps_sampled: 68000\n",
      "  num_env_steps_trained: 68000\n",
      "iterations_since_restore: 17\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 68000\n",
      "num_agent_steps_trained: 68000\n",
      "num_env_steps_sampled: 68000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 68000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.575\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015200091781951797\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020597176001654484\n",
      "  mean_inference_ms: 0.23355007403216482\n",
      "  mean_raw_obs_processing_ms: 0.03232402218234959\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.0\n",
      "  episode_reward_mean: 23.93\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 96\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 14\n",
      "    - 40\n",
      "    - 50\n",
      "    - 17\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 13\n",
      "    - 31\n",
      "    - 50\n",
      "    - 47\n",
      "    - 14\n",
      "    - 50\n",
      "    - 21\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 17\n",
      "    - 8\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 73.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 96.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 14.0\n",
      "    - 68.0\n",
      "    - 65.0\n",
      "    - 15.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 51.0\n",
      "    - 8.0\n",
      "    - 106.0\n",
      "    - 0.0\n",
      "    - 13.0\n",
      "    - 61.0\n",
      "    - 8.0\n",
      "    - 45.0\n",
      "    - 43.0\n",
      "    - 35.0\n",
      "    - 26.0\n",
      "    - 45.0\n",
      "    - 55.0\n",
      "    - 59.0\n",
      "    - 34.0\n",
      "    - 39.0\n",
      "    - 41.0\n",
      "    - 37.0\n",
      "    - 24.0\n",
      "    - 58.0\n",
      "    - 61.0\n",
      "    - 54.0\n",
      "    - 52.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 55.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 87.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 23.0\n",
      "    - 25.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 50.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 16.0\n",
      "    - 27.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015200091781951797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020597176001654484\n",
      "    mean_inference_ms: 0.23355007403216482\n",
      "    mean_raw_obs_processing_ms: 0.03232402218234959\n",
      "time_since_restore: 45.109302282333374\n",
      "time_this_iter_s: 2.6347758769989014\n",
      "time_total_s: 45.109302282333374\n",
      "timers:\n",
      "  learn_throughput: 2839.104\n",
      "  learn_time_ms: 1408.895\n",
      "  load_throughput: 22107281.592\n",
      "  load_time_ms: 0.181\n",
      "  training_iteration_time_ms: 2642.541\n",
      "  update_time_ms: 0.705\n",
      "timestamp: 1656299427\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 72000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 72000\n",
      "  num_env_steps_trained: 72000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-30\n",
      "done: false\n",
      "episode_len_mean: 40.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 89.0\n",
      "episode_reward_mean: 25.34\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 98\n",
      "episodes_total: 1919\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2612143453731333\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013063147551910851\n",
      "        policy_loss: 0.00047264739129972713\n",
      "        total_loss: 5.998597286849893\n",
      "        vf_explained_var: 0.007840971728806854\n",
      "        vf_loss: 5.998083802753238\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 72000\n",
      "  num_env_steps_trained: 72000\n",
      "iterations_since_restore: 18\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 72000\n",
      "num_agent_steps_trained: 72000\n",
      "num_env_steps_sampled: 72000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 72000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.55\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015192142588063777\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020585807838258523\n",
      "  mean_inference_ms: 0.23349464704019007\n",
      "  mean_raw_obs_processing_ms: 0.03228749079412234\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 40.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 89.0\n",
      "  episode_reward_mean: 25.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 98\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 5\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 14\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 21\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 44\n",
      "    - 36\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 24\n",
      "    - 25\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 26\n",
      "    - 8\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 1.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 89.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 86.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 64.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 19.0\n",
      "    - 76.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 67.0\n",
      "    - 7.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 74.0\n",
      "    - 74.0\n",
      "    - 37.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 36.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 22.0\n",
      "    - 3.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 22.0\n",
      "    - 41.0\n",
      "    - 53.0\n",
      "    - 20.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 27.0\n",
      "    - 31.0\n",
      "    - 43.0\n",
      "    - 3.0\n",
      "    - 2.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 65.0\n",
      "    - 25.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015192142588063777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020585807838258523\n",
      "    mean_inference_ms: 0.23349464704019007\n",
      "    mean_raw_obs_processing_ms: 0.03228749079412234\n",
      "time_since_restore: 47.75945043563843\n",
      "time_this_iter_s: 2.6501481533050537\n",
      "time_total_s: 47.75945043563843\n",
      "timers:\n",
      "  learn_throughput: 2835.414\n",
      "  learn_time_ms: 1410.729\n",
      "  load_throughput: 22107281.592\n",
      "  load_time_ms: 0.181\n",
      "  training_iteration_time_ms: 2644.545\n",
      "  update_time_ms: 0.701\n",
      "timestamp: 1656299430\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 72000\n",
      "training_iteration: 18\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 76000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_env_steps_sampled: 76000\n",
      "  num_env_steps_trained: 76000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-33\n",
      "done: false\n",
      "episode_len_mean: 41.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 90.0\n",
      "episode_reward_mean: 27.73\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 97\n",
      "episodes_total: 2016\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2484896309914129\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015174826361759696\n",
      "        policy_loss: -0.022982171768703128\n",
      "        total_loss: 6.397416086735264\n",
      "        vf_explained_var: 0.0019010982846701017\n",
      "        vf_loss: 6.420350857768008\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_env_steps_sampled: 76000\n",
      "  num_env_steps_trained: 76000\n",
      "iterations_since_restore: 19\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 76000\n",
      "num_agent_steps_trained: 76000\n",
      "num_env_steps_sampled: 76000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 76000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.624999999999998\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015188553176609046\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02057874554368983\n",
      "  mean_inference_ms: 0.2334745347277746\n",
      "  mean_raw_obs_processing_ms: 0.03225181112811856\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 90.0\n",
      "  episode_reward_mean: 27.73\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 97\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 23\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 37\n",
      "    - 49\n",
      "    - 26\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 4\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 29\n",
      "    - 23\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 37\n",
      "    - 12\n",
      "    - 29\n",
      "    - 41\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 17\n",
      "    - 40\n",
      "    - 6\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 84.0\n",
      "    - 65.0\n",
      "    - 25.0\n",
      "    - 81.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 13.0\n",
      "    - 1.0\n",
      "    - 2.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 68.0\n",
      "    - 31.0\n",
      "    - 42.0\n",
      "    - 37.0\n",
      "    - 25.0\n",
      "    - 30.0\n",
      "    - 58.0\n",
      "    - 20.0\n",
      "    - 58.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 76.0\n",
      "    - 1.0\n",
      "    - 8.0\n",
      "    - 42.0\n",
      "    - 56.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 28.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 42.0\n",
      "    - 25.0\n",
      "    - 81.0\n",
      "    - 72.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 22.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 14.0\n",
      "    - 14.0\n",
      "    - 56.0\n",
      "    - 22.0\n",
      "    - 6.0\n",
      "    - 90.0\n",
      "    - 30.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 59.0\n",
      "    - 36.0\n",
      "    - 42.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015188553176609046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02057874554368983\n",
      "    mean_inference_ms: 0.2334745347277746\n",
      "    mean_raw_obs_processing_ms: 0.03225181112811856\n",
      "time_since_restore: 50.40073347091675\n",
      "time_this_iter_s: 2.6412830352783203\n",
      "time_total_s: 50.40073347091675\n",
      "timers:\n",
      "  learn_throughput: 2840.174\n",
      "  learn_time_ms: 1408.365\n",
      "  load_throughput: 22023124.18\n",
      "  load_time_ms: 0.182\n",
      "  training_iteration_time_ms: 2641.912\n",
      "  update_time_ms: 0.695\n",
      "timestamp: 1656299433\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 80000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 80000\n",
      "  num_env_steps_trained: 80000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-35\n",
      "done: false\n",
      "episode_len_mean: 41.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 86.0\n",
      "episode_reward_mean: 27.08\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 96\n",
      "episodes_total: 2112\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2404530153479627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013449679913690253\n",
      "        policy_loss: 0.0014599712044801763\n",
      "        total_loss: 6.391987617554203\n",
      "        vf_explained_var: 0.0022440502720494425\n",
      "        vf_loss: 6.390485621460022\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 80000\n",
      "  num_env_steps_trained: 80000\n",
      "iterations_since_restore: 20\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 80000\n",
      "num_agent_steps_trained: 80000\n",
      "num_env_steps_sampled: 80000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 80000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.733333333333334\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015183111762162868\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020570561829819395\n",
      "  mean_inference_ms: 0.23342623988546052\n",
      "  mean_raw_obs_processing_ms: 0.03221968102229624\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 86.0\n",
      "  episode_reward_mean: 27.08\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 96\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 11\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 14\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 25\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 30\n",
      "    - 40\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 22\n",
      "    - 7\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 28\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 36.0\n",
      "    - 42.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 75.0\n",
      "    - 28.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 4.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 37.0\n",
      "    - 65.0\n",
      "    - 86.0\n",
      "    - 35.0\n",
      "    - 11.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 49.0\n",
      "    - 40.0\n",
      "    - 43.0\n",
      "    - 13.0\n",
      "    - 37.0\n",
      "    - 7.0\n",
      "    - 46.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 43.0\n",
      "    - 38.0\n",
      "    - 20.0\n",
      "    - 42.0\n",
      "    - 60.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 79.0\n",
      "    - 59.0\n",
      "    - 23.0\n",
      "    - 6.0\n",
      "    - 67.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 24.0\n",
      "    - 42.0\n",
      "    - 54.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 37.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 75.0\n",
      "    - 7.0\n",
      "    - 34.0\n",
      "    - 27.0\n",
      "    - 48.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 10.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015183111762162868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020570561829819395\n",
      "    mean_inference_ms: 0.23342623988546052\n",
      "    mean_raw_obs_processing_ms: 0.03221968102229624\n",
      "time_since_restore: 53.00929117202759\n",
      "time_this_iter_s: 2.60855770111084\n",
      "time_total_s: 53.00929117202759\n",
      "timers:\n",
      "  learn_throughput: 2844.505\n",
      "  learn_time_ms: 1406.22\n",
      "  load_throughput: 21746229.423\n",
      "  load_time_ms: 0.184\n",
      "  training_iteration_time_ms: 2638.534\n",
      "  update_time_ms: 0.681\n",
      "timestamp: 1656299435\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 80000\n",
      "training_iteration: 20\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 84000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_env_steps_sampled: 84000\n",
      "  num_env_steps_trained: 84000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-38\n",
      "done: false\n",
      "episode_len_mean: 41.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 87.0\n",
      "episode_reward_mean: 28.16\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 98\n",
      "episodes_total: 2210\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2420592061934932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013785809544866443\n",
      "        policy_loss: -0.017702913649820833\n",
      "        total_loss: 6.88372769483956\n",
      "        vf_explained_var: 0.0030736877072241996\n",
      "        vf_loss: 6.901387506915677\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_env_steps_sampled: 84000\n",
      "  num_env_steps_trained: 84000\n",
      "iterations_since_restore: 21\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 84000\n",
      "num_agent_steps_trained: 84000\n",
      "num_env_steps_sampled: 84000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 84000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.2\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015178331618596616\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020554666550960065\n",
      "  mean_inference_ms: 0.23335241082451913\n",
      "  mean_raw_obs_processing_ms: 0.03218381628580578\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 87.0\n",
      "  episode_reward_mean: 28.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 98\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 12\n",
      "    - 15\n",
      "    - 32\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 4\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 28\n",
      "    - 20\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 29\n",
      "    - 50\n",
      "    - 23\n",
      "    - 23\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 32\n",
      "    - 19\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 10.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 48.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 2.0\n",
      "    - 74.0\n",
      "    - 42.0\n",
      "    - 82.0\n",
      "    - 68.0\n",
      "    - 71.0\n",
      "    - 42.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 44.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 77.0\n",
      "    - 1.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 67.0\n",
      "    - 5.0\n",
      "    - 83.0\n",
      "    - 14.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 21.0\n",
      "    - 40.0\n",
      "    - 34.0\n",
      "    - 62.0\n",
      "    - 14.0\n",
      "    - 43.0\n",
      "    - 7.0\n",
      "    - 70.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 1.0\n",
      "    - 30.0\n",
      "    - 87.0\n",
      "    - 23.0\n",
      "    - 53.0\n",
      "    - 1.0\n",
      "    - 25.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 17.0\n",
      "    - 41.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 45.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 38.0\n",
      "    - 29.0\n",
      "    - 72.0\n",
      "    - 29.0\n",
      "    - 41.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015178331618596616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020554666550960065\n",
      "    mean_inference_ms: 0.23335241082451913\n",
      "    mean_raw_obs_processing_ms: 0.03218381628580578\n",
      "time_since_restore: 55.65518808364868\n",
      "time_this_iter_s: 2.6458969116210938\n",
      "time_total_s: 55.65518808364868\n",
      "timers:\n",
      "  learn_throughput: 2844.747\n",
      "  learn_time_ms: 1406.1\n",
      "  load_throughput: 23330852.454\n",
      "  load_time_ms: 0.171\n",
      "  training_iteration_time_ms: 2637.957\n",
      "  update_time_ms: 0.679\n",
      "timestamp: 1656299438\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 88000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_env_steps_sampled: 88000\n",
      "  num_env_steps_trained: 88000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-41\n",
      "done: false\n",
      "episode_len_mean: 42.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 100.0\n",
      "episode_reward_mean: 27.35\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 95\n",
      "episodes_total: 2305\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2332089981725138\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014625228885110588\n",
      "        policy_loss: -0.016310439706449548\n",
      "        total_loss: 6.3276917364007685\n",
      "        vf_explained_var: 0.002737910452709403\n",
      "        vf_loss: 6.34395644418014\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_env_steps_sampled: 88000\n",
      "  num_env_steps_trained: 88000\n",
      "iterations_since_restore: 22\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 88000\n",
      "num_agent_steps_trained: 88000\n",
      "num_env_steps_sampled: 88000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 88000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.95\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015181353970446789\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020561319446477055\n",
      "  mean_inference_ms: 0.23340497609893454\n",
      "  mean_raw_obs_processing_ms: 0.03217570590794612\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.0\n",
      "  episode_reward_mean: 27.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 95\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 30\n",
      "    - 36\n",
      "    - 50\n",
      "    - 31\n",
      "    - 48\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 38\n",
      "    - 33\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 15\n",
      "    - 16\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 23\n",
      "    - 26\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 10\n",
      "    - 49\n",
      "    episode_reward:\n",
      "    - 38.0\n",
      "    - 29.0\n",
      "    - 72.0\n",
      "    - 29.0\n",
      "    - 41.0\n",
      "    - 42.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 62.0\n",
      "    - 46.0\n",
      "    - 24.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 13.0\n",
      "    - 50.0\n",
      "    - 83.0\n",
      "    - 66.0\n",
      "    - 38.0\n",
      "    - 55.0\n",
      "    - 50.0\n",
      "    - 12.0\n",
      "    - 56.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 11.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 51.0\n",
      "    - 37.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 1.0\n",
      "    - 73.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 34.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 81.0\n",
      "    - 36.0\n",
      "    - 73.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 100.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 6.0\n",
      "    - 35.0\n",
      "    - 32.0\n",
      "    - 27.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 82.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015181353970446789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020561319446477055\n",
      "    mean_inference_ms: 0.23340497609893454\n",
      "    mean_raw_obs_processing_ms: 0.03217570590794612\n",
      "time_since_restore: 58.309993267059326\n",
      "time_this_iter_s: 2.6548051834106445\n",
      "time_total_s: 58.309993267059326\n",
      "timers:\n",
      "  learn_throughput: 2845.403\n",
      "  learn_time_ms: 1405.776\n",
      "  load_throughput: 23649867.494\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2638.954\n",
      "  update_time_ms: 0.656\n",
      "timestamp: 1656299441\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 88000\n",
      "training_iteration: 22\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 92000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_env_steps_sampled: 92000\n",
      "  num_env_steps_trained: 92000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-43\n",
      "done: false\n",
      "episode_len_mean: 43.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 85.0\n",
      "episode_reward_mean: 30.38\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 92\n",
      "episodes_total: 2397\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2233328264246706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014670343277475207\n",
      "        policy_loss: -0.01450362426719518\n",
      "        total_loss: 7.192753076553345\n",
      "        vf_explained_var: 0.0005176674294215377\n",
      "        vf_loss: 7.207210852638368\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_env_steps_sampled: 92000\n",
      "  num_env_steps_trained: 92000\n",
      "iterations_since_restore: 23\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 92000\n",
      "num_agent_steps_trained: 92000\n",
      "num_env_steps_sampled: 92000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 92000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.5\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015178336024014243\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020555075391417252\n",
      "  mean_inference_ms: 0.23338455186467688\n",
      "  mean_raw_obs_processing_ms: 0.032149449881746145\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 85.0\n",
      "  episode_reward_mean: 30.38\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 92\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 10\n",
      "    - 49\n",
      "    - 41\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 15\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 12\n",
      "    - 28\n",
      "    - 24\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 48\n",
      "    episode_reward:\n",
      "    - 27.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 82.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 26.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 31.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 85.0\n",
      "    - 53.0\n",
      "    - 3.0\n",
      "    - 39.0\n",
      "    - 31.0\n",
      "    - 51.0\n",
      "    - 23.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 51.0\n",
      "    - 47.0\n",
      "    - 57.0\n",
      "    - 2.0\n",
      "    - 23.0\n",
      "    - 35.0\n",
      "    - 62.0\n",
      "    - 31.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 46.0\n",
      "    - 1.0\n",
      "    - 56.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 54.0\n",
      "    - 66.0\n",
      "    - 13.0\n",
      "    - 33.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 32.0\n",
      "    - 44.0\n",
      "    - 38.0\n",
      "    - 52.0\n",
      "    - 36.0\n",
      "    - 27.0\n",
      "    - 63.0\n",
      "    - 48.0\n",
      "    - 42.0\n",
      "    - 60.0\n",
      "    - 18.0\n",
      "    - 73.0\n",
      "    - 12.0\n",
      "    - 30.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 29.0\n",
      "    - 38.0\n",
      "    - 37.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 46.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 59.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 34.0\n",
      "    - 79.0\n",
      "    - 17.0\n",
      "    - 20.0\n",
      "    - 6.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015178336024014243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020555075391417252\n",
      "    mean_inference_ms: 0.23338455186467688\n",
      "    mean_raw_obs_processing_ms: 0.032149449881746145\n",
      "time_since_restore: 60.944807052612305\n",
      "time_this_iter_s: 2.6348137855529785\n",
      "time_total_s: 60.944807052612305\n",
      "timers:\n",
      "  learn_throughput: 2849.328\n",
      "  learn_time_ms: 1403.84\n",
      "  load_throughput: 22705665.178\n",
      "  load_time_ms: 0.176\n",
      "  training_iteration_time_ms: 2636.731\n",
      "  update_time_ms: 0.645\n",
      "timestamp: 1656299443\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 96000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_env_steps_sampled: 96000\n",
      "  num_env_steps_trained: 96000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-46\n",
      "done: false\n",
      "episode_len_mean: 43.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 80.0\n",
      "episode_reward_mean: 28.17\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 93\n",
      "episodes_total: 2490\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2181006937898615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014519350654295375\n",
      "        policy_loss: -0.0040564252023575125\n",
      "        total_loss: 7.408592812732984\n",
      "        vf_explained_var: 0.002544748270383445\n",
      "        vf_loss: 7.412603899740404\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_env_steps_sampled: 96000\n",
      "  num_env_steps_trained: 96000\n",
      "iterations_since_restore: 24\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 96000\n",
      "num_agent_steps_trained: 96000\n",
      "num_env_steps_sampled: 96000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 96000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.05\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015175676741188013\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020544988683689258\n",
      "  mean_inference_ms: 0.23334808444891486\n",
      "  mean_raw_obs_processing_ms: 0.0321212204070396\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 80.0\n",
      "  episode_reward_mean: 28.17\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 93\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 48\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 13\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 45\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 17\n",
      "    - 30\n",
      "    - 50\n",
      "    - 31\n",
      "    - 18\n",
      "    - 21\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 79.0\n",
      "    - 17.0\n",
      "    - 20.0\n",
      "    - 6.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 21.0\n",
      "    - 34.0\n",
      "    - 13.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 60.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 41.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 4.0\n",
      "    - 10.0\n",
      "    - 17.0\n",
      "    - 7.0\n",
      "    - 15.0\n",
      "    - 74.0\n",
      "    - 55.0\n",
      "    - 60.0\n",
      "    - 43.0\n",
      "    - 51.0\n",
      "    - 9.0\n",
      "    - 70.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 54.0\n",
      "    - 38.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 25.0\n",
      "    - 71.0\n",
      "    - 12.0\n",
      "    - 47.0\n",
      "    - 22.0\n",
      "    - 54.0\n",
      "    - 49.0\n",
      "    - 6.0\n",
      "    - 39.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 13.0\n",
      "    - 15.0\n",
      "    - 60.0\n",
      "    - 60.0\n",
      "    - 31.0\n",
      "    - 26.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 2.0\n",
      "    - 43.0\n",
      "    - 5.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 20.0\n",
      "    - 59.0\n",
      "    - 44.0\n",
      "    - 64.0\n",
      "    - 46.0\n",
      "    - 44.0\n",
      "    - 43.0\n",
      "    - 56.0\n",
      "    - 22.0\n",
      "    - 30.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015175676741188013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020544988683689258\n",
      "    mean_inference_ms: 0.23334808444891486\n",
      "    mean_raw_obs_processing_ms: 0.0321212204070396\n",
      "time_since_restore: 63.63477897644043\n",
      "time_this_iter_s: 2.689971923828125\n",
      "time_total_s: 63.63477897644043\n",
      "timers:\n",
      "  learn_throughput: 2838.222\n",
      "  learn_time_ms: 1409.333\n",
      "  load_throughput: 22847904.126\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2641.475\n",
      "  update_time_ms: 0.641\n",
      "timestamp: 1656299446\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 96000\n",
      "training_iteration: 24\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 100000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_env_steps_sampled: 100000\n",
      "  num_env_steps_trained: 100000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-49\n",
      "done: false\n",
      "episode_len_mean: 41.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 111.0\n",
      "episode_reward_mean: 29.45\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 98\n",
      "episodes_total: 2588\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2148209951257194\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014489650755860312\n",
      "        policy_loss: -0.00906407744373365\n",
      "        total_loss: 7.198824554874051\n",
      "        vf_explained_var: 0.0023728355925570254\n",
      "        vf_loss: 7.207843352133228\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_env_steps_sampled: 100000\n",
      "  num_env_steps_trained: 100000\n",
      "iterations_since_restore: 25\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 100000\n",
      "num_agent_steps_trained: 100000\n",
      "num_env_steps_sampled: 100000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 100000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.5\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015169783318698267\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02053391582617906\n",
      "  mean_inference_ms: 0.23330496163955822\n",
      "  mean_raw_obs_processing_ms: 0.0320943677505652\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 111.0\n",
      "  episode_reward_mean: 29.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 98\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 4\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 21\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 50.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 60.0\n",
      "    - 95.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 14.0\n",
      "    - 45.0\n",
      "    - 31.0\n",
      "    - 45.0\n",
      "    - 76.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 20.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 70.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 26.0\n",
      "    - 64.0\n",
      "    - 40.0\n",
      "    - 18.0\n",
      "    - 2.0\n",
      "    - 111.0\n",
      "    - 44.0\n",
      "    - 10.0\n",
      "    - 52.0\n",
      "    - 57.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 99.0\n",
      "    - 4.0\n",
      "    - 8.0\n",
      "    - 36.0\n",
      "    - 42.0\n",
      "    - 48.0\n",
      "    - 65.0\n",
      "    - 92.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 34.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 30.0\n",
      "    - 15.0\n",
      "    - 83.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 45.0\n",
      "    - 35.0\n",
      "    - 40.0\n",
      "    - 8.0\n",
      "    - 61.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 76.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 57.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 47.0\n",
      "    - 60.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015169783318698267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02053391582617906\n",
      "    mean_inference_ms: 0.23330496163955822\n",
      "    mean_raw_obs_processing_ms: 0.0320943677505652\n",
      "time_since_restore: 66.26707005500793\n",
      "time_this_iter_s: 2.632291078567505\n",
      "time_total_s: 66.26707005500793\n",
      "timers:\n",
      "  learn_throughput: 2838.839\n",
      "  learn_time_ms: 1409.027\n",
      "  load_throughput: 22795130.435\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2640.718\n",
      "  update_time_ms: 0.641\n",
      "timestamp: 1656299449\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 104000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_env_steps_sampled: 104000\n",
      "  num_env_steps_trained: 104000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-51\n",
      "done: false\n",
      "episode_len_mean: 39.09803921568628\n",
      "episode_media: {}\n",
      "episode_reward_max: 79.0\n",
      "episode_reward_mean: 25.607843137254903\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 102\n",
      "episodes_total: 2690\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2216008615750138\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01370248016300461\n",
      "        policy_loss: -0.009832596797896649\n",
      "        total_loss: 6.6741507366139405\n",
      "        vf_explained_var: 0.0054686282911608295\n",
      "        vf_loss: 6.683940501302801\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_env_steps_sampled: 104000\n",
      "  num_env_steps_trained: 104000\n",
      "iterations_since_restore: 26\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 104000\n",
      "num_agent_steps_trained: 104000\n",
      "num_env_steps_sampled: 104000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 104000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.200000000000001\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01516775072043328\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02052169333462431\n",
      "  mean_inference_ms: 0.2332643949486091\n",
      "  mean_raw_obs_processing_ms: 0.0320881431207587\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 39.09803921568628\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 79.0\n",
      "  episode_reward_mean: 25.607843137254903\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 102\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 45\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 19\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 21\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 15\n",
      "    - 24\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 24\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 11\n",
      "    - 9\n",
      "    - 8\n",
      "    - 22\n",
      "    - 39\n",
      "    - 50\n",
      "    - 12\n",
      "    - 25\n",
      "    - 30\n",
      "    - 29\n",
      "    - 50\n",
      "    - 13\n",
      "    - 8\n",
      "    - 16\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 6\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 46.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 54.0\n",
      "    - 7.0\n",
      "    - 1.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 30.0\n",
      "    - 61.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 9.0\n",
      "    - 4.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 43.0\n",
      "    - 13.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 32.0\n",
      "    - 79.0\n",
      "    - 63.0\n",
      "    - 41.0\n",
      "    - 43.0\n",
      "    - 29.0\n",
      "    - 71.0\n",
      "    - 5.0\n",
      "    - 50.0\n",
      "    - 25.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 13.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 46.0\n",
      "    - 26.0\n",
      "    - 38.0\n",
      "    - 61.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 68.0\n",
      "    - 60.0\n",
      "    - 61.0\n",
      "    - 31.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 43.0\n",
      "    - 61.0\n",
      "    - 34.0\n",
      "    - 46.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01516775072043328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02052169333462431\n",
      "    mean_inference_ms: 0.2332643949486091\n",
      "    mean_raw_obs_processing_ms: 0.0320881431207587\n",
      "time_since_restore: 68.89900398254395\n",
      "time_this_iter_s: 2.6319339275360107\n",
      "time_total_s: 68.89900398254395\n",
      "timers:\n",
      "  learn_throughput: 2840.153\n",
      "  learn_time_ms: 1408.375\n",
      "  load_throughput: 22764200.814\n",
      "  load_time_ms: 0.176\n",
      "  training_iteration_time_ms: 2639.94\n",
      "  update_time_ms: 0.648\n",
      "timestamp: 1656299451\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 104000\n",
      "training_iteration: 26\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 108000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 108000\n",
      "  num_agent_steps_trained: 108000\n",
      "  num_env_steps_sampled: 108000\n",
      "  num_env_steps_trained: 108000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-54\n",
      "done: false\n",
      "episode_len_mean: 42.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 87.0\n",
      "episode_reward_mean: 29.08\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 94\n",
      "episodes_total: 2784\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.224208914849066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013025367404184033\n",
      "        policy_loss: -0.012466508327853135\n",
      "        total_loss: 7.224183852185485\n",
      "        vf_explained_var: 0.0027982286227646695\n",
      "        vf_loss: 7.236609623509069\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 108000\n",
      "  num_agent_steps_trained: 108000\n",
      "  num_env_steps_sampled: 108000\n",
      "  num_env_steps_trained: 108000\n",
      "iterations_since_restore: 27\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 108000\n",
      "num_agent_steps_trained: 108000\n",
      "num_env_steps_sampled: 108000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 108000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.975\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01516693389652557\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020522424017103736\n",
      "  mean_inference_ms: 0.23325989375609887\n",
      "  mean_raw_obs_processing_ms: 0.032074726094445914\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 87.0\n",
      "  episode_reward_mean: 29.08\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 94\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 31\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 48\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 21\n",
      "    - 17\n",
      "    - 50\n",
      "    - 18\n",
      "    - 11\n",
      "    - 15\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 27\n",
      "    - 44\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 43.0\n",
      "    - 61.0\n",
      "    - 34.0\n",
      "    - 46.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 32.0\n",
      "    - 35.0\n",
      "    - 24.0\n",
      "    - 54.0\n",
      "    - 81.0\n",
      "    - 45.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 87.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 15.0\n",
      "    - 30.0\n",
      "    - 34.0\n",
      "    - 26.0\n",
      "    - 7.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 22.0\n",
      "    - 38.0\n",
      "    - 45.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 57.0\n",
      "    - 30.0\n",
      "    - 61.0\n",
      "    - 58.0\n",
      "    - 23.0\n",
      "    - 48.0\n",
      "    - 45.0\n",
      "    - 36.0\n",
      "    - 66.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 49.0\n",
      "    - 6.0\n",
      "    - 34.0\n",
      "    - 28.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 30.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 55.0\n",
      "    - 59.0\n",
      "    - 40.0\n",
      "    - 46.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 10.0\n",
      "    - 62.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 18.0\n",
      "    - 63.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 34.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 71.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01516693389652557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020522424017103736\n",
      "    mean_inference_ms: 0.23325989375609887\n",
      "    mean_raw_obs_processing_ms: 0.032074726094445914\n",
      "time_since_restore: 71.53856897354126\n",
      "time_this_iter_s: 2.6395649909973145\n",
      "time_total_s: 71.53856897354126\n",
      "timers:\n",
      "  learn_throughput: 2839.468\n",
      "  learn_time_ms: 1408.715\n",
      "  load_throughput: 22841682.777\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2640.419\n",
      "  update_time_ms: 0.655\n",
      "timestamp: 1656299454\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 108000\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 112000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 112000\n",
      "  num_agent_steps_trained: 112000\n",
      "  num_env_steps_sampled: 112000\n",
      "  num_env_steps_trained: 112000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-57\n",
      "done: false\n",
      "episode_len_mean: 41.65\n",
      "episode_media: {}\n",
      "episode_reward_max: 96.0\n",
      "episode_reward_mean: 27.84\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 95\n",
      "episodes_total: 2879\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.212884009140794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013608702340273447\n",
      "        policy_loss: -0.0028478306267530687\n",
      "        total_loss: 6.524292273290696\n",
      "        vf_explained_var: 0.0040159171627413845\n",
      "        vf_loss: 6.527097583233669\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 112000\n",
      "  num_agent_steps_trained: 112000\n",
      "  num_env_steps_sampled: 112000\n",
      "  num_env_steps_trained: 112000\n",
      "iterations_since_restore: 28\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 112000\n",
      "num_agent_steps_trained: 112000\n",
      "num_env_steps_sampled: 112000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 112000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.075\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015163666226054542\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02051458593296787\n",
      "  mean_inference_ms: 0.23323023117350555\n",
      "  mean_raw_obs_processing_ms: 0.03206226736964866\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 96.0\n",
      "  episode_reward_mean: 27.84\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 95\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 12\n",
      "    - 27\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 44\n",
      "    - 35\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 30\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 21\n",
      "    - 10\n",
      "    - 43\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 11\n",
      "    - 22\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 34\n",
      "    - 41\n",
      "    - 25\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 71.0\n",
      "    - 28.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 91.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 90.0\n",
      "    - 25.0\n",
      "    - 53.0\n",
      "    - 52.0\n",
      "    - 54.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 12.0\n",
      "    - 0.0\n",
      "    - 79.0\n",
      "    - 7.0\n",
      "    - 26.0\n",
      "    - 52.0\n",
      "    - 36.0\n",
      "    - 35.0\n",
      "    - 53.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 51.0\n",
      "    - 15.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 42.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 5.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 70.0\n",
      "    - 62.0\n",
      "    - 96.0\n",
      "    - 28.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 29.0\n",
      "    - 33.0\n",
      "    - 64.0\n",
      "    - 36.0\n",
      "    - 49.0\n",
      "    - 36.0\n",
      "    - 56.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 14.0\n",
      "    - 44.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 31.0\n",
      "    - 46.0\n",
      "    - 15.0\n",
      "    - 5.0\n",
      "    - 95.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 5.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015163666226054542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02051458593296787\n",
      "    mean_inference_ms: 0.23323023117350555\n",
      "    mean_raw_obs_processing_ms: 0.03206226736964866\n",
      "time_since_restore: 74.15698981285095\n",
      "time_this_iter_s: 2.6184208393096924\n",
      "time_total_s: 74.15698981285095\n",
      "timers:\n",
      "  learn_throughput: 2845.717\n",
      "  learn_time_ms: 1405.621\n",
      "  load_throughput: 22857242.507\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2637.248\n",
      "  update_time_ms: 0.64\n",
      "timestamp: 1656299457\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 112000\n",
      "training_iteration: 28\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 116000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 116000\n",
      "  num_agent_steps_trained: 116000\n",
      "  num_env_steps_sampled: 116000\n",
      "  num_env_steps_trained: 116000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-10-59\n",
      "done: false\n",
      "episode_len_mean: 41.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 107.0\n",
      "episode_reward_mean: 31.56\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 96\n",
      "episodes_total: 2975\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.2060907087018413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012596132082524159\n",
      "        policy_loss: -0.0012449367562689447\n",
      "        total_loss: 7.277078656740086\n",
      "        vf_explained_var: 0.0023957383889023974\n",
      "        vf_loss: 7.278284248228996\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 116000\n",
      "  num_agent_steps_trained: 116000\n",
      "  num_env_steps_sampled: 116000\n",
      "  num_env_steps_trained: 116000\n",
      "iterations_since_restore: 29\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 116000\n",
      "num_agent_steps_trained: 116000\n",
      "num_env_steps_sampled: 116000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 116000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.566666666666668\n",
      "  ram_util_percent: 52.29999999999999\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015160935379305936\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02050543666101097\n",
      "  mean_inference_ms: 0.23320284333204108\n",
      "  mean_raw_obs_processing_ms: 0.03204264915604779\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 107.0\n",
      "  episode_reward_mean: 31.56\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 96\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 13\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 26\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 9\n",
      "    - 33\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 5.0\n",
      "    - 71.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 27.0\n",
      "    - 21.0\n",
      "    - 23.0\n",
      "    - 48.0\n",
      "    - 39.0\n",
      "    - 41.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 85.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 84.0\n",
      "    - 28.0\n",
      "    - 22.0\n",
      "    - 44.0\n",
      "    - 65.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 46.0\n",
      "    - 79.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 85.0\n",
      "    - 14.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 100.0\n",
      "    - 56.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 59.0\n",
      "    - 2.0\n",
      "    - 4.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 68.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 25.0\n",
      "    - 107.0\n",
      "    - 12.0\n",
      "    - 55.0\n",
      "    - 29.0\n",
      "    - 57.0\n",
      "    - 38.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 2.0\n",
      "    - 30.0\n",
      "    - 60.0\n",
      "    - 39.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 46.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 8.0\n",
      "    - 36.0\n",
      "    - 32.0\n",
      "    - 76.0\n",
      "    - 35.0\n",
      "    - 64.0\n",
      "    - 63.0\n",
      "    - 11.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015160935379305936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02050543666101097\n",
      "    mean_inference_ms: 0.23320284333204108\n",
      "    mean_raw_obs_processing_ms: 0.03204264915604779\n",
      "time_since_restore: 76.78272080421448\n",
      "time_this_iter_s: 2.6257309913635254\n",
      "time_total_s: 76.78272080421448\n",
      "timers:\n",
      "  learn_throughput: 2848.108\n",
      "  learn_time_ms: 1404.441\n",
      "  load_throughput: 22907176.406\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2635.676\n",
      "  update_time_ms: 0.657\n",
      "timestamp: 1656299459\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 116000\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 120000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 120000\n",
      "  num_agent_steps_trained: 120000\n",
      "  num_env_steps_sampled: 120000\n",
      "  num_env_steps_trained: 120000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-02\n",
      "done: false\n",
      "episode_len_mean: 42.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 98.0\n",
      "episode_reward_mean: 25.63\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 94\n",
      "episodes_total: 3069\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1835830733340273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016253061737785096\n",
      "        policy_loss: -0.014313340723835012\n",
      "        total_loss: 6.681465606791999\n",
      "        vf_explained_var: -0.0010677252405433245\n",
      "        vf_loss: 6.6957281122204435\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 120000\n",
      "  num_agent_steps_trained: 120000\n",
      "  num_env_steps_sampled: 120000\n",
      "  num_env_steps_trained: 120000\n",
      "iterations_since_restore: 30\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 120000\n",
      "num_agent_steps_trained: 120000\n",
      "num_env_steps_sampled: 120000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 120000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.85\n",
      "  ram_util_percent: 52.224999999999994\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01515780221101674\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020499150519352395\n",
      "  mean_inference_ms: 0.23318069186582682\n",
      "  mean_raw_obs_processing_ms: 0.0320321323017885\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 98.0\n",
      "  episode_reward_mean: 25.63\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 94\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 9\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 13\n",
      "    - 25\n",
      "    - 15\n",
      "    - 48\n",
      "    - 14\n",
      "    - 29\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 19\n",
      "    - 16\n",
      "    - 43\n",
      "    - 50\n",
      "    - 36\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 17\n",
      "    - 36\n",
      "    - 50\n",
      "    - 38\n",
      "    episode_reward:\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 60.0\n",
      "    - 16.0\n",
      "    - 41.0\n",
      "    - 35.0\n",
      "    - 45.0\n",
      "    - 32.0\n",
      "    - 33.0\n",
      "    - 29.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 28.0\n",
      "    - 33.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 81.0\n",
      "    - 37.0\n",
      "    - 89.0\n",
      "    - 44.0\n",
      "    - 29.0\n",
      "    - 28.0\n",
      "    - 19.0\n",
      "    - 56.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 52.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 80.0\n",
      "    - 57.0\n",
      "    - 23.0\n",
      "    - 75.0\n",
      "    - 27.0\n",
      "    - 75.0\n",
      "    - 36.0\n",
      "    - 28.0\n",
      "    - 14.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 57.0\n",
      "    - 65.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 9.0\n",
      "    - 54.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 56.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 41.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 20.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 98.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01515780221101674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020499150519352395\n",
      "    mean_inference_ms: 0.23318069186582682\n",
      "    mean_raw_obs_processing_ms: 0.0320321323017885\n",
      "time_since_restore: 79.41715908050537\n",
      "time_this_iter_s: 2.6344382762908936\n",
      "time_total_s: 79.41715908050537\n",
      "timers:\n",
      "  learn_throughput: 2842.824\n",
      "  learn_time_ms: 1407.052\n",
      "  load_throughput: 22963613.468\n",
      "  load_time_ms: 0.174\n",
      "  training_iteration_time_ms: 2638.268\n",
      "  update_time_ms: 0.66\n",
      "timestamp: 1656299462\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 120000\n",
      "training_iteration: 30\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 124000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 124000\n",
      "  num_agent_steps_trained: 124000\n",
      "  num_env_steps_sampled: 124000\n",
      "  num_env_steps_trained: 124000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-04\n",
      "done: false\n",
      "episode_len_mean: 40.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 123.0\n",
      "episode_reward_mean: 31.98\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 98\n",
      "episodes_total: 3167\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1859053498955183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016582119815872723\n",
      "        policy_loss: -0.015278910057398901\n",
      "        total_loss: 6.532170847154433\n",
      "        vf_explained_var: -0.029563250721141857\n",
      "        vf_loss: 6.547397951044703\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 124000\n",
      "  num_agent_steps_trained: 124000\n",
      "  num_env_steps_sampled: 124000\n",
      "  num_env_steps_trained: 124000\n",
      "iterations_since_restore: 31\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 124000\n",
      "num_agent_steps_trained: 124000\n",
      "num_env_steps_sampled: 124000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 124000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.475000000000001\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015157668424704287\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02049049620342079\n",
      "  mean_inference_ms: 0.23315180958422752\n",
      "  mean_raw_obs_processing_ms: 0.03201965468852943\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 40.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 123.0\n",
      "  episode_reward_mean: 31.98\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 98\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 13\n",
      "    - 28\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 19\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 16\n",
      "    - 43\n",
      "    - 50\n",
      "    - 13\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 10\n",
      "    - 8\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 17\n",
      "    episode_reward:\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 41.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 45.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 31.0\n",
      "    - 2.0\n",
      "    - 37.0\n",
      "    - 2.0\n",
      "    - 50.0\n",
      "    - 47.0\n",
      "    - 19.0\n",
      "    - 55.0\n",
      "    - 28.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 52.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 11.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 56.0\n",
      "    - 94.0\n",
      "    - 46.0\n",
      "    - 70.0\n",
      "    - 3.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 55.0\n",
      "    - 40.0\n",
      "    - 34.0\n",
      "    - 58.0\n",
      "    - 72.0\n",
      "    - 40.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 123.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 10.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 91.0\n",
      "    - 54.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 30.0\n",
      "    - 8.0\n",
      "    - 52.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 70.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 82.0\n",
      "    - 101.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015157668424704287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02049049620342079\n",
      "    mean_inference_ms: 0.23315180958422752\n",
      "    mean_raw_obs_processing_ms: 0.03201965468852943\n",
      "time_since_restore: 82.0497522354126\n",
      "time_this_iter_s: 2.6325931549072266\n",
      "time_total_s: 82.0497522354126\n",
      "timers:\n",
      "  learn_throughput: 2846.222\n",
      "  learn_time_ms: 1405.372\n",
      "  load_throughput: 23017171.08\n",
      "  load_time_ms: 0.174\n",
      "  training_iteration_time_ms: 2636.941\n",
      "  update_time_ms: 0.67\n",
      "timestamp: 1656299464\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 124000\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 128000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 128000\n",
      "  num_agent_steps_trained: 128000\n",
      "  num_env_steps_sampled: 128000\n",
      "  num_env_steps_trained: 128000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-07\n",
      "done: false\n",
      "episode_len_mean: 42.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 39.04\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 92\n",
      "episodes_total: 3259\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.168812846752905\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017526266054420543\n",
      "        policy_loss: -0.014034227748471562\n",
      "        total_loss: 7.72531380755927\n",
      "        vf_explained_var: 0.0019030790816071212\n",
      "        vf_loss: 7.739293233169023\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 128000\n",
      "  num_agent_steps_trained: 128000\n",
      "  num_env_steps_sampled: 128000\n",
      "  num_env_steps_trained: 128000\n",
      "iterations_since_restore: 32\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 128000\n",
      "num_agent_steps_trained: 128000\n",
      "num_env_steps_sampled: 128000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 128000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.375\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01515834566409578\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020491654246176595\n",
      "  mean_inference_ms: 0.23318863392493913\n",
      "  mean_raw_obs_processing_ms: 0.03201333518065597\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 39.04\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 92\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 17\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 44\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 40\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 12\n",
      "    episode_reward:\n",
      "    - 70.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 82.0\n",
      "    - 101.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 48.0\n",
      "    - 28.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 32.0\n",
      "    - 41.0\n",
      "    - 62.0\n",
      "    - 20.0\n",
      "    - 41.0\n",
      "    - 34.0\n",
      "    - 53.0\n",
      "    - 99.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 71.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 98.0\n",
      "    - 62.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 81.0\n",
      "    - 69.0\n",
      "    - 4.0\n",
      "    - 61.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 78.0\n",
      "    - 8.0\n",
      "    - 3.0\n",
      "    - 29.0\n",
      "    - 5.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 63.0\n",
      "    - 24.0\n",
      "    - 27.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 77.0\n",
      "    - 76.0\n",
      "    - 56.0\n",
      "    - 25.0\n",
      "    - 60.0\n",
      "    - 23.0\n",
      "    - 94.0\n",
      "    - 23.0\n",
      "    - 50.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 61.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 5.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 89.0\n",
      "    - 38.0\n",
      "    - 34.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 86.0\n",
      "    - 44.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01515834566409578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020491654246176595\n",
      "    mean_inference_ms: 0.23318863392493913\n",
      "    mean_raw_obs_processing_ms: 0.03201333518065597\n",
      "time_since_restore: 84.71136927604675\n",
      "time_this_iter_s: 2.6616170406341553\n",
      "time_total_s: 84.71136927604675\n",
      "timers:\n",
      "  learn_throughput: 2843.748\n",
      "  learn_time_ms: 1406.594\n",
      "  load_throughput: 22951047.88\n",
      "  load_time_ms: 0.174\n",
      "  training_iteration_time_ms: 2637.624\n",
      "  update_time_ms: 0.688\n",
      "timestamp: 1656299467\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 128000\n",
      "training_iteration: 32\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 132000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 132000\n",
      "  num_agent_steps_trained: 132000\n",
      "  num_env_steps_sampled: 132000\n",
      "  num_env_steps_trained: 132000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-10\n",
      "done: false\n",
      "episode_len_mean: 42.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 91.0\n",
      "episode_reward_mean: 33.88\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 94\n",
      "episodes_total: 3353\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1630017425424308\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018423505457899263\n",
      "        policy_loss: -0.010163958354662824\n",
      "        total_loss: 7.294925200554633\n",
      "        vf_explained_var: 0.0016268926922992993\n",
      "        vf_loss: 7.305031568260603\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 132000\n",
      "  num_agent_steps_trained: 132000\n",
      "  num_env_steps_sampled: 132000\n",
      "  num_env_steps_trained: 132000\n",
      "iterations_since_restore: 33\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 132000\n",
      "num_agent_steps_trained: 132000\n",
      "num_env_steps_sampled: 132000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 132000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.75\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015156881803760892\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020481071954205444\n",
      "  mean_inference_ms: 0.23316038256672156\n",
      "  mean_raw_obs_processing_ms: 0.031998964788299694\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 91.0\n",
      "  episode_reward_mean: 33.88\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 94\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 38\n",
      "    - 33\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 29\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 29\n",
      "    - 6\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 46\n",
      "    - 23\n",
      "    - 50\n",
      "    - 27\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 29\n",
      "    - 39\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    episode_reward:\n",
      "    - 86.0\n",
      "    - 44.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 66.0\n",
      "    - 51.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 79.0\n",
      "    - 36.0\n",
      "    - 58.0\n",
      "    - 64.0\n",
      "    - 61.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 1.0\n",
      "    - 55.0\n",
      "    - 80.0\n",
      "    - 11.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 38.0\n",
      "    - 38.0\n",
      "    - 19.0\n",
      "    - 84.0\n",
      "    - 5.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 55.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 91.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 40.0\n",
      "    - 49.0\n",
      "    - 69.0\n",
      "    - 65.0\n",
      "    - 4.0\n",
      "    - 58.0\n",
      "    - 31.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 41.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 73.0\n",
      "    - 61.0\n",
      "    - 57.0\n",
      "    - 28.0\n",
      "    - 4.0\n",
      "    - 37.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 82.0\n",
      "    - 32.0\n",
      "    - 37.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 28.0\n",
      "    - 43.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 17.0\n",
      "    - 73.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015156881803760892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020481071954205444\n",
      "    mean_inference_ms: 0.23316038256672156\n",
      "    mean_raw_obs_processing_ms: 0.031998964788299694\n",
      "time_since_restore: 87.34329843521118\n",
      "time_this_iter_s: 2.6319291591644287\n",
      "time_total_s: 87.34329843521118\n",
      "timers:\n",
      "  learn_throughput: 2843.955\n",
      "  learn_time_ms: 1406.492\n",
      "  load_throughput: 23899168.091\n",
      "  load_time_ms: 0.167\n",
      "  training_iteration_time_ms: 2637.339\n",
      "  update_time_ms: 0.716\n",
      "timestamp: 1656299470\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 132000\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 136000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 136000\n",
      "  num_agent_steps_trained: 136000\n",
      "  num_env_steps_sampled: 136000\n",
      "  num_env_steps_trained: 136000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-12\n",
      "done: false\n",
      "episode_len_mean: 43.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 100.0\n",
      "episode_reward_mean: 36.79\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 91\n",
      "episodes_total: 3444\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.157635203869112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015320650412030118\n",
      "        policy_loss: -0.004835975790516503\n",
      "        total_loss: 7.707689014557869\n",
      "        vf_explained_var: 0.0026494775408057755\n",
      "        vf_loss: 7.712477128736435\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 136000\n",
      "  num_agent_steps_trained: 136000\n",
      "  num_env_steps_sampled: 136000\n",
      "  num_env_steps_trained: 136000\n",
      "iterations_since_restore: 34\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 136000\n",
      "num_agent_steps_trained: 136000\n",
      "num_env_steps_sampled: 136000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 136000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.9\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01515596200390335\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020475096094802536\n",
      "  mean_inference_ms: 0.23314101285551575\n",
      "  mean_raw_obs_processing_ms: 0.03198624466245447\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.0\n",
      "  episode_reward_mean: 36.79\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 91\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 45\n",
      "    - 29\n",
      "    - 39\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 6\n",
      "    - 48\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 13\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 43.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 17.0\n",
      "    - 73.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 21.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 20.0\n",
      "    - 60.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 85.0\n",
      "    - 4.0\n",
      "    - 29.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 48.0\n",
      "    - 65.0\n",
      "    - 42.0\n",
      "    - 40.0\n",
      "    - 31.0\n",
      "    - 61.0\n",
      "    - 49.0\n",
      "    - 66.0\n",
      "    - 16.0\n",
      "    - 20.0\n",
      "    - 32.0\n",
      "    - 68.0\n",
      "    - 89.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 15.0\n",
      "    - 13.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 62.0\n",
      "    - 49.0\n",
      "    - 19.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 20.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 67.0\n",
      "    - 93.0\n",
      "    - 49.0\n",
      "    - 73.0\n",
      "    - 45.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 80.0\n",
      "    - 32.0\n",
      "    - 66.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 51.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 100.0\n",
      "    - 74.0\n",
      "    - 14.0\n",
      "    - 65.0\n",
      "    - 61.0\n",
      "    - 48.0\n",
      "    - 31.0\n",
      "    - 63.0\n",
      "    - 81.0\n",
      "    - 34.0\n",
      "    - 73.0\n",
      "    - 13.0\n",
      "    - 62.0\n",
      "    - 44.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01515596200390335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020475096094802536\n",
      "    mean_inference_ms: 0.23314101285551575\n",
      "    mean_raw_obs_processing_ms: 0.03198624466245447\n",
      "time_since_restore: 89.97215557098389\n",
      "time_this_iter_s: 2.628857135772705\n",
      "time_total_s: 89.97215557098389\n",
      "timers:\n",
      "  learn_throughput: 2856.514\n",
      "  learn_time_ms: 1400.308\n",
      "  load_throughput: 23854992.18\n",
      "  load_time_ms: 0.168\n",
      "  training_iteration_time_ms: 2631.262\n",
      "  update_time_ms: 0.733\n",
      "timestamp: 1656299472\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 136000\n",
      "training_iteration: 34\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 140000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 140000\n",
      "  num_agent_steps_trained: 140000\n",
      "  num_env_steps_sampled: 140000\n",
      "  num_env_steps_trained: 140000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-15\n",
      "done: false\n",
      "episode_len_mean: 42.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 99.0\n",
      "episode_reward_mean: 33.57\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 95\n",
      "episodes_total: 3539\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1626032572920604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016034357922191742\n",
      "        policy_loss: -0.010528496877160124\n",
      "        total_loss: 7.32429564947723\n",
      "        vf_explained_var: 0.0027951842354189964\n",
      "        vf_loss: 7.334774065274064\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 140000\n",
      "  num_agent_steps_trained: 140000\n",
      "  num_env_steps_sampled: 140000\n",
      "  num_env_steps_trained: 140000\n",
      "iterations_since_restore: 35\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 140000\n",
      "num_agent_steps_trained: 140000\n",
      "num_env_steps_sampled: 140000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 140000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.700000000000001\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015153839782907866\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0204677399881129\n",
      "  mean_inference_ms: 0.2331152076556661\n",
      "  mean_raw_obs_processing_ms: 0.03197764090269759\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 99.0\n",
      "  episode_reward_mean: 33.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 95\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 43\n",
      "    - 6\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 19\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 25\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 23\n",
      "    - 36\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 39\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 34.0\n",
      "    - 73.0\n",
      "    - 13.0\n",
      "    - 62.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 65.0\n",
      "    - 61.0\n",
      "    - 38.0\n",
      "    - 2.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 37.0\n",
      "    - 71.0\n",
      "    - 67.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 26.0\n",
      "    - 41.0\n",
      "    - 32.0\n",
      "    - 15.0\n",
      "    - 35.0\n",
      "    - 43.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 88.0\n",
      "    - 99.0\n",
      "    - 16.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 7.0\n",
      "    - 42.0\n",
      "    - 62.0\n",
      "    - 41.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 42.0\n",
      "    - 58.0\n",
      "    - 20.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 32.0\n",
      "    - 48.0\n",
      "    - 29.0\n",
      "    - 1.0\n",
      "    - 3.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 24.0\n",
      "    - 68.0\n",
      "    - 23.0\n",
      "    - 53.0\n",
      "    - 32.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 34.0\n",
      "    - 73.0\n",
      "    - 75.0\n",
      "    - 54.0\n",
      "    - 42.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 72.0\n",
      "    - 3.0\n",
      "    - 49.0\n",
      "    - 85.0\n",
      "    - 61.0\n",
      "    - 36.0\n",
      "    - 49.0\n",
      "    - 56.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015153839782907866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0204677399881129\n",
      "    mean_inference_ms: 0.2331152076556661\n",
      "    mean_raw_obs_processing_ms: 0.03197764090269759\n",
      "time_since_restore: 92.6067795753479\n",
      "time_this_iter_s: 2.6346240043640137\n",
      "time_total_s: 92.6067795753479\n",
      "timers:\n",
      "  learn_throughput: 2856.037\n",
      "  learn_time_ms: 1400.542\n",
      "  load_throughput: 23854992.18\n",
      "  load_time_ms: 0.168\n",
      "  training_iteration_time_ms: 2631.494\n",
      "  update_time_ms: 0.729\n",
      "timestamp: 1656299475\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 140000\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 144000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 144000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_env_steps_sampled: 144000\n",
      "  num_env_steps_trained: 144000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-18\n",
      "done: false\n",
      "episode_len_mean: 41.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 85.0\n",
      "episode_reward_mean: 31.47\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 97\n",
      "episodes_total: 3636\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1548808541349185\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0164965425713128\n",
      "        policy_loss: -0.007856180235963836\n",
      "        total_loss: 7.5527473649671\n",
      "        vf_explained_var: 0.0017117024749837896\n",
      "        vf_loss: 7.560551999204902\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 144000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_env_steps_sampled: 144000\n",
      "  num_env_steps_trained: 144000\n",
      "iterations_since_restore: 36\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 144000\n",
      "num_agent_steps_trained: 144000\n",
      "num_env_steps_sampled: 144000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 144000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.075000000000001\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01515160852469105\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020460786672245376\n",
      "  mean_inference_ms: 0.23311894267123356\n",
      "  mean_raw_obs_processing_ms: 0.03197207799576666\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 85.0\n",
      "  episode_reward_mean: 31.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 97\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 10\n",
      "    - 19\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 47\n",
      "    - 50\n",
      "    - 36\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 37\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 9\n",
      "    - 50\n",
      "    - 43\n",
      "    - 17\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 29\n",
      "    - 21\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 36.0\n",
      "    - 49.0\n",
      "    - 56.0\n",
      "    - 85.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 39.0\n",
      "    - 11.0\n",
      "    - 49.0\n",
      "    - 22.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 5.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 19.0\n",
      "    - 59.0\n",
      "    - 19.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 8.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 16.0\n",
      "    - 38.0\n",
      "    - 19.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 37.0\n",
      "    - 33.0\n",
      "    - 83.0\n",
      "    - 47.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 26.0\n",
      "    - 83.0\n",
      "    - 31.0\n",
      "    - 26.0\n",
      "    - 48.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 34.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 64.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 38.0\n",
      "    - 69.0\n",
      "    - 49.0\n",
      "    - 53.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 19.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 4.0\n",
      "    - 43.0\n",
      "    - 42.0\n",
      "    - 49.0\n",
      "    - 80.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 36.0\n",
      "    - 68.0\n",
      "    - 40.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01515160852469105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020460786672245376\n",
      "    mean_inference_ms: 0.23311894267123356\n",
      "    mean_raw_obs_processing_ms: 0.03197207799576666\n",
      "time_since_restore: 95.2635498046875\n",
      "time_this_iter_s: 2.6567702293395996\n",
      "time_total_s: 95.2635498046875\n",
      "timers:\n",
      "  learn_throughput: 2851.645\n",
      "  learn_time_ms: 1402.699\n",
      "  load_throughput: 23699980.223\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2633.955\n",
      "  update_time_ms: 0.709\n",
      "timestamp: 1656299478\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 144000\n",
      "training_iteration: 36\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 148000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 148000\n",
      "  num_agent_steps_trained: 148000\n",
      "  num_env_steps_sampled: 148000\n",
      "  num_env_steps_trained: 148000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-20\n",
      "done: false\n",
      "episode_len_mean: 42.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 106.0\n",
      "episode_reward_mean: 37.16\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 95\n",
      "episodes_total: 3731\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1473462522670788\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0171346895286553\n",
      "        policy_loss: -0.005281307318958864\n",
      "        total_loss: 8.045905948454333\n",
      "        vf_explained_var: 0.0015885501779535766\n",
      "        vf_loss: 8.051133728027343\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 148000\n",
      "  num_agent_steps_trained: 148000\n",
      "  num_env_steps_sampled: 148000\n",
      "  num_env_steps_trained: 148000\n",
      "iterations_since_restore: 37\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 148000\n",
      "num_agent_steps_trained: 148000\n",
      "num_env_steps_sampled: 148000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 148000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.525\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015149307403284693\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020455578990710238\n",
      "  mean_inference_ms: 0.23309635527321299\n",
      "  mean_raw_obs_processing_ms: 0.03195241370815434\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.0\n",
      "  episode_reward_mean: 37.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 95\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 21\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 8\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 16\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 5\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 21\n",
      "    - 46\n",
      "    - 50\n",
      "    - 25\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 36.0\n",
      "    - 68.0\n",
      "    - 40.0\n",
      "    - 66.0\n",
      "    - 7.0\n",
      "    - 60.0\n",
      "    - 85.0\n",
      "    - 76.0\n",
      "    - 46.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 51.0\n",
      "    - 53.0\n",
      "    - 47.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 79.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 63.0\n",
      "    - 45.0\n",
      "    - 31.0\n",
      "    - 25.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 11.0\n",
      "    - 55.0\n",
      "    - 25.0\n",
      "    - 106.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 79.0\n",
      "    - 38.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 76.0\n",
      "    - 26.0\n",
      "    - 52.0\n",
      "    - 69.0\n",
      "    - 38.0\n",
      "    - 46.0\n",
      "    - 49.0\n",
      "    - 40.0\n",
      "    - 28.0\n",
      "    - 88.0\n",
      "    - 26.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 64.0\n",
      "    - 42.0\n",
      "    - 67.0\n",
      "    - 44.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 50.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 18.0\n",
      "    - 20.0\n",
      "    - 33.0\n",
      "    - 50.0\n",
      "    - 57.0\n",
      "    - 26.0\n",
      "    - 13.0\n",
      "    - 39.0\n",
      "    - 33.0\n",
      "    - 40.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 76.0\n",
      "    - 12.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015149307403284693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020455578990710238\n",
      "    mean_inference_ms: 0.23309635527321299\n",
      "    mean_raw_obs_processing_ms: 0.03195241370815434\n",
      "time_since_restore: 97.89196372032166\n",
      "time_this_iter_s: 2.6284139156341553\n",
      "time_total_s: 97.89196372032166\n",
      "timers:\n",
      "  learn_throughput: 2852.711\n",
      "  learn_time_ms: 1402.175\n",
      "  load_throughput: 23643201.804\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2632.84\n",
      "  update_time_ms: 0.688\n",
      "timestamp: 1656299480\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 148000\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 152000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 152000\n",
      "  num_agent_steps_trained: 152000\n",
      "  num_env_steps_sampled: 152000\n",
      "  num_env_steps_trained: 152000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-23\n",
      "done: false\n",
      "episode_len_mean: 44.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 38.34\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 88\n",
      "episodes_total: 3819\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1311681470563335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018293118527940203\n",
      "        policy_loss: -0.01839712341265973\n",
      "        total_loss: 7.908649359979937\n",
      "        vf_explained_var: 0.0034437877516592702\n",
      "        vf_loss: 7.926989330271239\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 152000\n",
      "  num_agent_steps_trained: 152000\n",
      "  num_env_steps_sampled: 152000\n",
      "  num_env_steps_trained: 152000\n",
      "iterations_since_restore: 38\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 152000\n",
      "num_agent_steps_trained: 152000\n",
      "num_env_steps_sampled: 152000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 152000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.1\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015148508940849176\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020450355841677616\n",
      "  mean_inference_ms: 0.23330948328887505\n",
      "  mean_raw_obs_processing_ms: 0.03193818406120868\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 38.34\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 88\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 5\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 21\n",
      "    - 46\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 31\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    episode_reward:\n",
      "    - 40.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 76.0\n",
      "    - 12.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 70.0\n",
      "    - 49.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 101.0\n",
      "    - 49.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 16.0\n",
      "    - 31.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 38.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 13.0\n",
      "    - 72.0\n",
      "    - 56.0\n",
      "    - 76.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 60.0\n",
      "    - 57.0\n",
      "    - 10.0\n",
      "    - 99.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 43.0\n",
      "    - 48.0\n",
      "    - 71.0\n",
      "    - 56.0\n",
      "    - 57.0\n",
      "    - 22.0\n",
      "    - 44.0\n",
      "    - 50.0\n",
      "    - 2.0\n",
      "    - 66.0\n",
      "    - 1.0\n",
      "    - 20.0\n",
      "    - 72.0\n",
      "    - 44.0\n",
      "    - 51.0\n",
      "    - 2.0\n",
      "    - 28.0\n",
      "    - 33.0\n",
      "    - 23.0\n",
      "    - 41.0\n",
      "    - 31.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 35.0\n",
      "    - 38.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 24.0\n",
      "    - 43.0\n",
      "    - 87.0\n",
      "    - 45.0\n",
      "    - 9.0\n",
      "    - 32.0\n",
      "    - 68.0\n",
      "    - 70.0\n",
      "    - 65.0\n",
      "    - 15.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 68.0\n",
      "    - 61.0\n",
      "    - 19.0\n",
      "    - 49.0\n",
      "    - 51.0\n",
      "    - 73.0\n",
      "    - 67.0\n",
      "    - 52.0\n",
      "    - 90.0\n",
      "    - 59.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015148508940849176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020450355841677616\n",
      "    mean_inference_ms: 0.23330948328887505\n",
      "    mean_raw_obs_processing_ms: 0.03193818406120868\n",
      "time_since_restore: 100.55290746688843\n",
      "time_this_iter_s: 2.6609437465667725\n",
      "time_total_s: 100.55290746688843\n",
      "timers:\n",
      "  learn_throughput: 2851.819\n",
      "  learn_time_ms: 1402.614\n",
      "  load_throughput: 23599966.24\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2637.11\n",
      "  update_time_ms: 0.702\n",
      "timestamp: 1656299483\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 152000\n",
      "training_iteration: 38\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 156000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 156000\n",
      "  num_agent_steps_trained: 156000\n",
      "  num_env_steps_sampled: 156000\n",
      "  num_env_steps_trained: 156000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-26\n",
      "done: false\n",
      "episode_len_mean: 45.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 90.0\n",
      "episode_reward_mean: 41.74\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 88\n",
      "episodes_total: 3907\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1371052551013168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01618123082544214\n",
      "        policy_loss: -0.014694027618695331\n",
      "        total_loss: 7.705724618511815\n",
      "        vf_explained_var: 0.001707915977765155\n",
      "        vf_loss: 7.7203680594762165\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 156000\n",
      "  num_agent_steps_trained: 156000\n",
      "  num_env_steps_sampled: 156000\n",
      "  num_env_steps_trained: 156000\n",
      "iterations_since_restore: 39\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 156000\n",
      "num_agent_steps_trained: 156000\n",
      "num_env_steps_sampled: 156000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 156000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.5\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015147885871307457\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02044623424625187\n",
      "  mean_inference_ms: 0.2333286441598623\n",
      "  mean_raw_obs_processing_ms: 0.03192832542640059\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 90.0\n",
      "  episode_reward_mean: 41.74\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 88\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 20\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 19\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 68.0\n",
      "    - 61.0\n",
      "    - 19.0\n",
      "    - 49.0\n",
      "    - 51.0\n",
      "    - 73.0\n",
      "    - 67.0\n",
      "    - 52.0\n",
      "    - 90.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 69.0\n",
      "    - 49.0\n",
      "    - 78.0\n",
      "    - 38.0\n",
      "    - 70.0\n",
      "    - 52.0\n",
      "    - 18.0\n",
      "    - 9.0\n",
      "    - 60.0\n",
      "    - 32.0\n",
      "    - 71.0\n",
      "    - 15.0\n",
      "    - 78.0\n",
      "    - 26.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 84.0\n",
      "    - 29.0\n",
      "    - 14.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 66.0\n",
      "    - 41.0\n",
      "    - 20.0\n",
      "    - 62.0\n",
      "    - 30.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 47.0\n",
      "    - 54.0\n",
      "    - 60.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 68.0\n",
      "    - 75.0\n",
      "    - 33.0\n",
      "    - 39.0\n",
      "    - 47.0\n",
      "    - 83.0\n",
      "    - 56.0\n",
      "    - 63.0\n",
      "    - 57.0\n",
      "    - 81.0\n",
      "    - 1.0\n",
      "    - 76.0\n",
      "    - 62.0\n",
      "    - 41.0\n",
      "    - 41.0\n",
      "    - 69.0\n",
      "    - 45.0\n",
      "    - 33.0\n",
      "    - 68.0\n",
      "    - 52.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 41.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 17.0\n",
      "    - 42.0\n",
      "    - 84.0\n",
      "    - 72.0\n",
      "    - 58.0\n",
      "    - 51.0\n",
      "    - 71.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015147885871307457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02044623424625187\n",
      "    mean_inference_ms: 0.2333286441598623\n",
      "    mean_raw_obs_processing_ms: 0.03192832542640059\n",
      "time_since_restore: 103.18731951713562\n",
      "time_this_iter_s: 2.6344120502471924\n",
      "time_total_s: 103.18731951713562\n",
      "timers:\n",
      "  learn_throughput: 2850.366\n",
      "  learn_time_ms: 1403.329\n",
      "  load_throughput: 23629881.69\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2638.001\n",
      "  update_time_ms: 0.681\n",
      "timestamp: 1656299486\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 156000\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 160000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 160000\n",
      "  num_agent_steps_trained: 160000\n",
      "  num_env_steps_sampled: 160000\n",
      "  num_env_steps_trained: 160000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-28\n",
      "done: false\n",
      "episode_len_mean: 44.17\n",
      "episode_media: {}\n",
      "episode_reward_max: 107.0\n",
      "episode_reward_mean: 38.57\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 92\n",
      "episodes_total: 3999\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.138616241819115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016245075686764653\n",
      "        policy_loss: -0.011284813180726062\n",
      "        total_loss: 7.438807184593652\n",
      "        vf_explained_var: 0.0020213271341016216\n",
      "        vf_loss: 7.450041249154076\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 160000\n",
      "  num_agent_steps_trained: 160000\n",
      "  num_env_steps_sampled: 160000\n",
      "  num_env_steps_trained: 160000\n",
      "iterations_since_restore: 40\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 160000\n",
      "num_agent_steps_trained: 160000\n",
      "num_env_steps_sampled: 160000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 160000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.15\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01514625877618408\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020438724866920048\n",
      "  mean_inference_ms: 0.2333067327788709\n",
      "  mean_raw_obs_processing_ms: 0.03191930693216841\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 107.0\n",
      "  episode_reward_mean: 38.57\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 92\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 26\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 14\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 44\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 18.0\n",
      "    - 17.0\n",
      "    - 42.0\n",
      "    - 84.0\n",
      "    - 72.0\n",
      "    - 58.0\n",
      "    - 51.0\n",
      "    - 71.0\n",
      "    - 63.0\n",
      "    - 37.0\n",
      "    - 71.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 29.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 20.0\n",
      "    - 61.0\n",
      "    - 56.0\n",
      "    - 4.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 33.0\n",
      "    - 93.0\n",
      "    - 73.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 68.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 92.0\n",
      "    - 27.0\n",
      "    - 55.0\n",
      "    - 68.0\n",
      "    - 56.0\n",
      "    - 39.0\n",
      "    - 77.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 61.0\n",
      "    - 23.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 6.0\n",
      "    - 19.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 87.0\n",
      "    - 38.0\n",
      "    - 38.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 2.0\n",
      "    - 46.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 20.0\n",
      "    - 57.0\n",
      "    - 68.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 69.0\n",
      "    - 50.0\n",
      "    - 55.0\n",
      "    - 50.0\n",
      "    - 43.0\n",
      "    - 36.0\n",
      "    - 53.0\n",
      "    - 60.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 107.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 22.0\n",
      "    - 42.0\n",
      "    - 91.0\n",
      "    - 66.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01514625877618408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020438724866920048\n",
      "    mean_inference_ms: 0.2333067327788709\n",
      "    mean_raw_obs_processing_ms: 0.03191930693216841\n",
      "time_since_restore: 105.82272958755493\n",
      "time_this_iter_s: 2.6354100704193115\n",
      "time_total_s: 105.82272958755493\n",
      "timers:\n",
      "  learn_throughput: 2849.994\n",
      "  learn_time_ms: 1403.512\n",
      "  load_throughput: 23613252.639\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2638.103\n",
      "  update_time_ms: 0.682\n",
      "timestamp: 1656299488\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 160000\n",
      "training_iteration: 40\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 164000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 164000\n",
      "  num_agent_steps_trained: 164000\n",
      "  num_env_steps_sampled: 164000\n",
      "  num_env_steps_trained: 164000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-31\n",
      "done: false\n",
      "episode_len_mean: 44.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 107.0\n",
      "episode_reward_mean: 43.89\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 89\n",
      "episodes_total: 4088\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1338008648605757\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01621091182303605\n",
      "        policy_loss: -0.016456584539264442\n",
      "        total_loss: 8.466377332133632\n",
      "        vf_explained_var: 0.0023566956802081036\n",
      "        vf_loss: 8.482783250398533\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 164000\n",
      "  num_agent_steps_trained: 164000\n",
      "  num_env_steps_sampled: 164000\n",
      "  num_env_steps_trained: 164000\n",
      "iterations_since_restore: 41\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 164000\n",
      "num_agent_steps_trained: 164000\n",
      "num_env_steps_sampled: 164000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 164000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.925\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015146530121416068\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020435487986862566\n",
      "  mean_inference_ms: 0.23331497866865206\n",
      "  mean_raw_obs_processing_ms: 0.03191331665208899\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 107.0\n",
      "  episode_reward_mean: 43.89\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 89\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 44\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 40\n",
      "    - 48\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 40\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 60.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 107.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 22.0\n",
      "    - 42.0\n",
      "    - 91.0\n",
      "    - 66.0\n",
      "    - 35.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 51.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 49.0\n",
      "    - 45.0\n",
      "    - 101.0\n",
      "    - 72.0\n",
      "    - 57.0\n",
      "    - 41.0\n",
      "    - 48.0\n",
      "    - 13.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 64.0\n",
      "    - 34.0\n",
      "    - 8.0\n",
      "    - 18.0\n",
      "    - 13.0\n",
      "    - 51.0\n",
      "    - 56.0\n",
      "    - 81.0\n",
      "    - 59.0\n",
      "    - 13.0\n",
      "    - 72.0\n",
      "    - 51.0\n",
      "    - 31.0\n",
      "    - 64.0\n",
      "    - 44.0\n",
      "    - 55.0\n",
      "    - 19.0\n",
      "    - 52.0\n",
      "    - 50.0\n",
      "    - 65.0\n",
      "    - 27.0\n",
      "    - 62.0\n",
      "    - 2.0\n",
      "    - 59.0\n",
      "    - 45.0\n",
      "    - 83.0\n",
      "    - 96.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 52.0\n",
      "    - 57.0\n",
      "    - 36.0\n",
      "    - 31.0\n",
      "    - 62.0\n",
      "    - 64.0\n",
      "    - 26.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 49.0\n",
      "    - 102.0\n",
      "    - 64.0\n",
      "    - 25.0\n",
      "    - 54.0\n",
      "    - 42.0\n",
      "    - 58.0\n",
      "    - 48.0\n",
      "    - 48.0\n",
      "    - 74.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 37.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 32.0\n",
      "    - 64.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 9.0\n",
      "    - 43.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 61.0\n",
      "    - 77.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015146530121416068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020435487986862566\n",
      "    mean_inference_ms: 0.23331497866865206\n",
      "    mean_raw_obs_processing_ms: 0.03191331665208899\n",
      "time_since_restore: 108.47729468345642\n",
      "time_this_iter_s: 2.6545650959014893\n",
      "time_total_s: 108.47729468345642\n",
      "timers:\n",
      "  learn_throughput: 2846.686\n",
      "  learn_time_ms: 1405.142\n",
      "  load_throughput: 23838044.899\n",
      "  load_time_ms: 0.168\n",
      "  training_iteration_time_ms: 2640.303\n",
      "  update_time_ms: 0.68\n",
      "timestamp: 1656299491\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 164000\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 168000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 168000\n",
      "  num_agent_steps_trained: 168000\n",
      "  num_env_steps_sampled: 168000\n",
      "  num_env_steps_trained: 168000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-34\n",
      "done: false\n",
      "episode_len_mean: 43.45\n",
      "episode_media: {}\n",
      "episode_reward_max: 104.0\n",
      "episode_reward_mean: 36.64\n",
      "episode_reward_min: -8.0\n",
      "episodes_this_iter: 93\n",
      "episodes_total: 4181\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1277515411376953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01765283720958377\n",
      "        policy_loss: -0.015346852923312814\n",
      "        total_loss: 8.136386351944298\n",
      "        vf_explained_var: 0.002396223621983682\n",
      "        vf_loss: 8.151678021236133\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 168000\n",
      "  num_agent_steps_trained: 168000\n",
      "  num_env_steps_sampled: 168000\n",
      "  num_env_steps_trained: 168000\n",
      "iterations_since_restore: 42\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 168000\n",
      "num_agent_steps_trained: 168000\n",
      "num_env_steps_sampled: 168000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 168000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.325\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015146009616293388\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02042922687580021\n",
      "  mean_inference_ms: 0.23330011214538188\n",
      "  mean_raw_obs_processing_ms: 0.0319054702657959\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 104.0\n",
      "  episode_reward_mean: 36.64\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 93\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 20\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 48\n",
      "    - 43\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 28\n",
      "    - 8\n",
      "    episode_reward:\n",
      "    - 9.0\n",
      "    - 43.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 61.0\n",
      "    - 77.0\n",
      "    - 12.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 29.0\n",
      "    - 44.0\n",
      "    - 24.0\n",
      "    - 34.0\n",
      "    - 8.0\n",
      "    - 66.0\n",
      "    - 38.0\n",
      "    - 61.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 67.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 55.0\n",
      "    - 63.0\n",
      "    - 48.0\n",
      "    - 70.0\n",
      "    - 53.0\n",
      "    - 42.0\n",
      "    - 43.0\n",
      "    - 12.0\n",
      "    - 77.0\n",
      "    - 43.0\n",
      "    - 71.0\n",
      "    - 58.0\n",
      "    - 36.0\n",
      "    - 35.0\n",
      "    - 60.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 51.0\n",
      "    - 57.0\n",
      "    - 23.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 49.0\n",
      "    - 37.0\n",
      "    - -8.0\n",
      "    - 40.0\n",
      "    - 104.0\n",
      "    - 11.0\n",
      "    - 72.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 95.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 103.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 29.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 20.0\n",
      "    - 26.0\n",
      "    - 29.0\n",
      "    - 25.0\n",
      "    - 74.0\n",
      "    - 45.0\n",
      "    - 21.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 18.0\n",
      "    - 73.0\n",
      "    - 49.0\n",
      "    - 10.0\n",
      "    - 38.0\n",
      "    - 56.0\n",
      "    - 55.0\n",
      "    - 18.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015146009616293388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02042922687580021\n",
      "    mean_inference_ms: 0.23330011214538188\n",
      "    mean_raw_obs_processing_ms: 0.0319054702657959\n",
      "time_since_restore: 111.11823058128357\n",
      "time_this_iter_s: 2.6409358978271484\n",
      "time_total_s: 111.11823058128357\n",
      "timers:\n",
      "  learn_throughput: 2848.838\n",
      "  learn_time_ms: 1404.081\n",
      "  load_throughput: 23696632.768\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2638.233\n",
      "  update_time_ms: 0.667\n",
      "timestamp: 1656299494\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 168000\n",
      "training_iteration: 42\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 172000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 172000\n",
      "  num_agent_steps_trained: 172000\n",
      "  num_env_steps_sampled: 172000\n",
      "  num_env_steps_trained: 172000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-36\n",
      "done: false\n",
      "episode_len_mean: 43.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 120.0\n",
      "episode_reward_mean: 37.36\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 91\n",
      "episodes_total: 4272\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1229282188159164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017510303699721893\n",
      "        policy_loss: -0.017416247320912216\n",
      "        total_loss: 7.921971362124207\n",
      "        vf_explained_var: 0.0032756207450743645\n",
      "        vf_loss: 7.939332879486904\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 172000\n",
      "  num_agent_steps_trained: 172000\n",
      "  num_env_steps_sampled: 172000\n",
      "  num_env_steps_trained: 172000\n",
      "iterations_since_restore: 43\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 172000\n",
      "num_agent_steps_trained: 172000\n",
      "num_env_steps_sampled: 172000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 172000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.766666666666666\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015144499461645988\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020422490650592193\n",
      "  mean_inference_ms: 0.23327667908592178\n",
      "  mean_raw_obs_processing_ms: 0.03189478880199358\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 120.0\n",
      "  episode_reward_mean: 37.36\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 91\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 28\n",
      "    - 8\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 8\n",
      "    - 13\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 8\n",
      "    - 50\n",
      "    - 44\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 10.0\n",
      "    - 38.0\n",
      "    - 56.0\n",
      "    - 55.0\n",
      "    - 18.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 69.0\n",
      "    - 80.0\n",
      "    - 80.0\n",
      "    - 20.0\n",
      "    - 25.0\n",
      "    - 40.0\n",
      "    - 59.0\n",
      "    - 120.0\n",
      "    - 92.0\n",
      "    - 35.0\n",
      "    - 13.0\n",
      "    - 7.0\n",
      "    - 29.0\n",
      "    - 68.0\n",
      "    - 70.0\n",
      "    - 4.0\n",
      "    - 41.0\n",
      "    - 73.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 29.0\n",
      "    - 90.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 7.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 55.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 34.0\n",
      "    - 23.0\n",
      "    - 44.0\n",
      "    - 3.0\n",
      "    - 55.0\n",
      "    - 14.0\n",
      "    - 73.0\n",
      "    - 58.0\n",
      "    - 48.0\n",
      "    - 52.0\n",
      "    - 36.0\n",
      "    - 27.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 82.0\n",
      "    - 57.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 68.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 24.0\n",
      "    - 97.0\n",
      "    - 55.0\n",
      "    - 34.0\n",
      "    - 40.0\n",
      "    - 53.0\n",
      "    - 62.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 98.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 53.0\n",
      "    - 19.0\n",
      "    - 55.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015144499461645988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020422490650592193\n",
      "    mean_inference_ms: 0.23327667908592178\n",
      "    mean_raw_obs_processing_ms: 0.03189478880199358\n",
      "time_since_restore: 113.74374389648438\n",
      "time_this_iter_s: 2.6255133152008057\n",
      "time_total_s: 113.74374389648438\n",
      "timers:\n",
      "  learn_throughput: 2849.908\n",
      "  learn_time_ms: 1403.554\n",
      "  load_throughput: 22598620.69\n",
      "  load_time_ms: 0.177\n",
      "  training_iteration_time_ms: 2637.582\n",
      "  update_time_ms: 0.653\n",
      "timestamp: 1656299496\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 172000\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 176000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 176000\n",
      "  num_agent_steps_trained: 176000\n",
      "  num_env_steps_sampled: 176000\n",
      "  num_env_steps_trained: 176000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-39\n",
      "done: false\n",
      "episode_len_mean: 45.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 43.11\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 89\n",
      "episodes_total: 4361\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.120362045944378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016518988651573977\n",
      "        policy_loss: -0.008648778942804183\n",
      "        total_loss: 8.460956921115999\n",
      "        vf_explained_var: 0.002315830543477048\n",
      "        vf_loss: 8.469554082808957\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 176000\n",
      "  num_agent_steps_trained: 176000\n",
      "  num_env_steps_sampled: 176000\n",
      "  num_env_steps_trained: 176000\n",
      "iterations_since_restore: 44\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 176000\n",
      "num_agent_steps_trained: 176000\n",
      "num_env_steps_sampled: 176000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 176000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.325\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015145950486361566\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02041931256151933\n",
      "  mean_inference_ms: 0.23327976286899346\n",
      "  mean_raw_obs_processing_ms: 0.03188743593653243\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 43.11\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 89\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 33\n",
      "    - 47\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 53.0\n",
      "    - 62.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 98.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 53.0\n",
      "    - 19.0\n",
      "    - 55.0\n",
      "    - 38.0\n",
      "    - 93.0\n",
      "    - 42.0\n",
      "    - 36.0\n",
      "    - 58.0\n",
      "    - 75.0\n",
      "    - 78.0\n",
      "    - 25.0\n",
      "    - 33.0\n",
      "    - 43.0\n",
      "    - 89.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 49.0\n",
      "    - 42.0\n",
      "    - 64.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 86.0\n",
      "    - 58.0\n",
      "    - 44.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 66.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 86.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 46.0\n",
      "    - 36.0\n",
      "    - 31.0\n",
      "    - 101.0\n",
      "    - 11.0\n",
      "    - 62.0\n",
      "    - 83.0\n",
      "    - 35.0\n",
      "    - 6.0\n",
      "    - 35.0\n",
      "    - 82.0\n",
      "    - 66.0\n",
      "    - 43.0\n",
      "    - 41.0\n",
      "    - 4.0\n",
      "    - 19.0\n",
      "    - 86.0\n",
      "    - 37.0\n",
      "    - 34.0\n",
      "    - 30.0\n",
      "    - 78.0\n",
      "    - 46.0\n",
      "    - 51.0\n",
      "    - 30.0\n",
      "    - 69.0\n",
      "    - 49.0\n",
      "    - 48.0\n",
      "    - 25.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 56.0\n",
      "    - 56.0\n",
      "    - 42.0\n",
      "    - 53.0\n",
      "    - 44.0\n",
      "    - 21.0\n",
      "    - 29.0\n",
      "    - 53.0\n",
      "    - 64.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 77.0\n",
      "    - 54.0\n",
      "    - 38.0\n",
      "    - 51.0\n",
      "    - 4.0\n",
      "    - 38.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015145950486361566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02041931256151933\n",
      "    mean_inference_ms: 0.23327976286899346\n",
      "    mean_raw_obs_processing_ms: 0.03188743593653243\n",
      "time_since_restore: 116.42159867286682\n",
      "time_this_iter_s: 2.6778547763824463\n",
      "time_total_s: 116.42159867286682\n",
      "timers:\n",
      "  learn_throughput: 2841.272\n",
      "  learn_time_ms: 1407.82\n",
      "  load_throughput: 22571257.904\n",
      "  load_time_ms: 0.177\n",
      "  training_iteration_time_ms: 2642.491\n",
      "  update_time_ms: 0.649\n",
      "timestamp: 1656299499\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 176000\n",
      "training_iteration: 44\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 180000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 180000\n",
      "  num_agent_steps_trained: 180000\n",
      "  num_env_steps_sampled: 180000\n",
      "  num_env_steps_trained: 180000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-42\n",
      "done: false\n",
      "episode_len_mean: 43.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 40.89\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 91\n",
      "episodes_total: 4452\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.11070442955981\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01812644401885649\n",
      "        policy_loss: -0.013814555346384965\n",
      "        total_loss: 8.02689556229499\n",
      "        vf_explained_var: 0.0036510263719866354\n",
      "        vf_loss: 8.04065350281295\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 180000\n",
      "  num_agent_steps_trained: 180000\n",
      "  num_env_steps_sampled: 180000\n",
      "  num_env_steps_trained: 180000\n",
      "iterations_since_restore: 45\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 180000\n",
      "num_agent_steps_trained: 180000\n",
      "num_env_steps_sampled: 180000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 180000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.975\n",
      "  ram_util_percent: 52.15\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015149119876909145\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02042019243247327\n",
      "  mean_inference_ms: 0.23332602306561132\n",
      "  mean_raw_obs_processing_ms: 0.03188833226312807\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 40.89\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 91\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 47\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 10\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 77.0\n",
      "    - 54.0\n",
      "    - 38.0\n",
      "    - 51.0\n",
      "    - 4.0\n",
      "    - 38.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 22.0\n",
      "    - 40.0\n",
      "    - 53.0\n",
      "    - 44.0\n",
      "    - 70.0\n",
      "    - 48.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 41.0\n",
      "    - 88.0\n",
      "    - 53.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 101.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 69.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 88.0\n",
      "    - 44.0\n",
      "    - 40.0\n",
      "    - 69.0\n",
      "    - 40.0\n",
      "    - 57.0\n",
      "    - 49.0\n",
      "    - 24.0\n",
      "    - 63.0\n",
      "    - 39.0\n",
      "    - 56.0\n",
      "    - 1.0\n",
      "    - 39.0\n",
      "    - 69.0\n",
      "    - 69.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 28.0\n",
      "    - 71.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 49.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 70.0\n",
      "    - 60.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 39.0\n",
      "    - 29.0\n",
      "    - 60.0\n",
      "    - 17.0\n",
      "    - 60.0\n",
      "    - 23.0\n",
      "    - 67.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 65.0\n",
      "    - 84.0\n",
      "    - 45.0\n",
      "    - 4.0\n",
      "    - 54.0\n",
      "    - 52.0\n",
      "    - 15.0\n",
      "    - 1.0\n",
      "    - 2.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 67.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015149119876909145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02042019243247327\n",
      "    mean_inference_ms: 0.23332602306561132\n",
      "    mean_raw_obs_processing_ms: 0.03188833226312807\n",
      "time_since_restore: 119.09628558158875\n",
      "time_this_iter_s: 2.674686908721924\n",
      "time_total_s: 119.09628558158875\n",
      "timers:\n",
      "  learn_throughput: 2836.785\n",
      "  learn_time_ms: 1410.047\n",
      "  load_throughput: 22432432.143\n",
      "  load_time_ms: 0.178\n",
      "  training_iteration_time_ms: 2646.514\n",
      "  update_time_ms: 0.642\n",
      "timestamp: 1656299502\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 180000\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 184000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 184000\n",
      "  num_agent_steps_trained: 184000\n",
      "  num_env_steps_sampled: 184000\n",
      "  num_env_steps_trained: 184000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-44\n",
      "done: false\n",
      "episode_len_mean: 43.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 91.0\n",
      "episode_reward_mean: 36.83\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 92\n",
      "episodes_total: 4544\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.1014436712829017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018327404939199005\n",
      "        policy_loss: -0.01724354369085162\n",
      "        total_loss: 7.688817424671624\n",
      "        vf_explained_var: 0.0030096254041118006\n",
      "        vf_loss: 7.706003697200488\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 184000\n",
      "  num_agent_steps_trained: 184000\n",
      "  num_env_steps_sampled: 184000\n",
      "  num_env_steps_trained: 184000\n",
      "iterations_since_restore: 46\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 184000\n",
      "num_agent_steps_trained: 184000\n",
      "num_env_steps_sampled: 184000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 184000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.349999999999998\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015148202676968345\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020415659219787604\n",
      "  mean_inference_ms: 0.23331341442033385\n",
      "  mean_raw_obs_processing_ms: 0.03188401243865647\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 91.0\n",
      "  episode_reward_mean: 36.83\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 92\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 17\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 26\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 14\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 52.0\n",
      "    - 15.0\n",
      "    - 1.0\n",
      "    - 2.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 67.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 52.0\n",
      "    - 61.0\n",
      "    - 44.0\n",
      "    - 38.0\n",
      "    - 68.0\n",
      "    - 41.0\n",
      "    - 14.0\n",
      "    - 22.0\n",
      "    - 32.0\n",
      "    - 44.0\n",
      "    - 31.0\n",
      "    - 41.0\n",
      "    - 5.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 43.0\n",
      "    - 71.0\n",
      "    - 32.0\n",
      "    - 29.0\n",
      "    - 1.0\n",
      "    - 59.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 25.0\n",
      "    - 24.0\n",
      "    - 58.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 25.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 29.0\n",
      "    - 71.0\n",
      "    - 34.0\n",
      "    - 50.0\n",
      "    - 29.0\n",
      "    - 20.0\n",
      "    - 69.0\n",
      "    - 38.0\n",
      "    - 26.0\n",
      "    - 53.0\n",
      "    - 91.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 69.0\n",
      "    - 83.0\n",
      "    - 81.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 28.0\n",
      "    - 28.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 85.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 42.0\n",
      "    - 53.0\n",
      "    - 52.0\n",
      "    - 50.0\n",
      "    - 20.0\n",
      "    - 49.0\n",
      "    - 1.0\n",
      "    - 12.0\n",
      "    - 67.0\n",
      "    - 26.0\n",
      "    - 31.0\n",
      "    - 44.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015148202676968345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020415659219787604\n",
      "    mean_inference_ms: 0.23331341442033385\n",
      "    mean_raw_obs_processing_ms: 0.03188401243865647\n",
      "time_since_restore: 121.74814772605896\n",
      "time_this_iter_s: 2.651862144470215\n",
      "time_total_s: 121.74814772605896\n",
      "timers:\n",
      "  learn_throughput: 2836.992\n",
      "  learn_time_ms: 1409.944\n",
      "  load_throughput: 22819934.712\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2646.053\n",
      "  update_time_ms: 0.655\n",
      "timestamp: 1656299504\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 184000\n",
      "training_iteration: 46\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 188000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 188000\n",
      "  num_agent_steps_trained: 188000\n",
      "  num_env_steps_sampled: 188000\n",
      "  num_env_steps_trained: 188000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-47\n",
      "done: false\n",
      "episode_len_mean: 46.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 97.0\n",
      "episode_reward_mean: 42.16\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 88\n",
      "episodes_total: 4632\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0031250000000000006\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.077547767982688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021722070747605897\n",
      "        policy_loss: -0.02104912410960883\n",
      "        total_loss: 7.7615899575653895\n",
      "        vf_explained_var: 0.0077983189013696485\n",
      "        vf_loss: 7.782571221279201\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 188000\n",
      "  num_agent_steps_trained: 188000\n",
      "  num_env_steps_sampled: 188000\n",
      "  num_env_steps_trained: 188000\n",
      "iterations_since_restore: 47\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 188000\n",
      "num_agent_steps_trained: 188000\n",
      "num_env_steps_sampled: 188000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 188000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.45\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015147463584327408\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020413025945027828\n",
      "  mean_inference_ms: 0.2333089677579632\n",
      "  mean_raw_obs_processing_ms: 0.03187676073318536\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 97.0\n",
      "  episode_reward_mean: 42.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 88\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 48\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 15\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 33\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 42.0\n",
      "    - 53.0\n",
      "    - 52.0\n",
      "    - 50.0\n",
      "    - 20.0\n",
      "    - 49.0\n",
      "    - 1.0\n",
      "    - 12.0\n",
      "    - 67.0\n",
      "    - 26.0\n",
      "    - 31.0\n",
      "    - 44.0\n",
      "    - 10.0\n",
      "    - 30.0\n",
      "    - 88.0\n",
      "    - 54.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 39.0\n",
      "    - 18.0\n",
      "    - 72.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 78.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 52.0\n",
      "    - 65.0\n",
      "    - 50.0\n",
      "    - 58.0\n",
      "    - 44.0\n",
      "    - 70.0\n",
      "    - 77.0\n",
      "    - 80.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 14.0\n",
      "    - 52.0\n",
      "    - 38.0\n",
      "    - 83.0\n",
      "    - 56.0\n",
      "    - 47.0\n",
      "    - 82.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 71.0\n",
      "    - 32.0\n",
      "    - 56.0\n",
      "    - 47.0\n",
      "    - 35.0\n",
      "    - 55.0\n",
      "    - 72.0\n",
      "    - 78.0\n",
      "    - 22.0\n",
      "    - 97.0\n",
      "    - 51.0\n",
      "    - 47.0\n",
      "    - 66.0\n",
      "    - 58.0\n",
      "    - 39.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 54.0\n",
      "    - 69.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 47.0\n",
      "    - 29.0\n",
      "    - 30.0\n",
      "    - 58.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 31.0\n",
      "    - 77.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 13.0\n",
      "    - 27.0\n",
      "    - 47.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015147463584327408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020413025945027828\n",
      "    mean_inference_ms: 0.2333089677579632\n",
      "    mean_raw_obs_processing_ms: 0.03187676073318536\n",
      "time_since_restore: 124.39695572853088\n",
      "time_this_iter_s: 2.648808002471924\n",
      "time_total_s: 124.39695572853088\n",
      "timers:\n",
      "  learn_throughput: 2833.872\n",
      "  learn_time_ms: 1411.496\n",
      "  load_throughput: 21484461.519\n",
      "  load_time_ms: 0.186\n",
      "  training_iteration_time_ms: 2648.101\n",
      "  update_time_ms: 0.66\n",
      "timestamp: 1656299507\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 188000\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 192000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 192000\n",
      "  num_agent_steps_trained: 192000\n",
      "  num_env_steps_sampled: 192000\n",
      "  num_env_steps_trained: 192000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-50\n",
      "done: false\n",
      "episode_len_mean: 43.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 87.0\n",
      "episode_reward_mean: 39.26\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 91\n",
      "episodes_total: 4723\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0046875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0792867429794804\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01908737470255664\n",
      "        policy_loss: -0.008634064873800643\n",
      "        total_loss: 7.799589969778574\n",
      "        vf_explained_var: 0.002973121468738843\n",
      "        vf_loss: 7.808134574274863\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 192000\n",
      "  num_agent_steps_trained: 192000\n",
      "  num_env_steps_sampled: 192000\n",
      "  num_env_steps_trained: 192000\n",
      "iterations_since_restore: 48\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 192000\n",
      "num_agent_steps_trained: 192000\n",
      "num_env_steps_sampled: 192000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 192000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.433333333333335\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015147572855322764\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02040582264457688\n",
      "  mean_inference_ms: 0.23329045708607993\n",
      "  mean_raw_obs_processing_ms: 0.031868278246311485\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 87.0\n",
      "  episode_reward_mean: 39.26\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 91\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 33\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 40\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 47\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 70.0\n",
      "    - 31.0\n",
      "    - 77.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 13.0\n",
      "    - 27.0\n",
      "    - 47.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 49.0\n",
      "    - 4.0\n",
      "    - 53.0\n",
      "    - 44.0\n",
      "    - 13.0\n",
      "    - 83.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 51.0\n",
      "    - 43.0\n",
      "    - 60.0\n",
      "    - 46.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 46.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 78.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 8.0\n",
      "    - 75.0\n",
      "    - 39.0\n",
      "    - 39.0\n",
      "    - 22.0\n",
      "    - 70.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 84.0\n",
      "    - 12.0\n",
      "    - 42.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 63.0\n",
      "    - 39.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 42.0\n",
      "    - 37.0\n",
      "    - 41.0\n",
      "    - 81.0\n",
      "    - 65.0\n",
      "    - 26.0\n",
      "    - 85.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 63.0\n",
      "    - 63.0\n",
      "    - 32.0\n",
      "    - 27.0\n",
      "    - 57.0\n",
      "    - 73.0\n",
      "    - 48.0\n",
      "    - 32.0\n",
      "    - 39.0\n",
      "    - 65.0\n",
      "    - 57.0\n",
      "    - 71.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 65.0\n",
      "    - 87.0\n",
      "    - 20.0\n",
      "    - 49.0\n",
      "    - 62.0\n",
      "    - 59.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015147572855322764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02040582264457688\n",
      "    mean_inference_ms: 0.23329045708607993\n",
      "    mean_raw_obs_processing_ms: 0.031868278246311485\n",
      "time_since_restore: 127.0365879535675\n",
      "time_this_iter_s: 2.639632225036621\n",
      "time_total_s: 127.0365879535675\n",
      "timers:\n",
      "  learn_throughput: 2830.18\n",
      "  learn_time_ms: 1413.338\n",
      "  load_throughput: 21528571.795\n",
      "  load_time_ms: 0.186\n",
      "  training_iteration_time_ms: 2645.968\n",
      "  update_time_ms: 0.643\n",
      "timestamp: 1656299510\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 192000\n",
      "training_iteration: 48\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 196000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 196000\n",
      "  num_agent_steps_trained: 196000\n",
      "  num_env_steps_sampled: 196000\n",
      "  num_env_steps_trained: 196000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-52\n",
      "done: false\n",
      "episode_len_mean: 44.91\n",
      "episode_media: {}\n",
      "episode_reward_max: 94.0\n",
      "episode_reward_mean: 43.58\n",
      "episode_reward_min: -3.0\n",
      "episodes_this_iter: 88\n",
      "episodes_total: 4811\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.0046875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0659756359874561\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02176003783029975\n",
      "        policy_loss: -0.012134328795977497\n",
      "        total_loss: 8.012515621287848\n",
      "        vf_explained_var: 0.0027247511571453463\n",
      "        vf_loss: 8.024547922995783\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 196000\n",
      "  num_agent_steps_trained: 196000\n",
      "  num_env_steps_sampled: 196000\n",
      "  num_env_steps_trained: 196000\n",
      "iterations_since_restore: 49\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 196000\n",
      "num_agent_steps_trained: 196000\n",
      "num_env_steps_sampled: 196000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 196000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.999999999999998\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015147206393039498\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020400507994579195\n",
      "  mean_inference_ms: 0.2332818942484915\n",
      "  mean_raw_obs_processing_ms: 0.03185623923259495\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 94.0\n",
      "  episode_reward_mean: 43.58\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 88\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 12\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 36\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 39.0\n",
      "    - 65.0\n",
      "    - 57.0\n",
      "    - 71.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 65.0\n",
      "    - 87.0\n",
      "    - 20.0\n",
      "    - 49.0\n",
      "    - 62.0\n",
      "    - 59.0\n",
      "    - 54.0\n",
      "    - 57.0\n",
      "    - 43.0\n",
      "    - 51.0\n",
      "    - 61.0\n",
      "    - 53.0\n",
      "    - 90.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 81.0\n",
      "    - 81.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 31.0\n",
      "    - 32.0\n",
      "    - 33.0\n",
      "    - 57.0\n",
      "    - 59.0\n",
      "    - 56.0\n",
      "    - 51.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 34.0\n",
      "    - -3.0\n",
      "    - 63.0\n",
      "    - 39.0\n",
      "    - 31.0\n",
      "    - 84.0\n",
      "    - 18.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 22.0\n",
      "    - 2.0\n",
      "    - 40.0\n",
      "    - 38.0\n",
      "    - 55.0\n",
      "    - 71.0\n",
      "    - 54.0\n",
      "    - 76.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 71.0\n",
      "    - 46.0\n",
      "    - 69.0\n",
      "    - 29.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 90.0\n",
      "    - 22.0\n",
      "    - 64.0\n",
      "    - 62.0\n",
      "    - 45.0\n",
      "    - 46.0\n",
      "    - 70.0\n",
      "    - 79.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 41.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 61.0\n",
      "    - 67.0\n",
      "    - 56.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 34.0\n",
      "    - 94.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 29.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015147206393039498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020400507994579195\n",
      "    mean_inference_ms: 0.2332818942484915\n",
      "    mean_raw_obs_processing_ms: 0.03185623923259495\n",
      "time_since_restore: 129.65225791931152\n",
      "time_this_iter_s: 2.6156699657440186\n",
      "time_total_s: 129.65225791931152\n",
      "timers:\n",
      "  learn_throughput: 2833.723\n",
      "  learn_time_ms: 1411.571\n",
      "  load_throughput: 21489965.416\n",
      "  load_time_ms: 0.186\n",
      "  training_iteration_time_ms: 2644.098\n",
      "  update_time_ms: 0.65\n",
      "timestamp: 1656299512\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 196000\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 200000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 200000\n",
      "  num_agent_steps_trained: 200000\n",
      "  num_env_steps_sampled: 200000\n",
      "  num_env_steps_trained: 200000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-55\n",
      "done: false\n",
      "episode_len_mean: 42.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 102.0\n",
      "episode_reward_mean: 39.07\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 95\n",
      "episodes_total: 4906\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.007031250000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0633755147457122\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019793076222882636\n",
      "        policy_loss: -0.011836111874029201\n",
      "        total_loss: 8.220621267954508\n",
      "        vf_explained_var: 0.002550017128708542\n",
      "        vf_loss: 8.23231821521636\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 200000\n",
      "  num_agent_steps_trained: 200000\n",
      "  num_env_steps_sampled: 200000\n",
      "  num_env_steps_trained: 200000\n",
      "iterations_since_restore: 50\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 200000\n",
      "num_agent_steps_trained: 200000\n",
      "num_env_steps_sampled: 200000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 200000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.675\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015146760730051867\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02039516730670484\n",
      "  mean_inference_ms: 0.23328740642485912\n",
      "  mean_raw_obs_processing_ms: 0.03185180450965392\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 42.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 102.0\n",
      "  episode_reward_mean: 39.07\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 95\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 8\n",
      "    - 50\n",
      "    - 32\n",
      "    - 13\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    episode_reward:\n",
      "    - 34.0\n",
      "    - 94.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 29.0\n",
      "    - 43.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 70.0\n",
      "    - 94.0\n",
      "    - 93.0\n",
      "    - 70.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 47.0\n",
      "    - 93.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 35.0\n",
      "    - 50.0\n",
      "    - 14.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 52.0\n",
      "    - 56.0\n",
      "    - 46.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 71.0\n",
      "    - 15.0\n",
      "    - 81.0\n",
      "    - 102.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 44.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 39.0\n",
      "    - 67.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 11.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 16.0\n",
      "    - 5.0\n",
      "    - 36.0\n",
      "    - 46.0\n",
      "    - 37.0\n",
      "    - 30.0\n",
      "    - 42.0\n",
      "    - 39.0\n",
      "    - 47.0\n",
      "    - 55.0\n",
      "    - 55.0\n",
      "    - 58.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 65.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 8.0\n",
      "    - 22.0\n",
      "    - 47.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 61.0\n",
      "    - 29.0\n",
      "    - 32.0\n",
      "    - 102.0\n",
      "    - 51.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 76.0\n",
      "    - 63.0\n",
      "    - 53.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 4.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015146760730051867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02039516730670484\n",
      "    mean_inference_ms: 0.23328740642485912\n",
      "    mean_raw_obs_processing_ms: 0.03185180450965392\n",
      "time_since_restore: 132.28424096107483\n",
      "time_this_iter_s: 2.6319830417633057\n",
      "time_total_s: 132.28424096107483\n",
      "timers:\n",
      "  learn_throughput: 2835.647\n",
      "  learn_time_ms: 1410.613\n",
      "  load_throughput: 21342343.213\n",
      "  load_time_ms: 0.187\n",
      "  training_iteration_time_ms: 2643.759\n",
      "  update_time_ms: 0.654\n",
      "timestamp: 1656299515\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200000\n",
      "training_iteration: 50\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 204000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 204000\n",
      "  num_agent_steps_trained: 204000\n",
      "  num_env_steps_sampled: 204000\n",
      "  num_env_steps_trained: 204000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-11-58\n",
      "done: false\n",
      "episode_len_mean: 44.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 92.0\n",
      "episode_reward_mean: 38.12\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 90\n",
      "episodes_total: 4996\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.007031250000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0300174871439576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024072860960438047\n",
      "        policy_loss: -0.013727696016631138\n",
      "        total_loss: 7.763487052917481\n",
      "        vf_explained_var: 0.0036507453328819685\n",
      "        vf_loss: 7.77704550322666\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 204000\n",
      "  num_agent_steps_trained: 204000\n",
      "  num_env_steps_sampled: 204000\n",
      "  num_env_steps_trained: 204000\n",
      "iterations_since_restore: 51\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 204000\n",
      "num_agent_steps_trained: 204000\n",
      "num_env_steps_sampled: 204000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 204000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.85\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015145698583915866\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020386922345401112\n",
      "  mean_inference_ms: 0.23325661390422822\n",
      "  mean_raw_obs_processing_ms: 0.03184010077661968\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 92.0\n",
      "  episode_reward_mean: 38.12\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 90\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 15\n",
      "    - 42\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 47.0\n",
      "    - 76.0\n",
      "    - 63.0\n",
      "    - 53.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 4.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 20.0\n",
      "    - 53.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 19.0\n",
      "    - 31.0\n",
      "    - 84.0\n",
      "    - 54.0\n",
      "    - 51.0\n",
      "    - 50.0\n",
      "    - 31.0\n",
      "    - 67.0\n",
      "    - 66.0\n",
      "    - 92.0\n",
      "    - 89.0\n",
      "    - 60.0\n",
      "    - 71.0\n",
      "    - 14.0\n",
      "    - 75.0\n",
      "    - 55.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 2.0\n",
      "    - 58.0\n",
      "    - 38.0\n",
      "    - 28.0\n",
      "    - 34.0\n",
      "    - 67.0\n",
      "    - 16.0\n",
      "    - 15.0\n",
      "    - 23.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 38.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 47.0\n",
      "    - 46.0\n",
      "    - 40.0\n",
      "    - 15.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 4.0\n",
      "    - 65.0\n",
      "    - 29.0\n",
      "    - 22.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 57.0\n",
      "    - 47.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 52.0\n",
      "    - 69.0\n",
      "    - 3.0\n",
      "    - 74.0\n",
      "    - 35.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 85.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 46.0\n",
      "    - 76.0\n",
      "    - 44.0\n",
      "    - 40.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 76.0\n",
      "    - 60.0\n",
      "    - 69.0\n",
      "    - 64.0\n",
      "    - 3.0\n",
      "    - 26.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015145698583915866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020386922345401112\n",
      "    mean_inference_ms: 0.23325661390422822\n",
      "    mean_raw_obs_processing_ms: 0.03184010077661968\n",
      "time_since_restore: 134.9207420349121\n",
      "time_this_iter_s: 2.6365010738372803\n",
      "time_total_s: 134.9207420349121\n",
      "timers:\n",
      "  learn_throughput: 2836.807\n",
      "  learn_time_ms: 1410.036\n",
      "  load_throughput: 21137981.605\n",
      "  load_time_ms: 0.189\n",
      "  training_iteration_time_ms: 2641.936\n",
      "  update_time_ms: 0.643\n",
      "timestamp: 1656299518\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 204000\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 208000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 208000\n",
      "  num_agent_steps_trained: 208000\n",
      "  num_env_steps_sampled: 208000\n",
      "  num_env_steps_trained: 208000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-00\n",
      "done: false\n",
      "episode_len_mean: 46.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 106.0\n",
      "episode_reward_mean: 42.74\n",
      "episode_reward_min: -17.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 5082\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.010546875000000002\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0345505289493069\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02044493504599618\n",
      "        policy_loss: -0.007844176806349266\n",
      "        total_loss: 8.377899154027302\n",
      "        vf_explained_var: 0.0008658027777107813\n",
      "        vf_loss: 8.385527708709882\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 208000\n",
      "  num_agent_steps_trained: 208000\n",
      "  num_env_steps_sampled: 208000\n",
      "  num_env_steps_trained: 208000\n",
      "iterations_since_restore: 52\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 208000\n",
      "num_agent_steps_trained: 208000\n",
      "num_env_steps_sampled: 208000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 208000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.466666666666669\n",
      "  ram_util_percent: 52.20000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015145042948095553\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020381823683384544\n",
      "  mean_inference_ms: 0.2332353386039638\n",
      "  mean_raw_obs_processing_ms: 0.031828594209232765\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.0\n",
      "  episode_reward_mean: 42.74\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 24\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 38\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 41.0\n",
      "    - 46.0\n",
      "    - 76.0\n",
      "    - 44.0\n",
      "    - 40.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 76.0\n",
      "    - 60.0\n",
      "    - 69.0\n",
      "    - 64.0\n",
      "    - 3.0\n",
      "    - 26.0\n",
      "    - 76.0\n",
      "    - 62.0\n",
      "    - 31.0\n",
      "    - 49.0\n",
      "    - 64.0\n",
      "    - 75.0\n",
      "    - 45.0\n",
      "    - 23.0\n",
      "    - 67.0\n",
      "    - 3.0\n",
      "    - 26.0\n",
      "    - 56.0\n",
      "    - 65.0\n",
      "    - 55.0\n",
      "    - 21.0\n",
      "    - 58.0\n",
      "    - 20.0\n",
      "    - 22.0\n",
      "    - 51.0\n",
      "    - 49.0\n",
      "    - 61.0\n",
      "    - 4.0\n",
      "    - 33.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 8.0\n",
      "    - 60.0\n",
      "    - 48.0\n",
      "    - 62.0\n",
      "    - 46.0\n",
      "    - 51.0\n",
      "    - 49.0\n",
      "    - 66.0\n",
      "    - 69.0\n",
      "    - 22.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 42.0\n",
      "    - 21.0\n",
      "    - 45.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 20.0\n",
      "    - 74.0\n",
      "    - 59.0\n",
      "    - 63.0\n",
      "    - 52.0\n",
      "    - 38.0\n",
      "    - 58.0\n",
      "    - 102.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 27.0\n",
      "    - 90.0\n",
      "    - 5.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 20.0\n",
      "    - 76.0\n",
      "    - 70.0\n",
      "    - 53.0\n",
      "    - 34.0\n",
      "    - 75.0\n",
      "    - 16.0\n",
      "    - 28.0\n",
      "    - 106.0\n",
      "    - 59.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - -17.0\n",
      "    - 72.0\n",
      "    - 72.0\n",
      "    - 104.0\n",
      "    - 2.0\n",
      "    - 41.0\n",
      "    - 87.0\n",
      "    - 85.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015145042948095553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020381823683384544\n",
      "    mean_inference_ms: 0.2332353386039638\n",
      "    mean_raw_obs_processing_ms: 0.031828594209232765\n",
      "time_since_restore: 137.54599022865295\n",
      "time_this_iter_s: 2.6252481937408447\n",
      "time_total_s: 137.54599022865295\n",
      "timers:\n",
      "  learn_throughput: 2839.217\n",
      "  learn_time_ms: 1408.839\n",
      "  load_throughput: 21296288.398\n",
      "  load_time_ms: 0.188\n",
      "  training_iteration_time_ms: 2640.368\n",
      "  update_time_ms: 0.656\n",
      "timestamp: 1656299520\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 208000\n",
      "training_iteration: 52\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 212000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 212000\n",
      "  num_agent_steps_trained: 212000\n",
      "  num_env_steps_sampled: 212000\n",
      "  num_env_steps_trained: 212000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-03\n",
      "done: false\n",
      "episode_len_mean: 43.35\n",
      "episode_media: {}\n",
      "episode_reward_max: 104.0\n",
      "episode_reward_mean: 43.37\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 94\n",
      "episodes_total: 5176\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.015820312499999996\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0420683133345778\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02012551618659849\n",
      "        policy_loss: -0.017375166217486065\n",
      "        total_loss: 8.089352152424475\n",
      "        vf_explained_var: 0.00406331874990976\n",
      "        vf_loss: 8.106408885217482\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 212000\n",
      "  num_agent_steps_trained: 212000\n",
      "  num_env_steps_sampled: 212000\n",
      "  num_env_steps_trained: 212000\n",
      "iterations_since_restore: 53\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 212000\n",
      "num_agent_steps_trained: 212000\n",
      "num_env_steps_sampled: 212000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 212000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.575\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015146258307991965\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020377841372234657\n",
      "  mean_inference_ms: 0.23325644579841337\n",
      "  mean_raw_obs_processing_ms: 0.031829836810229356\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 104.0\n",
      "  episode_reward_mean: 43.37\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 94\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 26\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 10\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 25\n",
      "    - 34\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 16\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 49\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 48\n",
      "    - 13\n",
      "    - 10\n",
      "    episode_reward:\n",
      "    - 104.0\n",
      "    - 2.0\n",
      "    - 41.0\n",
      "    - 87.0\n",
      "    - 85.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 56.0\n",
      "    - 73.0\n",
      "    - 57.0\n",
      "    - 30.0\n",
      "    - 50.0\n",
      "    - 24.0\n",
      "    - 42.0\n",
      "    - 46.0\n",
      "    - 44.0\n",
      "    - 47.0\n",
      "    - 87.0\n",
      "    - 5.0\n",
      "    - 64.0\n",
      "    - 57.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 62.0\n",
      "    - 70.0\n",
      "    - 44.0\n",
      "    - 2.0\n",
      "    - 35.0\n",
      "    - 58.0\n",
      "    - 33.0\n",
      "    - 79.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 36.0\n",
      "    - 51.0\n",
      "    - 44.0\n",
      "    - 65.0\n",
      "    - 83.0\n",
      "    - 71.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 26.0\n",
      "    - 58.0\n",
      "    - 81.0\n",
      "    - 2.0\n",
      "    - 54.0\n",
      "    - 92.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 87.0\n",
      "    - 45.0\n",
      "    - 40.0\n",
      "    - 85.0\n",
      "    - 73.0\n",
      "    - 15.0\n",
      "    - 24.0\n",
      "    - 45.0\n",
      "    - 36.0\n",
      "    - 39.0\n",
      "    - 40.0\n",
      "    - 65.0\n",
      "    - 73.0\n",
      "    - 51.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 2.0\n",
      "    - 43.0\n",
      "    - 68.0\n",
      "    - 41.0\n",
      "    - 28.0\n",
      "    - 75.0\n",
      "    - 86.0\n",
      "    - 56.0\n",
      "    - 96.0\n",
      "    - 63.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 67.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015146258307991965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020377841372234657\n",
      "    mean_inference_ms: 0.23325644579841337\n",
      "    mean_raw_obs_processing_ms: 0.031829836810229356\n",
      "time_since_restore: 140.18217611312866\n",
      "time_this_iter_s: 2.636185884475708\n",
      "time_total_s: 140.18217611312866\n",
      "timers:\n",
      "  learn_throughput: 2839.573\n",
      "  learn_time_ms: 1408.663\n",
      "  load_throughput: 22304195.693\n",
      "  load_time_ms: 0.179\n",
      "  training_iteration_time_ms: 2641.445\n",
      "  update_time_ms: 0.652\n",
      "timestamp: 1656299523\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 212000\n",
      "training_iteration: 53\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 216000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 216000\n",
      "  num_agent_steps_trained: 216000\n",
      "  num_env_steps_sampled: 216000\n",
      "  num_env_steps_trained: 216000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-06\n",
      "done: false\n",
      "episode_len_mean: 43.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 95.0\n",
      "episode_reward_mean: 41.83\n",
      "episode_reward_min: -25.0\n",
      "episodes_this_iter: 89\n",
      "episodes_total: 5265\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.023730468750000008\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0438477785997493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019995118237991806\n",
      "        policy_loss: -0.010530821896929254\n",
      "        total_loss: 8.201771460297287\n",
      "        vf_explained_var: 0.000945293326531687\n",
      "        vf_loss: 8.211827767023475\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 216000\n",
      "  num_agent_steps_trained: 216000\n",
      "  num_env_steps_sampled: 216000\n",
      "  num_env_steps_trained: 216000\n",
      "iterations_since_restore: 54\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 216000\n",
      "num_agent_steps_trained: 216000\n",
      "num_env_steps_sampled: 216000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 216000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.6\n",
      "  ram_util_percent: 52.125\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015146272293554702\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020372076701278913\n",
      "  mean_inference_ms: 0.23324683766131216\n",
      "  mean_raw_obs_processing_ms: 0.031822861712150444\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.0\n",
      "  episode_reward_mean: 41.83\n",
      "  episode_reward_min: -25.0\n",
      "  episodes_this_iter: 89\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 48\n",
      "    - 13\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 6\n",
      "    - 50\n",
      "    - 10\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 45\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 63.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 67.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 44.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 47.0\n",
      "    - 56.0\n",
      "    - 59.0\n",
      "    - 80.0\n",
      "    - 63.0\n",
      "    - 65.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 94.0\n",
      "    - 71.0\n",
      "    - 66.0\n",
      "    - 60.0\n",
      "    - 53.0\n",
      "    - 61.0\n",
      "    - 82.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 51.0\n",
      "    - 52.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - -25.0\n",
      "    - 20.0\n",
      "    - 54.0\n",
      "    - 44.0\n",
      "    - 51.0\n",
      "    - 20.0\n",
      "    - 48.0\n",
      "    - 22.0\n",
      "    - 74.0\n",
      "    - 39.0\n",
      "    - 90.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 8.0\n",
      "    - 45.0\n",
      "    - 95.0\n",
      "    - 76.0\n",
      "    - 70.0\n",
      "    - 61.0\n",
      "    - 76.0\n",
      "    - 45.0\n",
      "    - 68.0\n",
      "    - 44.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 71.0\n",
      "    - 34.0\n",
      "    - 69.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 76.0\n",
      "    - 44.0\n",
      "    - 59.0\n",
      "    - 52.0\n",
      "    - 26.0\n",
      "    - 4.0\n",
      "    - 31.0\n",
      "    - 91.0\n",
      "    - 24.0\n",
      "    - 62.0\n",
      "    - 5.0\n",
      "    - 43.0\n",
      "    - 25.0\n",
      "    - 52.0\n",
      "    - 73.0\n",
      "    - 28.0\n",
      "    - 65.0\n",
      "    - 44.0\n",
      "    - 70.0\n",
      "    - 84.0\n",
      "    - 14.0\n",
      "    - 58.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015146272293554702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020372076701278913\n",
      "    mean_inference_ms: 0.23324683766131216\n",
      "    mean_raw_obs_processing_ms: 0.031822861712150444\n",
      "time_since_restore: 142.81374526023865\n",
      "time_this_iter_s: 2.6315691471099854\n",
      "time_total_s: 142.81374526023865\n",
      "timers:\n",
      "  learn_throughput: 2847.252\n",
      "  learn_time_ms: 1404.863\n",
      "  load_throughput: 22363657.691\n",
      "  load_time_ms: 0.179\n",
      "  training_iteration_time_ms: 2636.818\n",
      "  update_time_ms: 0.661\n",
      "timestamp: 1656299526\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 216000\n",
      "training_iteration: 54\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 220000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 220000\n",
      "  num_agent_steps_trained: 220000\n",
      "  num_env_steps_sampled: 220000\n",
      "  num_env_steps_trained: 220000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-08\n",
      "done: false\n",
      "episode_len_mean: 45.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 92.0\n",
      "episode_reward_mean: 42.52\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 88\n",
      "episodes_total: 5353\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.023730468750000008\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0167633649482521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022738212684509894\n",
      "        policy_loss: -0.014077382661922965\n",
      "        total_loss: 7.816023977853918\n",
      "        vf_explained_var: 0.0042879861529155445\n",
      "        vf_loss: 7.829561778550507\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 220000\n",
      "  num_agent_steps_trained: 220000\n",
      "  num_env_steps_sampled: 220000\n",
      "  num_env_steps_trained: 220000\n",
      "iterations_since_restore: 55\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 220000\n",
      "num_agent_steps_trained: 220000\n",
      "num_env_steps_sampled: 220000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 220000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.075\n",
      "  ram_util_percent: 52.175\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015145907889780484\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020368169476103495\n",
      "  mean_inference_ms: 0.2332344949903603\n",
      "  mean_raw_obs_processing_ms: 0.031817388414660315\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 92.0\n",
      "  episode_reward_mean: 42.52\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 88\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 22\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 31\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 38\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 5.0\n",
      "    - 43.0\n",
      "    - 25.0\n",
      "    - 52.0\n",
      "    - 73.0\n",
      "    - 28.0\n",
      "    - 65.0\n",
      "    - 44.0\n",
      "    - 70.0\n",
      "    - 84.0\n",
      "    - 14.0\n",
      "    - 58.0\n",
      "    - 35.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 91.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 76.0\n",
      "    - 92.0\n",
      "    - 45.0\n",
      "    - 78.0\n",
      "    - 41.0\n",
      "    - 44.0\n",
      "    - 47.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 40.0\n",
      "    - 57.0\n",
      "    - 15.0\n",
      "    - 64.0\n",
      "    - 24.0\n",
      "    - 15.0\n",
      "    - 69.0\n",
      "    - 57.0\n",
      "    - 46.0\n",
      "    - 77.0\n",
      "    - 25.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 62.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 2.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 42.0\n",
      "    - 32.0\n",
      "    - 61.0\n",
      "    - 5.0\n",
      "    - 10.0\n",
      "    - 25.0\n",
      "    - 34.0\n",
      "    - 83.0\n",
      "    - 77.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 75.0\n",
      "    - 27.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 46.0\n",
      "    - 72.0\n",
      "    - 72.0\n",
      "    - 25.0\n",
      "    - 14.0\n",
      "    - 44.0\n",
      "    - 84.0\n",
      "    - 69.0\n",
      "    - 65.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 27.0\n",
      "    - 46.0\n",
      "    - 25.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 66.0\n",
      "    - 68.0\n",
      "    - 18.0\n",
      "    - 37.0\n",
      "    - 78.0\n",
      "    - 77.0\n",
      "    - 27.0\n",
      "    - 44.0\n",
      "    - 87.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015145907889780484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020368169476103495\n",
      "    mean_inference_ms: 0.2332344949903603\n",
      "    mean_raw_obs_processing_ms: 0.031817388414660315\n",
      "time_since_restore: 145.44144463539124\n",
      "time_this_iter_s: 2.627699375152588\n",
      "time_total_s: 145.44144463539124\n",
      "timers:\n",
      "  learn_throughput: 2853.485\n",
      "  learn_time_ms: 1401.795\n",
      "  load_throughput: 22489565.684\n",
      "  load_time_ms: 0.178\n",
      "  training_iteration_time_ms: 2632.104\n",
      "  update_time_ms: 0.654\n",
      "timestamp: 1656299528\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 220000\n",
      "training_iteration: 55\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 224000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 224000\n",
      "  num_agent_steps_trained: 224000\n",
      "  num_env_steps_sampled: 224000\n",
      "  num_env_steps_trained: 224000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-11\n",
      "done: false\n",
      "episode_len_mean: 46.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 100.0\n",
      "episode_reward_mean: 48.01\n",
      "episode_reward_min: -12.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 5439\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.03559570312499999\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0178925196970663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021825808525005336\n",
      "        policy_loss: -0.009325095728760766\n",
      "        total_loss: 8.547517250942928\n",
      "        vf_explained_var: 0.0019064760336311914\n",
      "        vf_loss: 8.556065445561563\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 224000\n",
      "  num_agent_steps_trained: 224000\n",
      "  num_env_steps_sampled: 224000\n",
      "  num_env_steps_trained: 224000\n",
      "iterations_since_restore: 56\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 224000\n",
      "num_agent_steps_trained: 224000\n",
      "num_env_steps_sampled: 224000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 224000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.0\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015148774439739433\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020369195021538535\n",
      "  mean_inference_ms: 0.23326993926600628\n",
      "  mean_raw_obs_processing_ms: 0.031818935147185846\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.0\n",
      "  episode_reward_mean: 48.01\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 46.0\n",
      "    - 25.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 66.0\n",
      "    - 68.0\n",
      "    - 18.0\n",
      "    - 37.0\n",
      "    - 78.0\n",
      "    - 77.0\n",
      "    - 27.0\n",
      "    - 44.0\n",
      "    - 87.0\n",
      "    - 3.0\n",
      "    - 79.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 39.0\n",
      "    - 67.0\n",
      "    - 44.0\n",
      "    - 71.0\n",
      "    - 68.0\n",
      "    - 47.0\n",
      "    - 52.0\n",
      "    - 67.0\n",
      "    - 54.0\n",
      "    - 19.0\n",
      "    - 44.0\n",
      "    - 71.0\n",
      "    - 45.0\n",
      "    - 47.0\n",
      "    - 47.0\n",
      "    - 83.0\n",
      "    - 29.0\n",
      "    - 57.0\n",
      "    - 46.0\n",
      "    - 39.0\n",
      "    - 72.0\n",
      "    - 68.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 31.0\n",
      "    - 48.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 45.0\n",
      "    - 46.0\n",
      "    - 49.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 100.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 70.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 20.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 82.0\n",
      "    - 50.0\n",
      "    - 47.0\n",
      "    - 49.0\n",
      "    - 37.0\n",
      "    - 33.0\n",
      "    - 72.0\n",
      "    - 27.0\n",
      "    - 81.0\n",
      "    - 64.0\n",
      "    - 53.0\n",
      "    - 33.0\n",
      "    - 28.0\n",
      "    - 88.0\n",
      "    - 55.0\n",
      "    - 69.0\n",
      "    - 91.0\n",
      "    - 82.0\n",
      "    - 11.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 40.0\n",
      "    - -12.0\n",
      "    - 85.0\n",
      "    - 30.0\n",
      "    - 40.0\n",
      "    - 52.0\n",
      "    - 83.0\n",
      "    - 82.0\n",
      "    - 69.0\n",
      "    - 37.0\n",
      "    - 97.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015148774439739433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020369195021538535\n",
      "    mean_inference_ms: 0.23326993926600628\n",
      "    mean_raw_obs_processing_ms: 0.031818935147185846\n",
      "time_since_restore: 148.11733984947205\n",
      "time_this_iter_s: 2.6758952140808105\n",
      "time_total_s: 148.11733984947205\n",
      "timers:\n",
      "  learn_throughput: 2852.005\n",
      "  learn_time_ms: 1402.522\n",
      "  load_throughput: 22274583.112\n",
      "  load_time_ms: 0.18\n",
      "  training_iteration_time_ms: 2634.501\n",
      "  update_time_ms: 0.648\n",
      "timestamp: 1656299531\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 224000\n",
      "training_iteration: 56\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 228000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 228000\n",
      "  num_agent_steps_trained: 228000\n",
      "  num_env_steps_sampled: 228000\n",
      "  num_env_steps_trained: 228000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-14\n",
      "done: false\n",
      "episode_len_mean: 45.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 97.0\n",
      "episode_reward_mean: 41.38\n",
      "episode_reward_min: -12.0\n",
      "episodes_this_iter: 90\n",
      "episodes_total: 5529\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.05339355468749998\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0315427179618548\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018764399633973194\n",
      "        policy_loss: -0.01156313990953789\n",
      "        total_loss: 8.16750135114116\n",
      "        vf_explained_var: 0.0008718057345318538\n",
      "        vf_loss: 8.178062583554176\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 228000\n",
      "  num_agent_steps_trained: 228000\n",
      "  num_env_steps_sampled: 228000\n",
      "  num_env_steps_trained: 228000\n",
      "iterations_since_restore: 57\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 228000\n",
      "num_agent_steps_trained: 228000\n",
      "num_env_steps_sampled: 228000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 228000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.299999999999999\n",
      "  ram_util_percent: 52.13333333333333\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015148390474334356\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020363342164336985\n",
      "  mean_inference_ms: 0.23325354830036893\n",
      "  mean_raw_obs_processing_ms: 0.0318112553023792\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 97.0\n",
      "  episode_reward_mean: 41.38\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 90\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 10\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - -12.0\n",
      "    - 85.0\n",
      "    - 30.0\n",
      "    - 40.0\n",
      "    - 52.0\n",
      "    - 83.0\n",
      "    - 82.0\n",
      "    - 69.0\n",
      "    - 37.0\n",
      "    - 97.0\n",
      "    - 77.0\n",
      "    - 85.0\n",
      "    - 23.0\n",
      "    - 48.0\n",
      "    - 75.0\n",
      "    - 29.0\n",
      "    - 73.0\n",
      "    - 52.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 23.0\n",
      "    - 36.0\n",
      "    - 31.0\n",
      "    - 65.0\n",
      "    - 28.0\n",
      "    - 47.0\n",
      "    - 40.0\n",
      "    - 6.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 46.0\n",
      "    - 39.0\n",
      "    - 67.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 82.0\n",
      "    - 63.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 45.0\n",
      "    - 65.0\n",
      "    - 61.0\n",
      "    - 38.0\n",
      "    - 17.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 53.0\n",
      "    - 73.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 51.0\n",
      "    - 31.0\n",
      "    - 55.0\n",
      "    - 69.0\n",
      "    - 27.0\n",
      "    - 33.0\n",
      "    - 50.0\n",
      "    - 52.0\n",
      "    - 74.0\n",
      "    - 33.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 35.0\n",
      "    - 66.0\n",
      "    - 59.0\n",
      "    - 55.0\n",
      "    - 59.0\n",
      "    - 42.0\n",
      "    - 71.0\n",
      "    - 56.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 54.0\n",
      "    - 63.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 48.0\n",
      "    - 31.0\n",
      "    - 36.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015148390474334356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020363342164336985\n",
      "    mean_inference_ms: 0.23325354830036893\n",
      "    mean_raw_obs_processing_ms: 0.0318112553023792\n",
      "time_since_restore: 150.74993801116943\n",
      "time_this_iter_s: 2.6325981616973877\n",
      "time_total_s: 150.74993801116943\n",
      "timers:\n",
      "  learn_throughput: 2853.71\n",
      "  learn_time_ms: 1401.684\n",
      "  load_throughput: 23703328.624\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2632.873\n",
      "  update_time_ms: 0.646\n",
      "timestamp: 1656299534\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 228000\n",
      "training_iteration: 57\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 232000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 232000\n",
      "  num_agent_steps_trained: 232000\n",
      "  num_env_steps_sampled: 232000\n",
      "  num_env_steps_trained: 232000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-16\n",
      "done: false\n",
      "episode_len_mean: 46.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 98.0\n",
      "episode_reward_mean: 43.3\n",
      "episode_reward_min: -11.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 5615\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.05339355468749998\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0227654708969978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021116545345504036\n",
      "        policy_loss: -0.007917111611334227\n",
      "        total_loss: 8.051841288740917\n",
      "        vf_explained_var: 0.002701356077706942\n",
      "        vf_loss: 8.058630923814672\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 232000\n",
      "  num_agent_steps_trained: 232000\n",
      "  num_env_steps_sampled: 232000\n",
      "  num_env_steps_trained: 232000\n",
      "iterations_since_restore: 58\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 232000\n",
      "num_agent_steps_trained: 232000\n",
      "num_env_steps_sampled: 232000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 232000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.775\n",
      "  ram_util_percent: 52.2\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015148344383003933\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020362030938110235\n",
      "  mean_inference_ms: 0.23325997865655246\n",
      "  mean_raw_obs_processing_ms: 0.031807916075566454\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 98.0\n",
      "  episode_reward_mean: 43.3\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 48\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 56.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 54.0\n",
      "    - 63.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 48.0\n",
      "    - 31.0\n",
      "    - 36.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 60.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 84.0\n",
      "    - 37.0\n",
      "    - 46.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 57.0\n",
      "    - 19.0\n",
      "    - 75.0\n",
      "    - 72.0\n",
      "    - 41.0\n",
      "    - 54.0\n",
      "    - 55.0\n",
      "    - 76.0\n",
      "    - 67.0\n",
      "    - 43.0\n",
      "    - 64.0\n",
      "    - 40.0\n",
      "    - 38.0\n",
      "    - 24.0\n",
      "    - 64.0\n",
      "    - 63.0\n",
      "    - 50.0\n",
      "    - 72.0\n",
      "    - 34.0\n",
      "    - 46.0\n",
      "    - -11.0\n",
      "    - 53.0\n",
      "    - 25.0\n",
      "    - 73.0\n",
      "    - 46.0\n",
      "    - 85.0\n",
      "    - 83.0\n",
      "    - 39.0\n",
      "    - 34.0\n",
      "    - 22.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 11.0\n",
      "    - 31.0\n",
      "    - 53.0\n",
      "    - 50.0\n",
      "    - 78.0\n",
      "    - 40.0\n",
      "    - 54.0\n",
      "    - 60.0\n",
      "    - 60.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 33.0\n",
      "    - 59.0\n",
      "    - 84.0\n",
      "    - 58.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 56.0\n",
      "    - 87.0\n",
      "    - 30.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 98.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 74.0\n",
      "    - 49.0\n",
      "    - 81.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015148344383003933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020362030938110235\n",
      "    mean_inference_ms: 0.23325997865655246\n",
      "    mean_raw_obs_processing_ms: 0.031807916075566454\n",
      "time_since_restore: 153.39553713798523\n",
      "time_this_iter_s: 2.645599126815796\n",
      "time_total_s: 153.39553713798523\n",
      "timers:\n",
      "  learn_throughput: 2854.167\n",
      "  learn_time_ms: 1401.46\n",
      "  load_throughput: 23720084.83\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2633.462\n",
      "  update_time_ms: 0.67\n",
      "timestamp: 1656299536\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 232000\n",
      "training_iteration: 58\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 236000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 236000\n",
      "  num_agent_steps_trained: 236000\n",
      "  num_env_steps_sampled: 236000\n",
      "  num_env_steps_trained: 236000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-19\n",
      "done: false\n",
      "episode_len_mean: 46.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 98.0\n",
      "episode_reward_mean: 45.9\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 85\n",
      "episodes_total: 5700\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.08009033203125\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0285239331183895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02096163471259417\n",
      "        policy_loss: -0.01570495518365054\n",
      "        total_loss: 8.419806268650998\n",
      "        vf_explained_var: 0.0020530674406277235\n",
      "        vf_loss: 8.433832395204934\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 236000\n",
      "  num_agent_steps_trained: 236000\n",
      "  num_env_steps_sampled: 236000\n",
      "  num_env_steps_trained: 236000\n",
      "iterations_since_restore: 59\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 236000\n",
      "num_agent_steps_trained: 236000\n",
      "num_env_steps_sampled: 236000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 236000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.575\n",
      "  ram_util_percent: 51.85000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015147788941484653\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020357632872089977\n",
      "  mean_inference_ms: 0.23323998585768485\n",
      "  mean_raw_obs_processing_ms: 0.031798723255116776\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 98.0\n",
      "  episode_reward_mean: 45.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 85\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 32\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 87.0\n",
      "    - 30.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 98.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 74.0\n",
      "    - 49.0\n",
      "    - 81.0\n",
      "    - 87.0\n",
      "    - 96.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 19.0\n",
      "    - 39.0\n",
      "    - 25.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 33.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 37.0\n",
      "    - 49.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 55.0\n",
      "    - 37.0\n",
      "    - 39.0\n",
      "    - 80.0\n",
      "    - 79.0\n",
      "    - 68.0\n",
      "    - 45.0\n",
      "    - 53.0\n",
      "    - 2.0\n",
      "    - 27.0\n",
      "    - 27.0\n",
      "    - 56.0\n",
      "    - 51.0\n",
      "    - 68.0\n",
      "    - 52.0\n",
      "    - 86.0\n",
      "    - 67.0\n",
      "    - 57.0\n",
      "    - 33.0\n",
      "    - 35.0\n",
      "    - 46.0\n",
      "    - 75.0\n",
      "    - 30.0\n",
      "    - 32.0\n",
      "    - 42.0\n",
      "    - 54.0\n",
      "    - 51.0\n",
      "    - 90.0\n",
      "    - 16.0\n",
      "    - 21.0\n",
      "    - 11.0\n",
      "    - 61.0\n",
      "    - 25.0\n",
      "    - 59.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 43.0\n",
      "    - 30.0\n",
      "    - 26.0\n",
      "    - 32.0\n",
      "    - 77.0\n",
      "    - 86.0\n",
      "    - 50.0\n",
      "    - 60.0\n",
      "    - 38.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 77.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 56.0\n",
      "    - 28.0\n",
      "    - 42.0\n",
      "    - 92.0\n",
      "    - 80.0\n",
      "    - 49.0\n",
      "    - 71.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015147788941484653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020357632872089977\n",
      "    mean_inference_ms: 0.23323998585768485\n",
      "    mean_raw_obs_processing_ms: 0.031798723255116776\n",
      "time_since_restore: 156.01355814933777\n",
      "time_this_iter_s: 2.618021011352539\n",
      "time_total_s: 156.01355814933777\n",
      "timers:\n",
      "  learn_throughput: 2852.756\n",
      "  learn_time_ms: 1402.153\n",
      "  load_throughput: 23713379.505\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2633.695\n",
      "  update_time_ms: 0.663\n",
      "timestamp: 1656299539\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 236000\n",
      "training_iteration: 59\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 240000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 240000\n",
      "  num_agent_steps_trained: 240000\n",
      "  num_env_steps_sampled: 240000\n",
      "  num_env_steps_trained: 240000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-22\n",
      "done: false\n",
      "episode_len_mean: 46.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 106.0\n",
      "episode_reward_mean: 47.15\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 5787\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.12013549804687497\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0367074417170659\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01906834767069868\n",
      "        policy_loss: -0.014534314433413168\n",
      "        total_loss: 8.781964280015679\n",
      "        vf_explained_var: 0.0015556999432143346\n",
      "        vf_loss: 8.794207812893776\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 240000\n",
      "  num_agent_steps_trained: 240000\n",
      "  num_env_steps_sampled: 240000\n",
      "  num_env_steps_trained: 240000\n",
      "iterations_since_restore: 60\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 240000\n",
      "num_agent_steps_trained: 240000\n",
      "num_env_steps_sampled: 240000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 240000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.05\n",
      "  ram_util_percent: 51.775000000000006\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015146611718478593\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020353271257567612\n",
      "  mean_inference_ms: 0.2332144971038385\n",
      "  mean_raw_obs_processing_ms: 0.03178822354666546\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.0\n",
      "  episode_reward_mean: 47.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 4\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 56.0\n",
      "    - 28.0\n",
      "    - 42.0\n",
      "    - 92.0\n",
      "    - 80.0\n",
      "    - 49.0\n",
      "    - 71.0\n",
      "    - 46.0\n",
      "    - 8.0\n",
      "    - 78.0\n",
      "    - 58.0\n",
      "    - 64.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 85.0\n",
      "    - 69.0\n",
      "    - 70.0\n",
      "    - 73.0\n",
      "    - 94.0\n",
      "    - 75.0\n",
      "    - 81.0\n",
      "    - 37.0\n",
      "    - 58.0\n",
      "    - 59.0\n",
      "    - 53.0\n",
      "    - 32.0\n",
      "    - 55.0\n",
      "    - 81.0\n",
      "    - 10.0\n",
      "    - 39.0\n",
      "    - 45.0\n",
      "    - 41.0\n",
      "    - 41.0\n",
      "    - 99.0\n",
      "    - 13.0\n",
      "    - 53.0\n",
      "    - 14.0\n",
      "    - 50.0\n",
      "    - 68.0\n",
      "    - 58.0\n",
      "    - 46.0\n",
      "    - 63.0\n",
      "    - 39.0\n",
      "    - 67.0\n",
      "    - 27.0\n",
      "    - 44.0\n",
      "    - 53.0\n",
      "    - 35.0\n",
      "    - 24.0\n",
      "    - 27.0\n",
      "    - 19.0\n",
      "    - 83.0\n",
      "    - 49.0\n",
      "    - 24.0\n",
      "    - 54.0\n",
      "    - 28.0\n",
      "    - 82.0\n",
      "    - 57.0\n",
      "    - 46.0\n",
      "    - 42.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 10.0\n",
      "    - 59.0\n",
      "    - 32.0\n",
      "    - 85.0\n",
      "    - 23.0\n",
      "    - 22.0\n",
      "    - 94.0\n",
      "    - 46.0\n",
      "    - 66.0\n",
      "    - 8.0\n",
      "    - 23.0\n",
      "    - 14.0\n",
      "    - 32.0\n",
      "    - 43.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 106.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 38.0\n",
      "    - 72.0\n",
      "    - 47.0\n",
      "    - 64.0\n",
      "    - 4.0\n",
      "    - 49.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015146611718478593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020353271257567612\n",
      "    mean_inference_ms: 0.2332144971038385\n",
      "    mean_raw_obs_processing_ms: 0.03178822354666546\n",
      "time_since_restore: 158.6589891910553\n",
      "time_this_iter_s: 2.6454310417175293\n",
      "time_total_s: 158.6589891910553\n",
      "timers:\n",
      "  learn_throughput: 2848.055\n",
      "  learn_time_ms: 1404.467\n",
      "  load_throughput: 23865172.119\n",
      "  load_time_ms: 0.168\n",
      "  training_iteration_time_ms: 2635.045\n",
      "  update_time_ms: 0.658\n",
      "timestamp: 1656299542\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 240000\n",
      "training_iteration: 60\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 244000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 244000\n",
      "  num_agent_steps_trained: 244000\n",
      "  num_env_steps_sampled: 244000\n",
      "  num_env_steps_trained: 244000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-24\n",
      "done: false\n",
      "episode_len_mean: 44.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 96.0\n",
      "episode_reward_mean: 46.33\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 89\n",
      "episodes_total: 5876\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.12013549804687497\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0350880929218826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020404847976701553\n",
      "        policy_loss: -0.014023687474189266\n",
      "        total_loss: 8.48624413244186\n",
      "        vf_explained_var: 0.002873939403923609\n",
      "        vf_loss: 8.497816484717912\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 244000\n",
      "  num_agent_steps_trained: 244000\n",
      "  num_env_steps_sampled: 244000\n",
      "  num_env_steps_trained: 244000\n",
      "iterations_since_restore: 61\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 244000\n",
      "num_agent_steps_trained: 244000\n",
      "num_env_steps_sampled: 244000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 244000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.799999999999999\n",
      "  ram_util_percent: 51.73333333333333\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015145304248163915\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02034731562851149\n",
      "  mean_inference_ms: 0.2331973235562281\n",
      "  mean_raw_obs_processing_ms: 0.031782233564961355\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 96.0\n",
      "  episode_reward_mean: 46.33\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 89\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 29\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 18\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 38.0\n",
      "    - 72.0\n",
      "    - 47.0\n",
      "    - 64.0\n",
      "    - 4.0\n",
      "    - 49.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 25.0\n",
      "    - 94.0\n",
      "    - 1.0\n",
      "    - 79.0\n",
      "    - 45.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 73.0\n",
      "    - 80.0\n",
      "    - 74.0\n",
      "    - 42.0\n",
      "    - 47.0\n",
      "    - 56.0\n",
      "    - 60.0\n",
      "    - 78.0\n",
      "    - 10.0\n",
      "    - 46.0\n",
      "    - 51.0\n",
      "    - 88.0\n",
      "    - 62.0\n",
      "    - 52.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 53.0\n",
      "    - 85.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 64.0\n",
      "    - 58.0\n",
      "    - 53.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 24.0\n",
      "    - 27.0\n",
      "    - 46.0\n",
      "    - 96.0\n",
      "    - 86.0\n",
      "    - 42.0\n",
      "    - 24.0\n",
      "    - 57.0\n",
      "    - 93.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 71.0\n",
      "    - 55.0\n",
      "    - 65.0\n",
      "    - 42.0\n",
      "    - 70.0\n",
      "    - 55.0\n",
      "    - 66.0\n",
      "    - 40.0\n",
      "    - 27.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 56.0\n",
      "    - 43.0\n",
      "    - 47.0\n",
      "    - 84.0\n",
      "    - 67.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 22.0\n",
      "    - 68.0\n",
      "    - 42.0\n",
      "    - 68.0\n",
      "    - 90.0\n",
      "    - 52.0\n",
      "    - 24.0\n",
      "    - 88.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 37.0\n",
      "    - 43.0\n",
      "    - 18.0\n",
      "    - 6.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015145304248163915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02034731562851149\n",
      "    mean_inference_ms: 0.2331973235562281\n",
      "    mean_raw_obs_processing_ms: 0.031782233564961355\n",
      "time_since_restore: 161.2957022190094\n",
      "time_this_iter_s: 2.6367130279541016\n",
      "time_total_s: 161.2957022190094\n",
      "timers:\n",
      "  learn_throughput: 2848.878\n",
      "  learn_time_ms: 1404.062\n",
      "  load_throughput: 23726793.947\n",
      "  load_time_ms: 0.169\n",
      "  training_iteration_time_ms: 2635.076\n",
      "  update_time_ms: 0.684\n",
      "timestamp: 1656299544\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 244000\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 248000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 248000\n",
      "  num_agent_steps_trained: 248000\n",
      "  num_env_steps_sampled: 248000\n",
      "  num_env_steps_trained: 248000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-27\n",
      "done: false\n",
      "episode_len_mean: 45.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 107.0\n",
      "episode_reward_mean: 46.06\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 89\n",
      "episodes_total: 5965\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.056763618735857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015619324667940056\n",
      "        policy_loss: -0.007091418638705246\n",
      "        total_loss: 8.138827329040856\n",
      "        vf_explained_var: 0.002697118700191539\n",
      "        vf_loss: 8.143104093305526\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 248000\n",
      "  num_agent_steps_trained: 248000\n",
      "  num_env_steps_sampled: 248000\n",
      "  num_env_steps_trained: 248000\n",
      "iterations_since_restore: 62\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 248000\n",
      "num_agent_steps_trained: 248000\n",
      "num_env_steps_sampled: 248000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 248000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.25\n",
      "  ram_util_percent: 51.75\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015145126052179434\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020342611819799017\n",
      "  mean_inference_ms: 0.23318550391268034\n",
      "  mean_raw_obs_processing_ms: 0.031777749404661965\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 107.0\n",
      "  episode_reward_mean: 46.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 89\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 12\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 90.0\n",
      "    - 52.0\n",
      "    - 24.0\n",
      "    - 88.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 37.0\n",
      "    - 43.0\n",
      "    - 18.0\n",
      "    - 6.0\n",
      "    - 79.0\n",
      "    - 59.0\n",
      "    - 41.0\n",
      "    - 57.0\n",
      "    - 47.0\n",
      "    - 80.0\n",
      "    - 44.0\n",
      "    - 38.0\n",
      "    - 2.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 68.0\n",
      "    - 70.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 63.0\n",
      "    - 44.0\n",
      "    - 67.0\n",
      "    - 59.0\n",
      "    - 66.0\n",
      "    - 46.0\n",
      "    - 58.0\n",
      "    - 41.0\n",
      "    - 35.0\n",
      "    - 70.0\n",
      "    - 36.0\n",
      "    - 29.0\n",
      "    - 48.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 71.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 64.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 57.0\n",
      "    - 47.0\n",
      "    - 89.0\n",
      "    - 56.0\n",
      "    - 107.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 75.0\n",
      "    - 74.0\n",
      "    - 27.0\n",
      "    - 35.0\n",
      "    - 73.0\n",
      "    - 82.0\n",
      "    - 30.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 64.0\n",
      "    - 28.0\n",
      "    - 59.0\n",
      "    - 76.0\n",
      "    - 47.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 65.0\n",
      "    - 57.0\n",
      "    - 53.0\n",
      "    - 68.0\n",
      "    - 71.0\n",
      "    - 71.0\n",
      "    - 50.0\n",
      "    - 75.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 79.0\n",
      "    - 42.0\n",
      "    - 43.0\n",
      "    - 44.0\n",
      "    - 2.0\n",
      "    - 44.0\n",
      "    - 78.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015145126052179434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020342611819799017\n",
      "    mean_inference_ms: 0.23318550391268034\n",
      "    mean_raw_obs_processing_ms: 0.031777749404661965\n",
      "time_since_restore: 163.92263007164001\n",
      "time_this_iter_s: 2.6269278526306152\n",
      "time_total_s: 163.92263007164001\n",
      "timers:\n",
      "  learn_throughput: 2848.969\n",
      "  learn_time_ms: 1404.017\n",
      "  load_throughput: 23363342.153\n",
      "  load_time_ms: 0.171\n",
      "  training_iteration_time_ms: 2635.238\n",
      "  update_time_ms: 0.693\n",
      "timestamp: 1656299547\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 248000\n",
      "training_iteration: 62\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 252000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 252000\n",
      "  num_agent_steps_trained: 252000\n",
      "  num_env_steps_sampled: 252000\n",
      "  num_env_steps_trained: 252000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-30\n",
      "done: false\n",
      "episode_len_mean: 45.69\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 46.92\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 6052\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0648115870132242\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016523704394417745\n",
      "        policy_loss: -0.006339343883077143\n",
      "        total_loss: 8.305046173834032\n",
      "        vf_explained_var: 0.003864876557421941\n",
      "        vf_loss: 8.308407927072176\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 252000\n",
      "  num_agent_steps_trained: 252000\n",
      "  num_env_steps_sampled: 252000\n",
      "  num_env_steps_trained: 252000\n",
      "iterations_since_restore: 63\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 252000\n",
      "num_agent_steps_trained: 252000\n",
      "num_env_steps_sampled: 252000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 252000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.05\n",
      "  ram_util_percent: 51.77499999999999\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015144914960509919\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02034029904823721\n",
      "  mean_inference_ms: 0.23318002538431762\n",
      "  mean_raw_obs_processing_ms: 0.031773032141145095\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 46.92\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 33\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 41\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 45\n",
      "    episode_reward:\n",
      "    - 71.0\n",
      "    - 50.0\n",
      "    - 75.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 79.0\n",
      "    - 42.0\n",
      "    - 43.0\n",
      "    - 44.0\n",
      "    - 2.0\n",
      "    - 44.0\n",
      "    - 78.0\n",
      "    - 63.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 60.0\n",
      "    - 53.0\n",
      "    - 68.0\n",
      "    - 49.0\n",
      "    - 62.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 38.0\n",
      "    - 44.0\n",
      "    - 75.0\n",
      "    - 44.0\n",
      "    - 82.0\n",
      "    - 53.0\n",
      "    - 11.0\n",
      "    - 84.0\n",
      "    - 82.0\n",
      "    - 9.0\n",
      "    - 1.0\n",
      "    - 51.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 53.0\n",
      "    - 67.0\n",
      "    - 26.0\n",
      "    - 70.0\n",
      "    - 52.0\n",
      "    - 29.0\n",
      "    - 55.0\n",
      "    - 101.0\n",
      "    - 0.0\n",
      "    - 76.0\n",
      "    - 47.0\n",
      "    - 42.0\n",
      "    - 95.0\n",
      "    - 60.0\n",
      "    - 31.0\n",
      "    - 69.0\n",
      "    - 26.0\n",
      "    - 47.0\n",
      "    - 41.0\n",
      "    - 68.0\n",
      "    - 42.0\n",
      "    - 54.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 43.0\n",
      "    - 97.0\n",
      "    - 72.0\n",
      "    - 68.0\n",
      "    - 53.0\n",
      "    - 56.0\n",
      "    - 61.0\n",
      "    - 79.0\n",
      "    - 52.0\n",
      "    - 57.0\n",
      "    - 1.0\n",
      "    - 42.0\n",
      "    - 44.0\n",
      "    - 63.0\n",
      "    - 37.0\n",
      "    - 60.0\n",
      "    - 77.0\n",
      "    - 18.0\n",
      "    - 66.0\n",
      "    - 14.0\n",
      "    - 22.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 67.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 53.0\n",
      "    - 46.0\n",
      "    - 54.0\n",
      "    - 38.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015144914960509919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02034029904823721\n",
      "    mean_inference_ms: 0.23318002538431762\n",
      "    mean_raw_obs_processing_ms: 0.031773032141145095\n",
      "time_since_restore: 166.5766623020172\n",
      "time_this_iter_s: 2.6540322303771973\n",
      "time_total_s: 166.5766623020172\n",
      "timers:\n",
      "  learn_throughput: 2843.24\n",
      "  learn_time_ms: 1406.846\n",
      "  load_throughput: 23259692.222\n",
      "  load_time_ms: 0.172\n",
      "  training_iteration_time_ms: 2637.026\n",
      "  update_time_ms: 0.675\n",
      "timestamp: 1656299550\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 252000\n",
      "training_iteration: 63\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 256000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 256000\n",
      "  num_agent_steps_trained: 256000\n",
      "  num_env_steps_sampled: 256000\n",
      "  num_env_steps_trained: 256000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-32\n",
      "done: false\n",
      "episode_len_mean: 43.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 38.86\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 92\n",
      "episodes_total: 6144\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0746760127364947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014347089413473362\n",
      "        policy_loss: -0.013388967065901685\n",
      "        total_loss: 7.840658852874592\n",
      "        vf_explained_var: 0.004661295875426262\n",
      "        vf_loss: 7.851462447258734\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 256000\n",
      "  num_agent_steps_trained: 256000\n",
      "  num_env_steps_sampled: 256000\n",
      "  num_env_steps_trained: 256000\n",
      "iterations_since_restore: 64\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 256000\n",
      "num_agent_steps_trained: 256000\n",
      "num_env_steps_sampled: 256000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 256000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.225\n",
      "  ram_util_percent: 51.8\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015144561937488723\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02033717131534034\n",
      "  mean_inference_ms: 0.23316838315194033\n",
      "  mean_raw_obs_processing_ms: 0.031770479844539505\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 38.86\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 92\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 9\n",
      "    - 41\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 29\n",
      "    - 47\n",
      "    - 47\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 53.0\n",
      "    - 46.0\n",
      "    - 54.0\n",
      "    - 38.0\n",
      "    - 45.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 54.0\n",
      "    - 55.0\n",
      "    - 8.0\n",
      "    - 47.0\n",
      "    - 85.0\n",
      "    - 80.0\n",
      "    - 64.0\n",
      "    - 53.0\n",
      "    - 53.0\n",
      "    - 48.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 66.0\n",
      "    - 62.0\n",
      "    - 37.0\n",
      "    - 101.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 27.0\n",
      "    - 23.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 30.0\n",
      "    - 35.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 64.0\n",
      "    - 44.0\n",
      "    - 95.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 34.0\n",
      "    - 61.0\n",
      "    - 37.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 66.0\n",
      "    - 94.0\n",
      "    - 5.0\n",
      "    - 44.0\n",
      "    - 47.0\n",
      "    - 66.0\n",
      "    - 26.0\n",
      "    - 13.0\n",
      "    - 2.0\n",
      "    - 69.0\n",
      "    - 74.0\n",
      "    - 34.0\n",
      "    - 21.0\n",
      "    - 54.0\n",
      "    - 58.0\n",
      "    - 38.0\n",
      "    - 37.0\n",
      "    - 40.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 60.0\n",
      "    - 10.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 2.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 27.0\n",
      "    - 74.0\n",
      "    - 2.0\n",
      "    - 18.0\n",
      "    - 40.0\n",
      "    - 72.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 74.0\n",
      "    - 69.0\n",
      "    - 73.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015144561937488723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02033717131534034\n",
      "    mean_inference_ms: 0.23316838315194033\n",
      "    mean_raw_obs_processing_ms: 0.031770479844539505\n",
      "time_since_restore: 169.2406141757965\n",
      "time_this_iter_s: 2.663951873779297\n",
      "time_total_s: 169.2406141757965\n",
      "timers:\n",
      "  learn_throughput: 2836.788\n",
      "  learn_time_ms: 1410.046\n",
      "  load_throughput: 23237141.274\n",
      "  load_time_ms: 0.172\n",
      "  training_iteration_time_ms: 2640.266\n",
      "  update_time_ms: 0.666\n",
      "timestamp: 1656299552\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 256000\n",
      "training_iteration: 64\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 260000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 260000\n",
      "  num_agent_steps_trained: 260000\n",
      "  num_env_steps_sampled: 260000\n",
      "  num_env_steps_trained: 260000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-35\n",
      "done: false\n",
      "episode_len_mean: 45.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 93.0\n",
      "episode_reward_mean: 45.06\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 6230\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0656750837961833\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01608499588184459\n",
      "        policy_loss: -0.017244616961006515\n",
      "        total_loss: 7.850753153524091\n",
      "        vf_explained_var: 0.004556485658050865\n",
      "        vf_loss: 7.865099182436543\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 260000\n",
      "  num_agent_steps_trained: 260000\n",
      "  num_env_steps_sampled: 260000\n",
      "  num_env_steps_trained: 260000\n",
      "iterations_since_restore: 65\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 260000\n",
      "num_agent_steps_trained: 260000\n",
      "num_env_steps_sampled: 260000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 260000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.8\n",
      "  ram_util_percent: 51.8\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015144387938603883\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020335229012278896\n",
      "  mean_inference_ms: 0.23315803080483655\n",
      "  mean_raw_obs_processing_ms: 0.031764511572673376\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 93.0\n",
      "  episode_reward_mean: 45.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 20\n",
      "    - 29\n",
      "    - 47\n",
      "    - 47\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 74.0\n",
      "    - 2.0\n",
      "    - 18.0\n",
      "    - 40.0\n",
      "    - 72.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 17.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 74.0\n",
      "    - 69.0\n",
      "    - 73.0\n",
      "    - 57.0\n",
      "    - 48.0\n",
      "    - 8.0\n",
      "    - 50.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 36.0\n",
      "    - 61.0\n",
      "    - 72.0\n",
      "    - 50.0\n",
      "    - 65.0\n",
      "    - 44.0\n",
      "    - 63.0\n",
      "    - 60.0\n",
      "    - 90.0\n",
      "    - 32.0\n",
      "    - 93.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 24.0\n",
      "    - 68.0\n",
      "    - 35.0\n",
      "    - 22.0\n",
      "    - 76.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 72.0\n",
      "    - 54.0\n",
      "    - 62.0\n",
      "    - 87.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 31.0\n",
      "    - 48.0\n",
      "    - 34.0\n",
      "    - 70.0\n",
      "    - 60.0\n",
      "    - 51.0\n",
      "    - 29.0\n",
      "    - 43.0\n",
      "    - 43.0\n",
      "    - 80.0\n",
      "    - 2.0\n",
      "    - 71.0\n",
      "    - 38.0\n",
      "    - 79.0\n",
      "    - 44.0\n",
      "    - 67.0\n",
      "    - 20.0\n",
      "    - 55.0\n",
      "    - 38.0\n",
      "    - 28.0\n",
      "    - 75.0\n",
      "    - 43.0\n",
      "    - 53.0\n",
      "    - 63.0\n",
      "    - 48.0\n",
      "    - 91.0\n",
      "    - 45.0\n",
      "    - 26.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 16.0\n",
      "    - 63.0\n",
      "    - 2.0\n",
      "    - 58.0\n",
      "    - 39.0\n",
      "    - 4.0\n",
      "    - 38.0\n",
      "    - 23.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 74.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 68.0\n",
      "    - 89.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015144387938603883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020335229012278896\n",
      "    mean_inference_ms: 0.23315803080483655\n",
      "    mean_raw_obs_processing_ms: 0.031764511572673376\n",
      "time_since_restore: 171.9330461025238\n",
      "time_this_iter_s: 2.692431926727295\n",
      "time_total_s: 171.9330461025238\n",
      "timers:\n",
      "  learn_throughput: 2823.408\n",
      "  learn_time_ms: 1416.728\n",
      "  load_throughput: 23074152.111\n",
      "  load_time_ms: 0.173\n",
      "  training_iteration_time_ms: 2646.755\n",
      "  update_time_ms: 0.689\n",
      "timestamp: 1656299555\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 260000\n",
      "training_iteration: 65\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 264000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 264000\n",
      "  num_agent_steps_trained: 264000\n",
      "  num_env_steps_sampled: 264000\n",
      "  num_env_steps_trained: 264000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-38\n",
      "done: false\n",
      "episode_len_mean: 44.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 111.0\n",
      "episode_reward_mean: 44.48\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 91\n",
      "episodes_total: 6321\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0681674240096923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01588285666469845\n",
      "        policy_loss: -0.01877074899772803\n",
      "        total_loss: 8.076240173975627\n",
      "        vf_explained_var: 0.002538138115277854\n",
      "        vf_loss: 8.092148814150082\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 264000\n",
      "  num_agent_steps_trained: 264000\n",
      "  num_env_steps_sampled: 264000\n",
      "  num_env_steps_trained: 264000\n",
      "iterations_since_restore: 66\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 264000\n",
      "num_agent_steps_trained: 264000\n",
      "num_env_steps_sampled: 264000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 264000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.866666666666664\n",
      "  ram_util_percent: 50.73333333333333\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015144857043631603\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020333476533047595\n",
      "  mean_inference_ms: 0.23315808751620515\n",
      "  mean_raw_obs_processing_ms: 0.03175906259299496\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 111.0\n",
      "  episode_reward_mean: 44.48\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 91\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 44\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 28\n",
      "    - 15\n",
      "    - 28\n",
      "    - 50\n",
      "    - 20\n",
      "    - 31\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 74.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 68.0\n",
      "    - 89.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 23.0\n",
      "    - 37.0\n",
      "    - 7.0\n",
      "    - 85.0\n",
      "    - 10.0\n",
      "    - 96.0\n",
      "    - 47.0\n",
      "    - 67.0\n",
      "    - 31.0\n",
      "    - 52.0\n",
      "    - 22.0\n",
      "    - 67.0\n",
      "    - 55.0\n",
      "    - 69.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 49.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 52.0\n",
      "    - 67.0\n",
      "    - 53.0\n",
      "    - 2.0\n",
      "    - 21.0\n",
      "    - 61.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 76.0\n",
      "    - 50.0\n",
      "    - 53.0\n",
      "    - 39.0\n",
      "    - 2.0\n",
      "    - 1.0\n",
      "    - 50.0\n",
      "    - 72.0\n",
      "    - 34.0\n",
      "    - 65.0\n",
      "    - 42.0\n",
      "    - 72.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 71.0\n",
      "    - 86.0\n",
      "    - 90.0\n",
      "    - 8.0\n",
      "    - 19.0\n",
      "    - 78.0\n",
      "    - 33.0\n",
      "    - 97.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 83.0\n",
      "    - 60.0\n",
      "    - 80.0\n",
      "    - 31.0\n",
      "    - 54.0\n",
      "    - 61.0\n",
      "    - 17.0\n",
      "    - 50.0\n",
      "    - 31.0\n",
      "    - 111.0\n",
      "    - 52.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 20.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 64.0\n",
      "    - 65.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 59.0\n",
      "    - 82.0\n",
      "    - 67.0\n",
      "    - 31.0\n",
      "    - 58.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015144857043631603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020333476533047595\n",
      "    mean_inference_ms: 0.23315808751620515\n",
      "    mean_raw_obs_processing_ms: 0.03175906259299496\n",
      "time_since_restore: 174.62920713424683\n",
      "time_this_iter_s: 2.6961610317230225\n",
      "time_total_s: 174.62920713424683\n",
      "timers:\n",
      "  learn_throughput: 2816.608\n",
      "  learn_time_ms: 1420.148\n",
      "  load_throughput: 22888425.648\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2648.74\n",
      "  update_time_ms: 0.7\n",
      "timestamp: 1656299558\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 264000\n",
      "training_iteration: 66\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 268000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 268000\n",
      "  num_agent_steps_trained: 268000\n",
      "  num_env_steps_sampled: 268000\n",
      "  num_env_steps_trained: 268000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-40\n",
      "done: false\n",
      "episode_len_mean: 44.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 84.0\n",
      "episode_reward_mean: 40.25\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 90\n",
      "episodes_total: 6411\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0614039742177532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01599041880959625\n",
      "        policy_loss: -0.012410978287939103\n",
      "        total_loss: 7.590749571913032\n",
      "        vf_explained_var: 0.003833360120814334\n",
      "        vf_loss: 7.600279027672224\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 268000\n",
      "  num_agent_steps_trained: 268000\n",
      "  num_env_steps_sampled: 268000\n",
      "  num_env_steps_trained: 268000\n",
      "iterations_since_restore: 67\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 268000\n",
      "num_agent_steps_trained: 268000\n",
      "num_env_steps_sampled: 268000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 268000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.525\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015145224165469779\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020332119348032928\n",
      "  mean_inference_ms: 0.23317142060532617\n",
      "  mean_raw_obs_processing_ms: 0.03175876506402904\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 84.0\n",
      "  episode_reward_mean: 40.25\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 90\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 44\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 32\n",
      "    - 50\n",
      "    - 14\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 64.0\n",
      "    - 65.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 59.0\n",
      "    - 82.0\n",
      "    - 67.0\n",
      "    - 31.0\n",
      "    - 58.0\n",
      "    - 38.0\n",
      "    - 73.0\n",
      "    - 43.0\n",
      "    - 49.0\n",
      "    - 55.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 62.0\n",
      "    - 45.0\n",
      "    - 41.0\n",
      "    - 8.0\n",
      "    - 21.0\n",
      "    - 78.0\n",
      "    - 75.0\n",
      "    - 81.0\n",
      "    - 32.0\n",
      "    - 52.0\n",
      "    - 56.0\n",
      "    - 2.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 28.0\n",
      "    - 65.0\n",
      "    - 21.0\n",
      "    - 64.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 30.0\n",
      "    - 63.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 76.0\n",
      "    - 52.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 82.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 69.0\n",
      "    - 69.0\n",
      "    - 31.0\n",
      "    - 2.0\n",
      "    - 84.0\n",
      "    - 56.0\n",
      "    - 7.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 70.0\n",
      "    - 59.0\n",
      "    - 41.0\n",
      "    - 63.0\n",
      "    - 32.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 35.0\n",
      "    - 37.0\n",
      "    - 50.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 43.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 6.0\n",
      "    - 31.0\n",
      "    - 15.0\n",
      "    - 34.0\n",
      "    - 4.0\n",
      "    - 74.0\n",
      "    - 46.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 45.0\n",
      "    - 34.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015145224165469779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020332119348032928\n",
      "    mean_inference_ms: 0.23317142060532617\n",
      "    mean_raw_obs_processing_ms: 0.03175876506402904\n",
      "time_since_restore: 177.2808620929718\n",
      "time_this_iter_s: 2.6516549587249756\n",
      "time_total_s: 177.2808620929718\n",
      "timers:\n",
      "  learn_throughput: 2815.36\n",
      "  learn_time_ms: 1420.777\n",
      "  load_throughput: 22770380.022\n",
      "  load_time_ms: 0.176\n",
      "  training_iteration_time_ms: 2650.665\n",
      "  update_time_ms: 0.696\n",
      "timestamp: 1656299560\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 268000\n",
      "training_iteration: 67\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 272000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 272000\n",
      "  num_agent_steps_trained: 272000\n",
      "  num_env_steps_sampled: 272000\n",
      "  num_env_steps_trained: 272000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-43\n",
      "done: false\n",
      "episode_len_mean: 44.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 88.0\n",
      "episode_reward_mean: 39.8\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 91\n",
      "episodes_total: 6502\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0621457165287387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014614524049383978\n",
      "        policy_loss: -0.006334900671756395\n",
      "        total_loss: 8.116747908951133\n",
      "        vf_explained_var: 0.0027468061575325587\n",
      "        vf_loss: 8.120449191267772\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 272000\n",
      "  num_agent_steps_trained: 272000\n",
      "  num_env_steps_sampled: 272000\n",
      "  num_env_steps_trained: 272000\n",
      "iterations_since_restore: 68\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 272000\n",
      "num_agent_steps_trained: 272000\n",
      "num_env_steps_sampled: 272000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 272000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.1\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015144833365018635\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020329143477205887\n",
      "  mean_inference_ms: 0.23317023655922456\n",
      "  mean_raw_obs_processing_ms: 0.03175669012449924\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 88.0\n",
      "  episode_reward_mean: 39.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 91\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 11\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 13\n",
      "    - 8\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 39\n",
      "    - 20\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 6\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 34.0\n",
      "    - 4.0\n",
      "    - 74.0\n",
      "    - 46.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 45.0\n",
      "    - 34.0\n",
      "    - 39.0\n",
      "    - 20.0\n",
      "    - 83.0\n",
      "    - 36.0\n",
      "    - 88.0\n",
      "    - 37.0\n",
      "    - 42.0\n",
      "    - 60.0\n",
      "    - 18.0\n",
      "    - 50.0\n",
      "    - 37.0\n",
      "    - 49.0\n",
      "    - 65.0\n",
      "    - 37.0\n",
      "    - 4.0\n",
      "    - 67.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 86.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 16.0\n",
      "    - 74.0\n",
      "    - 62.0\n",
      "    - 2.0\n",
      "    - 69.0\n",
      "    - 25.0\n",
      "    - 61.0\n",
      "    - 41.0\n",
      "    - 40.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 73.0\n",
      "    - 72.0\n",
      "    - 81.0\n",
      "    - 65.0\n",
      "    - 57.0\n",
      "    - 84.0\n",
      "    - 60.0\n",
      "    - 39.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 52.0\n",
      "    - 71.0\n",
      "    - 46.0\n",
      "    - 5.0\n",
      "    - 60.0\n",
      "    - 60.0\n",
      "    - 72.0\n",
      "    - 20.0\n",
      "    - 54.0\n",
      "    - 39.0\n",
      "    - 54.0\n",
      "    - 42.0\n",
      "    - 56.0\n",
      "    - 38.0\n",
      "    - 27.0\n",
      "    - 55.0\n",
      "    - 7.0\n",
      "    - 25.0\n",
      "    - 52.0\n",
      "    - 88.0\n",
      "    - 38.0\n",
      "    - 3.0\n",
      "    - 35.0\n",
      "    - 22.0\n",
      "    - 33.0\n",
      "    - 26.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 26.0\n",
      "    - 50.0\n",
      "    - 31.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 49.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015144833365018635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020329143477205887\n",
      "    mean_inference_ms: 0.23317023655922456\n",
      "    mean_raw_obs_processing_ms: 0.03175669012449924\n",
      "time_since_restore: 179.9355342388153\n",
      "time_this_iter_s: 2.654672145843506\n",
      "time_total_s: 179.9355342388153\n",
      "timers:\n",
      "  learn_throughput: 2812.543\n",
      "  learn_time_ms: 1422.2\n",
      "  load_throughput: 22810626.785\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2651.584\n",
      "  update_time_ms: 0.668\n",
      "timestamp: 1656299563\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 272000\n",
      "training_iteration: 68\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 276000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 276000\n",
      "  num_agent_steps_trained: 276000\n",
      "  num_env_steps_sampled: 276000\n",
      "  num_env_steps_trained: 276000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-46\n",
      "done: false\n",
      "episode_len_mean: 44.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 91.0\n",
      "episode_reward_mean: 41.84\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 89\n",
      "episodes_total: 6591\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.052244612606623\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015931481457445572\n",
      "        policy_loss: -0.007114633880755914\n",
      "        total_loss: 8.091231304086664\n",
      "        vf_explained_var: 0.0030602717912325294\n",
      "        vf_loss: 8.095475037123567\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 276000\n",
      "  num_agent_steps_trained: 276000\n",
      "  num_env_steps_sampled: 276000\n",
      "  num_env_steps_trained: 276000\n",
      "iterations_since_restore: 69\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 276000\n",
      "num_agent_steps_trained: 276000\n",
      "num_env_steps_sampled: 276000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 276000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.825\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015144498734416827\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020326031926126\n",
      "  mean_inference_ms: 0.2331629454026912\n",
      "  mean_raw_obs_processing_ms: 0.03175275964237161\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 91.0\n",
      "  episode_reward_mean: 41.84\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 89\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 6\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 46\n",
      "    - 7\n",
      "    - 37\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 32\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 50.0\n",
      "    - 31.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 49.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 38.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 57.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 47.0\n",
      "    - 46.0\n",
      "    - 76.0\n",
      "    - 58.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 86.0\n",
      "    - 60.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 36.0\n",
      "    - 3.0\n",
      "    - 40.0\n",
      "    - 2.0\n",
      "    - 20.0\n",
      "    - 12.0\n",
      "    - 79.0\n",
      "    - 5.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 48.0\n",
      "    - 37.0\n",
      "    - 91.0\n",
      "    - 53.0\n",
      "    - 53.0\n",
      "    - 89.0\n",
      "    - 83.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 65.0\n",
      "    - 4.0\n",
      "    - 37.0\n",
      "    - 35.0\n",
      "    - 66.0\n",
      "    - 69.0\n",
      "    - 69.0\n",
      "    - 29.0\n",
      "    - 72.0\n",
      "    - 81.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 91.0\n",
      "    - 55.0\n",
      "    - 37.0\n",
      "    - 46.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 42.0\n",
      "    - 89.0\n",
      "    - 32.0\n",
      "    - 86.0\n",
      "    - 33.0\n",
      "    - 46.0\n",
      "    - 62.0\n",
      "    - 82.0\n",
      "    - 62.0\n",
      "    - 30.0\n",
      "    - 63.0\n",
      "    - 16.0\n",
      "    - 45.0\n",
      "    - 76.0\n",
      "    - 82.0\n",
      "    - 29.0\n",
      "    - 54.0\n",
      "    - 43.0\n",
      "    - 61.0\n",
      "    - 51.0\n",
      "    - 65.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015144498734416827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020326031926126\n",
      "    mean_inference_ms: 0.2331629454026912\n",
      "    mean_raw_obs_processing_ms: 0.03175275964237161\n",
      "time_since_restore: 182.56433629989624\n",
      "time_this_iter_s: 2.6288020610809326\n",
      "time_total_s: 182.56433629989624\n",
      "timers:\n",
      "  learn_throughput: 2811.102\n",
      "  learn_time_ms: 1422.93\n",
      "  load_throughput: 22841682.777\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2652.664\n",
      "  update_time_ms: 0.657\n",
      "timestamp: 1656299566\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 276000\n",
      "training_iteration: 69\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 280000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 280000\n",
      "  num_agent_steps_trained: 280000\n",
      "  num_env_steps_sampled: 280000\n",
      "  num_env_steps_trained: 280000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-48\n",
      "done: false\n",
      "episode_len_mean: 45.91\n",
      "episode_media: {}\n",
      "episode_reward_max: 94.0\n",
      "episode_reward_mean: 43.66\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 6678\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0590675744959104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01544300034211608\n",
      "        policy_loss: -0.018599621759307\n",
      "        total_loss: 7.936079890753633\n",
      "        vf_explained_var: 0.003720954989874235\n",
      "        vf_loss: 7.9518966254367625\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 280000\n",
      "  num_agent_steps_trained: 280000\n",
      "  num_env_steps_sampled: 280000\n",
      "  num_env_steps_trained: 280000\n",
      "iterations_since_restore: 70\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 280000\n",
      "num_agent_steps_trained: 280000\n",
      "num_env_steps_sampled: 280000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 280000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.25\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015143914453828447\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020323497204866477\n",
      "  mean_inference_ms: 0.2331565906220628\n",
      "  mean_raw_obs_processing_ms: 0.03174674773952353\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 94.0\n",
      "  episode_reward_mean: 43.66\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 9\n",
      "    - 38\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 62.0\n",
      "    - 30.0\n",
      "    - 63.0\n",
      "    - 16.0\n",
      "    - 45.0\n",
      "    - 76.0\n",
      "    - 82.0\n",
      "    - 29.0\n",
      "    - 54.0\n",
      "    - 43.0\n",
      "    - 61.0\n",
      "    - 51.0\n",
      "    - 65.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 44.0\n",
      "    - 36.0\n",
      "    - 34.0\n",
      "    - 28.0\n",
      "    - 73.0\n",
      "    - 53.0\n",
      "    - 59.0\n",
      "    - 58.0\n",
      "    - 42.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 36.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 89.0\n",
      "    - 50.0\n",
      "    - 63.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 29.0\n",
      "    - 21.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 91.0\n",
      "    - 37.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 81.0\n",
      "    - 82.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 33.0\n",
      "    - 27.0\n",
      "    - 54.0\n",
      "    - 49.0\n",
      "    - 34.0\n",
      "    - 48.0\n",
      "    - 37.0\n",
      "    - 67.0\n",
      "    - 49.0\n",
      "    - 60.0\n",
      "    - 18.0\n",
      "    - 60.0\n",
      "    - 27.0\n",
      "    - 5.0\n",
      "    - 33.0\n",
      "    - 16.0\n",
      "    - 67.0\n",
      "    - 77.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 16.0\n",
      "    - 2.0\n",
      "    - 52.0\n",
      "    - 83.0\n",
      "    - 69.0\n",
      "    - 70.0\n",
      "    - 47.0\n",
      "    - 67.0\n",
      "    - 15.0\n",
      "    - 21.0\n",
      "    - 40.0\n",
      "    - 65.0\n",
      "    - 83.0\n",
      "    - 71.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 61.0\n",
      "    - 94.0\n",
      "    - 53.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015143914453828447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020323497204866477\n",
      "    mean_inference_ms: 0.2331565906220628\n",
      "    mean_raw_obs_processing_ms: 0.03174674773952353\n",
      "time_since_restore: 185.22109746932983\n",
      "time_this_iter_s: 2.6567611694335938\n",
      "time_total_s: 185.22109746932983\n",
      "timers:\n",
      "  learn_throughput: 2809.537\n",
      "  learn_time_ms: 1423.722\n",
      "  load_throughput: 22869705.562\n",
      "  load_time_ms: 0.175\n",
      "  training_iteration_time_ms: 2653.812\n",
      "  update_time_ms: 0.663\n",
      "timestamp: 1656299568\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 280000\n",
      "training_iteration: 70\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 284000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 284000\n",
      "  num_agent_steps_trained: 284000\n",
      "  num_env_steps_sampled: 284000\n",
      "  num_env_steps_trained: 284000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-51\n",
      "done: false\n",
      "episode_len_mean: 45.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 106.0\n",
      "episode_reward_mean: 47.14\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 90\n",
      "episodes_total: 6768\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0548643676183558\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014915068191726522\n",
      "        policy_loss: -0.011900328523329189\n",
      "        total_loss: 8.222939112109522\n",
      "        vf_explained_var: 0.003415201364024993\n",
      "        vf_loss: 8.232151689837055\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 284000\n",
      "  num_agent_steps_trained: 284000\n",
      "  num_env_steps_sampled: 284000\n",
      "  num_env_steps_trained: 284000\n",
      "iterations_since_restore: 71\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 284000\n",
      "num_agent_steps_trained: 284000\n",
      "num_env_steps_sampled: 284000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 284000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.5\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01514524742852017\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02032549613899102\n",
      "  mean_inference_ms: 0.23317657361528168\n",
      "  mean_raw_obs_processing_ms: 0.031750344090096326\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.0\n",
      "  episode_reward_mean: 47.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 90\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 6\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 40.0\n",
      "    - 65.0\n",
      "    - 83.0\n",
      "    - 71.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 61.0\n",
      "    - 94.0\n",
      "    - 53.0\n",
      "    - 2.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 106.0\n",
      "    - 56.0\n",
      "    - 66.0\n",
      "    - 80.0\n",
      "    - 47.0\n",
      "    - 55.0\n",
      "    - 40.0\n",
      "    - 78.0\n",
      "    - 40.0\n",
      "    - 11.0\n",
      "    - 40.0\n",
      "    - 70.0\n",
      "    - 60.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 46.0\n",
      "    - 44.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 98.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 67.0\n",
      "    - 63.0\n",
      "    - 21.0\n",
      "    - 29.0\n",
      "    - 39.0\n",
      "    - 77.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 78.0\n",
      "    - 46.0\n",
      "    - 68.0\n",
      "    - 44.0\n",
      "    - 67.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 91.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 63.0\n",
      "    - 39.0\n",
      "    - 76.0\n",
      "    - 88.0\n",
      "    - 75.0\n",
      "    - 49.0\n",
      "    - 57.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 61.0\n",
      "    - 55.0\n",
      "    - 36.0\n",
      "    - 48.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 49.0\n",
      "    - 45.0\n",
      "    - 63.0\n",
      "    - 68.0\n",
      "    - 68.0\n",
      "    - 48.0\n",
      "    - 25.0\n",
      "    - 89.0\n",
      "    - 48.0\n",
      "    - 49.0\n",
      "    - 18.0\n",
      "    - 39.0\n",
      "    - 58.0\n",
      "    - 49.0\n",
      "    - 46.0\n",
      "    - 36.0\n",
      "    - 6.0\n",
      "    - 60.0\n",
      "    - 31.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01514524742852017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02032549613899102\n",
      "    mean_inference_ms: 0.23317657361528168\n",
      "    mean_raw_obs_processing_ms: 0.031750344090096326\n",
      "time_since_restore: 187.87623167037964\n",
      "time_this_iter_s: 2.6551342010498047\n",
      "time_total_s: 187.87623167037964\n",
      "timers:\n",
      "  learn_throughput: 2808.932\n",
      "  learn_time_ms: 1424.029\n",
      "  load_throughput: 23039296.896\n",
      "  load_time_ms: 0.174\n",
      "  training_iteration_time_ms: 2655.677\n",
      "  update_time_ms: 0.653\n",
      "timestamp: 1656299571\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 284000\n",
      "training_iteration: 71\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 288000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 288000\n",
      "  num_agent_steps_trained: 288000\n",
      "  num_env_steps_sampled: 288000\n",
      "  num_env_steps_trained: 288000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-54\n",
      "done: false\n",
      "episode_len_mean: 44.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 42.94\n",
      "episode_reward_min: -40.0\n",
      "episodes_this_iter: 91\n",
      "episodes_total: 6859\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0631363734122246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013181058203761456\n",
      "        policy_loss: -0.014126571997879974\n",
      "        total_loss: 8.339294347968153\n",
      "        vf_explained_var: 0.002248489600355907\n",
      "        vf_loss: 8.351045659280592\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 288000\n",
      "  num_agent_steps_trained: 288000\n",
      "  num_env_steps_sampled: 288000\n",
      "  num_env_steps_trained: 288000\n",
      "iterations_since_restore: 72\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 288000\n",
      "num_agent_steps_trained: 288000\n",
      "num_env_steps_sampled: 288000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 288000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.549999999999999\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015144747937366256\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020322373123169778\n",
      "  mean_inference_ms: 0.2331707908194027\n",
      "  mean_raw_obs_processing_ms: 0.031748683860891115\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 42.94\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 91\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 20\n",
      "    - 42\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 14\n",
      "    - 36\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 18.0\n",
      "    - 39.0\n",
      "    - 58.0\n",
      "    - 49.0\n",
      "    - 46.0\n",
      "    - 36.0\n",
      "    - 6.0\n",
      "    - 60.0\n",
      "    - 31.0\n",
      "    - 53.0\n",
      "    - 28.0\n",
      "    - 76.0\n",
      "    - 53.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 65.0\n",
      "    - 68.0\n",
      "    - 83.0\n",
      "    - 26.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 74.0\n",
      "    - 60.0\n",
      "    - 42.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 11.0\n",
      "    - -40.0\n",
      "    - 72.0\n",
      "    - 6.0\n",
      "    - 51.0\n",
      "    - 34.0\n",
      "    - 75.0\n",
      "    - 91.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 58.0\n",
      "    - 77.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 56.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 69.0\n",
      "    - 53.0\n",
      "    - 36.0\n",
      "    - 41.0\n",
      "    - 47.0\n",
      "    - 60.0\n",
      "    - 77.0\n",
      "    - 62.0\n",
      "    - 56.0\n",
      "    - 75.0\n",
      "    - 21.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 79.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 54.0\n",
      "    - 87.0\n",
      "    - 101.0\n",
      "    - 79.0\n",
      "    - 56.0\n",
      "    - 66.0\n",
      "    - 70.0\n",
      "    - 38.0\n",
      "    - 15.0\n",
      "    - 94.0\n",
      "    - 42.0\n",
      "    - 42.0\n",
      "    - 95.0\n",
      "    - 13.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 75.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 77.0\n",
      "    - 40.0\n",
      "    - 1.0\n",
      "    - 8.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015144747937366256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020322373123169778\n",
      "    mean_inference_ms: 0.2331707908194027\n",
      "    mean_raw_obs_processing_ms: 0.031748683860891115\n",
      "time_since_restore: 190.51347970962524\n",
      "time_this_iter_s: 2.6372480392456055\n",
      "time_total_s: 190.51347970962524\n",
      "timers:\n",
      "  learn_throughput: 2807.094\n",
      "  learn_time_ms: 1424.961\n",
      "  load_throughput: 23093208.534\n",
      "  load_time_ms: 0.173\n",
      "  training_iteration_time_ms: 2656.716\n",
      "  update_time_ms: 0.649\n",
      "timestamp: 1656299574\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 288000\n",
      "training_iteration: 72\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 292000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 292000\n",
      "  num_agent_steps_trained: 292000\n",
      "  num_env_steps_sampled: 292000\n",
      "  num_env_steps_trained: 292000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-56\n",
      "done: false\n",
      "episode_len_mean: 44.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 104.0\n",
      "episode_reward_mean: 40.97\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 6946\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0504613844297266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014861878098279078\n",
      "        policy_loss: -0.007414018989650793\n",
      "        total_loss: 8.394972245411207\n",
      "        vf_explained_var: 0.0034202777570293795\n",
      "        vf_loss: 8.39970809618632\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 292000\n",
      "  num_agent_steps_trained: 292000\n",
      "  num_env_steps_sampled: 292000\n",
      "  num_env_steps_trained: 292000\n",
      "iterations_since_restore: 73\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 292000\n",
      "num_agent_steps_trained: 292000\n",
      "num_env_steps_sampled: 292000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 292000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.975000000000001\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015143762641381201\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020319386781553383\n",
      "  mean_inference_ms: 0.2331616668644264\n",
      "  mean_raw_obs_processing_ms: 0.03174256659233038\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 104.0\n",
      "  episode_reward_mean: 40.97\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 29\n",
      "    - 14\n",
      "    - 36\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 29\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 40\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 13.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 75.0\n",
      "    - 6.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 77.0\n",
      "    - 40.0\n",
      "    - 1.0\n",
      "    - 8.0\n",
      "    - 50.0\n",
      "    - 35.0\n",
      "    - 21.0\n",
      "    - 74.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 78.0\n",
      "    - 33.0\n",
      "    - 22.0\n",
      "    - 60.0\n",
      "    - 52.0\n",
      "    - 33.0\n",
      "    - 72.0\n",
      "    - 62.0\n",
      "    - 35.0\n",
      "    - 62.0\n",
      "    - 23.0\n",
      "    - 21.0\n",
      "    - 48.0\n",
      "    - 67.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 76.0\n",
      "    - 36.0\n",
      "    - 80.0\n",
      "    - 45.0\n",
      "    - 104.0\n",
      "    - 48.0\n",
      "    - 47.0\n",
      "    - 70.0\n",
      "    - 77.0\n",
      "    - 63.0\n",
      "    - 21.0\n",
      "    - 66.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 51.0\n",
      "    - 2.0\n",
      "    - 35.0\n",
      "    - 59.0\n",
      "    - 61.0\n",
      "    - 60.0\n",
      "    - 41.0\n",
      "    - 23.0\n",
      "    - 11.0\n",
      "    - 47.0\n",
      "    - 55.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 63.0\n",
      "    - 35.0\n",
      "    - 66.0\n",
      "    - 52.0\n",
      "    - 12.0\n",
      "    - 49.0\n",
      "    - 2.0\n",
      "    - 45.0\n",
      "    - 72.0\n",
      "    - 79.0\n",
      "    - 1.0\n",
      "    - 47.0\n",
      "    - 20.0\n",
      "    - 85.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 60.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 21.0\n",
      "    - 84.0\n",
      "    - 19.0\n",
      "    - 62.0\n",
      "    - 68.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015143762641381201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020319386781553383\n",
      "    mean_inference_ms: 0.2331616668644264\n",
      "    mean_raw_obs_processing_ms: 0.03174256659233038\n",
      "time_since_restore: 193.15684986114502\n",
      "time_this_iter_s: 2.6433701515197754\n",
      "time_total_s: 193.15684986114502\n",
      "timers:\n",
      "  learn_throughput: 2808.673\n",
      "  learn_time_ms: 1424.16\n",
      "  load_throughput: 23064635.689\n",
      "  load_time_ms: 0.173\n",
      "  training_iteration_time_ms: 2655.655\n",
      "  update_time_ms: 0.665\n",
      "timestamp: 1656299576\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 292000\n",
      "training_iteration: 73\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 296000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 296000\n",
      "  num_agent_steps_trained: 296000\n",
      "  num_env_steps_sampled: 296000\n",
      "  num_env_steps_trained: 296000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-12-59\n",
      "done: false\n",
      "episode_len_mean: 44.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 100.0\n",
      "episode_reward_mean: 45.56\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 90\n",
      "episodes_total: 7036\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0563594546369326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01563109975853955\n",
      "        policy_loss: -0.009285766282869923\n",
      "        total_loss: 8.492575657239524\n",
      "        vf_explained_var: 0.0018830252590999808\n",
      "        vf_loss: 8.499044697259063\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 296000\n",
      "  num_agent_steps_trained: 296000\n",
      "  num_env_steps_sampled: 296000\n",
      "  num_env_steps_trained: 296000\n",
      "iterations_since_restore: 74\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 296000\n",
      "num_agent_steps_trained: 296000\n",
      "num_env_steps_sampled: 296000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 296000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.1\n",
      "  ram_util_percent: 48.65\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015143227957117764\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020316979525615772\n",
      "  mean_inference_ms: 0.23315777443838331\n",
      "  mean_raw_obs_processing_ms: 0.03173925577411593\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.0\n",
      "  episode_reward_mean: 45.56\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 90\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 46\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 14\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 33.0\n",
      "    - 60.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 21.0\n",
      "    - 84.0\n",
      "    - 19.0\n",
      "    - 62.0\n",
      "    - 68.0\n",
      "    - 58.0\n",
      "    - 40.0\n",
      "    - 39.0\n",
      "    - 44.0\n",
      "    - 46.0\n",
      "    - 49.0\n",
      "    - 61.0\n",
      "    - 73.0\n",
      "    - 39.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 16.0\n",
      "    - 4.0\n",
      "    - 75.0\n",
      "    - 57.0\n",
      "    - 58.0\n",
      "    - 18.0\n",
      "    - 60.0\n",
      "    - 49.0\n",
      "    - 7.0\n",
      "    - 91.0\n",
      "    - 71.0\n",
      "    - 65.0\n",
      "    - 36.0\n",
      "    - 37.0\n",
      "    - 77.0\n",
      "    - 66.0\n",
      "    - 54.0\n",
      "    - 17.0\n",
      "    - 66.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 75.0\n",
      "    - 61.0\n",
      "    - 13.0\n",
      "    - 83.0\n",
      "    - 18.0\n",
      "    - 60.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 25.0\n",
      "    - 100.0\n",
      "    - 52.0\n",
      "    - 48.0\n",
      "    - 87.0\n",
      "    - 76.0\n",
      "    - 63.0\n",
      "    - 34.0\n",
      "    - 29.0\n",
      "    - 64.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 54.0\n",
      "    - 2.0\n",
      "    - 59.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 17.0\n",
      "    - 62.0\n",
      "    - 34.0\n",
      "    - 89.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 87.0\n",
      "    - 69.0\n",
      "    - 22.0\n",
      "    - 65.0\n",
      "    - 52.0\n",
      "    - 37.0\n",
      "    - 45.0\n",
      "    - 23.0\n",
      "    - 80.0\n",
      "    - 48.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 86.0\n",
      "    - 66.0\n",
      "    - 70.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015143227957117764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020316979525615772\n",
      "    mean_inference_ms: 0.23315777443838331\n",
      "    mean_raw_obs_processing_ms: 0.03173925577411593\n",
      "time_since_restore: 195.80497884750366\n",
      "time_this_iter_s: 2.6481289863586426\n",
      "time_total_s: 195.80497884750366\n",
      "timers:\n",
      "  learn_throughput: 2812.3\n",
      "  learn_time_ms: 1422.323\n",
      "  load_throughput: 22932225.26\n",
      "  load_time_ms: 0.174\n",
      "  training_iteration_time_ms: 2654.049\n",
      "  update_time_ms: 0.662\n",
      "timestamp: 1656299579\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 296000\n",
      "training_iteration: 74\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 300000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 300000\n",
      "  num_agent_steps_trained: 300000\n",
      "  num_env_steps_sampled: 300000\n",
      "  num_env_steps_trained: 300000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-02\n",
      "done: false\n",
      "episode_len_mean: 45.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 97.0\n",
      "episode_reward_mean: 44.21\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 7123\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0462081816247715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01457988250379761\n",
      "        policy_loss: -0.013873597429764848\n",
      "        total_loss: 8.06982180995326\n",
      "        vf_explained_var: 0.0012284959516217632\n",
      "        vf_loss: 8.081068047656808\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 300000\n",
      "  num_agent_steps_trained: 300000\n",
      "  num_env_steps_sampled: 300000\n",
      "  num_env_steps_trained: 300000\n",
      "iterations_since_restore: 75\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 300000\n",
      "num_agent_steps_trained: 300000\n",
      "num_env_steps_sampled: 300000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 300000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.1\n",
      "  ram_util_percent: 48.65\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015143778119860021\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02031520413600206\n",
      "  mean_inference_ms: 0.23316028145767742\n",
      "  mean_raw_obs_processing_ms: 0.031736074524086194\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 97.0\n",
      "  episode_reward_mean: 44.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 4\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 35\n",
      "    - 50\n",
      "    - 28\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 22.0\n",
      "    - 65.0\n",
      "    - 52.0\n",
      "    - 37.0\n",
      "    - 45.0\n",
      "    - 23.0\n",
      "    - 80.0\n",
      "    - 48.0\n",
      "    - 37.0\n",
      "    - 0.0\n",
      "    - 86.0\n",
      "    - 66.0\n",
      "    - 70.0\n",
      "    - 92.0\n",
      "    - 80.0\n",
      "    - 26.0\n",
      "    - 61.0\n",
      "    - 61.0\n",
      "    - 58.0\n",
      "    - 2.0\n",
      "    - 55.0\n",
      "    - 75.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 78.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 82.0\n",
      "    - 0.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 41.0\n",
      "    - 87.0\n",
      "    - 57.0\n",
      "    - 29.0\n",
      "    - 78.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 93.0\n",
      "    - 1.0\n",
      "    - 23.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 76.0\n",
      "    - 28.0\n",
      "    - 0.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 63.0\n",
      "    - 70.0\n",
      "    - 48.0\n",
      "    - 67.0\n",
      "    - 69.0\n",
      "    - 66.0\n",
      "    - 27.0\n",
      "    - 31.0\n",
      "    - 6.0\n",
      "    - 74.0\n",
      "    - 25.0\n",
      "    - 45.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 79.0\n",
      "    - 69.0\n",
      "    - 46.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 49.0\n",
      "    - 2.0\n",
      "    - 29.0\n",
      "    - 49.0\n",
      "    - 70.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 11.0\n",
      "    - 65.0\n",
      "    - 38.0\n",
      "    - 34.0\n",
      "    - 42.0\n",
      "    - 48.0\n",
      "    - 20.0\n",
      "    - 43.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 75.0\n",
      "    - 69.0\n",
      "    - 64.0\n",
      "    - 59.0\n",
      "    - 3.0\n",
      "    - 52.0\n",
      "    - 97.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015143778119860021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02031520413600206\n",
      "    mean_inference_ms: 0.23316028145767742\n",
      "    mean_raw_obs_processing_ms: 0.031736074524086194\n",
      "time_since_restore: 198.42663288116455\n",
      "time_this_iter_s: 2.6216540336608887\n",
      "time_total_s: 198.42663288116455\n",
      "timers:\n",
      "  learn_throughput: 2827.378\n",
      "  learn_time_ms: 1414.739\n",
      "  load_throughput: 23077325.997\n",
      "  load_time_ms: 0.173\n",
      "  training_iteration_time_ms: 2646.973\n",
      "  update_time_ms: 0.657\n",
      "timestamp: 1656299582\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 300000\n",
      "training_iteration: 75\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 304000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 304000\n",
      "  num_agent_steps_trained: 304000\n",
      "  num_env_steps_sampled: 304000\n",
      "  num_env_steps_trained: 304000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-04\n",
      "done: false\n",
      "episode_len_mean: 45.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 47.47\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 89\n",
      "episodes_total: 7212\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0489783948467624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013658029683214683\n",
      "        policy_loss: -0.01889100457511602\n",
      "        total_loss: 8.409821321118262\n",
      "        vf_explained_var: 0.0021480704507520123\n",
      "        vf_loss: 8.426251117132043\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 304000\n",
      "  num_agent_steps_trained: 304000\n",
      "  num_env_steps_sampled: 304000\n",
      "  num_env_steps_trained: 304000\n",
      "iterations_since_restore: 76\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 304000\n",
      "num_agent_steps_trained: 304000\n",
      "num_env_steps_sampled: 304000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 304000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.933333333333332\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015143520067695067\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020313295168595077\n",
      "  mean_inference_ms: 0.23315398289837977\n",
      "  mean_raw_obs_processing_ms: 0.031733318163734474\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 47.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 89\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 34\n",
      "    - 10\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    episode_reward:\n",
      "    - 59.0\n",
      "    - 75.0\n",
      "    - 69.0\n",
      "    - 64.0\n",
      "    - 59.0\n",
      "    - 3.0\n",
      "    - 52.0\n",
      "    - 97.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 60.0\n",
      "    - 68.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 18.0\n",
      "    - 88.0\n",
      "    - 21.0\n",
      "    - 84.0\n",
      "    - 65.0\n",
      "    - 76.0\n",
      "    - 43.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 67.0\n",
      "    - 63.0\n",
      "    - 55.0\n",
      "    - 37.0\n",
      "    - 10.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 48.0\n",
      "    - 86.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 50.0\n",
      "    - 76.0\n",
      "    - 56.0\n",
      "    - 84.0\n",
      "    - 8.0\n",
      "    - 36.0\n",
      "    - 27.0\n",
      "    - 53.0\n",
      "    - 89.0\n",
      "    - 64.0\n",
      "    - 55.0\n",
      "    - 74.0\n",
      "    - 39.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 58.0\n",
      "    - 39.0\n",
      "    - 63.0\n",
      "    - 32.0\n",
      "    - 100.0\n",
      "    - 91.0\n",
      "    - 101.0\n",
      "    - 38.0\n",
      "    - 90.0\n",
      "    - 66.0\n",
      "    - 48.0\n",
      "    - 32.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 79.0\n",
      "    - 24.0\n",
      "    - 46.0\n",
      "    - 45.0\n",
      "    - 53.0\n",
      "    - 79.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 35.0\n",
      "    - 56.0\n",
      "    - 49.0\n",
      "    - 35.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 78.0\n",
      "    - 17.0\n",
      "    - 69.0\n",
      "    - 38.0\n",
      "    - 25.0\n",
      "    - 69.0\n",
      "    - 61.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015143520067695067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020313295168595077\n",
      "    mean_inference_ms: 0.23315398289837977\n",
      "    mean_raw_obs_processing_ms: 0.031733318163734474\n",
      "time_since_restore: 201.07261681556702\n",
      "time_this_iter_s: 2.645983934402466\n",
      "time_total_s: 201.07261681556702\n",
      "timers:\n",
      "  learn_throughput: 2836.91\n",
      "  learn_time_ms: 1409.985\n",
      "  load_throughput: 23211422.247\n",
      "  load_time_ms: 0.172\n",
      "  training_iteration_time_ms: 2642.01\n",
      "  update_time_ms: 0.653\n",
      "timestamp: 1656299584\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 304000\n",
      "training_iteration: 76\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 308000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 308000\n",
      "  num_agent_steps_trained: 308000\n",
      "  num_env_steps_sampled: 308000\n",
      "  num_env_steps_trained: 308000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-07\n",
      "done: false\n",
      "episode_len_mean: 44.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 100.0\n",
      "episode_reward_mean: 39.74\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 90\n",
      "episodes_total: 7302\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0368310106697902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016502804352190866\n",
      "        policy_loss: -0.01304313822459149\n",
      "        total_loss: 8.001808705893897\n",
      "        vf_explained_var: 0.002698877293576476\n",
      "        vf_loss: 8.01187800668901\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 308000\n",
      "  num_agent_steps_trained: 308000\n",
      "  num_env_steps_sampled: 308000\n",
      "  num_env_steps_trained: 308000\n",
      "iterations_since_restore: 77\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 308000\n",
      "num_agent_steps_trained: 308000\n",
      "num_env_steps_sampled: 308000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 308000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.65\n",
      "  ram_util_percent: 48.625\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142931430940818\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02030991588347556\n",
      "  mean_inference_ms: 0.23313997381728194\n",
      "  mean_raw_obs_processing_ms: 0.03172845833733223\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.0\n",
      "  episode_reward_mean: 39.74\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 90\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 49\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 31\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 54.0\n",
      "    - 78.0\n",
      "    - 17.0\n",
      "    - 69.0\n",
      "    - 38.0\n",
      "    - 25.0\n",
      "    - 69.0\n",
      "    - 61.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 5.0\n",
      "    - 100.0\n",
      "    - 50.0\n",
      "    - 54.0\n",
      "    - 59.0\n",
      "    - 50.0\n",
      "    - 79.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 32.0\n",
      "    - 41.0\n",
      "    - 44.0\n",
      "    - 13.0\n",
      "    - 68.0\n",
      "    - 51.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 22.0\n",
      "    - 72.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 9.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 21.0\n",
      "    - 43.0\n",
      "    - 36.0\n",
      "    - 48.0\n",
      "    - 34.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 79.0\n",
      "    - 39.0\n",
      "    - 51.0\n",
      "    - 27.0\n",
      "    - 22.0\n",
      "    - 13.0\n",
      "    - 74.0\n",
      "    - 22.0\n",
      "    - 73.0\n",
      "    - 65.0\n",
      "    - 82.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 71.0\n",
      "    - 45.0\n",
      "    - 47.0\n",
      "    - 10.0\n",
      "    - 33.0\n",
      "    - 21.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 83.0\n",
      "    - 58.0\n",
      "    - 60.0\n",
      "    - 25.0\n",
      "    - 29.0\n",
      "    - 25.0\n",
      "    - 50.0\n",
      "    - 54.0\n",
      "    - 27.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 29.0\n",
      "    - 31.0\n",
      "    - 88.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 44.0\n",
      "    - 57.0\n",
      "    - 58.0\n",
      "    - 54.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 47.0\n",
      "    - 58.0\n",
      "    - 44.0\n",
      "    - 54.0\n",
      "    - 42.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142931430940818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02030991588347556\n",
      "    mean_inference_ms: 0.23313997381728194\n",
      "    mean_raw_obs_processing_ms: 0.03172845833733223\n",
      "time_since_restore: 203.70057797431946\n",
      "time_this_iter_s: 2.6279611587524414\n",
      "time_total_s: 203.70057797431946\n",
      "timers:\n",
      "  learn_throughput: 2839.434\n",
      "  learn_time_ms: 1408.731\n",
      "  load_throughput: 23386138.835\n",
      "  load_time_ms: 0.171\n",
      "  training_iteration_time_ms: 2639.631\n",
      "  update_time_ms: 0.665\n",
      "timestamp: 1656299587\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 308000\n",
      "training_iteration: 77\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 312000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 312000\n",
      "  num_agent_steps_trained: 312000\n",
      "  num_env_steps_sampled: 312000\n",
      "  num_env_steps_trained: 312000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-10\n",
      "done: false\n",
      "episode_len_mean: 46.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 45.43\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 85\n",
      "episodes_total: 7387\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0412891514839664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014462620319087578\n",
      "        policy_loss: -0.017876835852380722\n",
      "        total_loss: 8.57428095981639\n",
      "        vf_explained_var: 0.0034191233496512134\n",
      "        vf_loss: 8.589551548804007\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 312000\n",
      "  num_agent_steps_trained: 312000\n",
      "  num_env_steps_sampled: 312000\n",
      "  num_env_steps_trained: 312000\n",
      "iterations_since_restore: 78\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 312000\n",
      "num_agent_steps_trained: 312000\n",
      "num_env_steps_sampled: 312000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 312000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.175\n",
      "  ram_util_percent: 48.65\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142377003938364\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020307861048867712\n",
      "  mean_inference_ms: 0.233131357374849\n",
      "  mean_raw_obs_processing_ms: 0.031723701183843746\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 45.43\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 85\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 46\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 9\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    episode_reward:\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 44.0\n",
      "    - 57.0\n",
      "    - 58.0\n",
      "    - 54.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 47.0\n",
      "    - 58.0\n",
      "    - 44.0\n",
      "    - 54.0\n",
      "    - 42.0\n",
      "    - 29.0\n",
      "    - 0.0\n",
      "    - 28.0\n",
      "    - 77.0\n",
      "    - 17.0\n",
      "    - 65.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 63.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 37.0\n",
      "    - 57.0\n",
      "    - 84.0\n",
      "    - 24.0\n",
      "    - 24.0\n",
      "    - 61.0\n",
      "    - 64.0\n",
      "    - 74.0\n",
      "    - 56.0\n",
      "    - 67.0\n",
      "    - 37.0\n",
      "    - 90.0\n",
      "    - 64.0\n",
      "    - 92.0\n",
      "    - 85.0\n",
      "    - 42.0\n",
      "    - 28.0\n",
      "    - 76.0\n",
      "    - 59.0\n",
      "    - 55.0\n",
      "    - 76.0\n",
      "    - 21.0\n",
      "    - 22.0\n",
      "    - 56.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 44.0\n",
      "    - 14.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 71.0\n",
      "    - 32.0\n",
      "    - 38.0\n",
      "    - 43.0\n",
      "    - 67.0\n",
      "    - 55.0\n",
      "    - 62.0\n",
      "    - 22.0\n",
      "    - 65.0\n",
      "    - 23.0\n",
      "    - 101.0\n",
      "    - 59.0\n",
      "    - 57.0\n",
      "    - 29.0\n",
      "    - 50.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 27.0\n",
      "    - 28.0\n",
      "    - 43.0\n",
      "    - 40.0\n",
      "    - 5.0\n",
      "    - 69.0\n",
      "    - 57.0\n",
      "    - 32.0\n",
      "    - 42.0\n",
      "    - 26.0\n",
      "    - 59.0\n",
      "    - 26.0\n",
      "    - 40.0\n",
      "    - 57.0\n",
      "    - 35.0\n",
      "    - 89.0\n",
      "    - 20.0\n",
      "    - 58.0\n",
      "    - 64.0\n",
      "    - 4.0\n",
      "    - 24.0\n",
      "    - 66.0\n",
      "    - 51.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142377003938364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020307861048867712\n",
      "    mean_inference_ms: 0.233131357374849\n",
      "    mean_raw_obs_processing_ms: 0.031723701183843746\n",
      "time_since_restore: 206.33962392807007\n",
      "time_this_iter_s: 2.6390459537506104\n",
      "time_total_s: 206.33962392807007\n",
      "timers:\n",
      "  learn_throughput: 2841.994\n",
      "  learn_time_ms: 1407.463\n",
      "  load_throughput: 21670390.08\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2638.075\n",
      "  update_time_ms: 0.686\n",
      "timestamp: 1656299590\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 312000\n",
      "training_iteration: 78\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 316000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 316000\n",
      "  num_agent_steps_trained: 316000\n",
      "  num_env_steps_sampled: 316000\n",
      "  num_env_steps_trained: 316000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-12\n",
      "done: false\n",
      "episode_len_mean: 43.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 103.0\n",
      "episode_reward_mean: 41.25\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 92\n",
      "episodes_total: 7479\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0255132254733834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017984968193015583\n",
      "        policy_loss: -0.011364611986804232\n",
      "        total_loss: 7.865994988718341\n",
      "        vf_explained_var: 0.0006982261775642313\n",
      "        vf_loss: 7.874118645473193\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 316000\n",
      "  num_agent_steps_trained: 316000\n",
      "  num_env_steps_sampled: 316000\n",
      "  num_env_steps_trained: 316000\n",
      "iterations_since_restore: 79\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 316000\n",
      "num_agent_steps_trained: 316000\n",
      "num_env_steps_sampled: 316000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 316000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.350000000000001\n",
      "  ram_util_percent: 48.625\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142194444965338\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02030490674665795\n",
      "  mean_inference_ms: 0.23312838007061706\n",
      "  mean_raw_obs_processing_ms: 0.03172219766243932\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.0\n",
      "  episode_reward_mean: 41.25\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 92\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 43\n",
      "    - 24\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 6\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 9\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 42\n",
      "    - 50\n",
      "    - 24\n",
      "    - 26\n",
      "    - 50\n",
      "    - 16\n",
      "    episode_reward:\n",
      "    - 89.0\n",
      "    - 20.0\n",
      "    - 58.0\n",
      "    - 64.0\n",
      "    - 4.0\n",
      "    - 24.0\n",
      "    - 66.0\n",
      "    - 51.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 57.0\n",
      "    - 47.0\n",
      "    - 64.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 51.0\n",
      "    - 64.0\n",
      "    - 36.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 12.0\n",
      "    - 46.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 79.0\n",
      "    - 86.0\n",
      "    - 50.0\n",
      "    - 4.0\n",
      "    - 59.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 82.0\n",
      "    - 63.0\n",
      "    - 76.0\n",
      "    - 97.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 53.0\n",
      "    - 31.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 16.0\n",
      "    - 37.0\n",
      "    - 72.0\n",
      "    - 63.0\n",
      "    - 28.0\n",
      "    - 78.0\n",
      "    - 55.0\n",
      "    - 61.0\n",
      "    - 23.0\n",
      "    - 37.0\n",
      "    - 61.0\n",
      "    - 56.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 76.0\n",
      "    - 4.0\n",
      "    - 86.0\n",
      "    - 33.0\n",
      "    - 41.0\n",
      "    - 59.0\n",
      "    - 40.0\n",
      "    - 51.0\n",
      "    - 44.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 16.0\n",
      "    - 67.0\n",
      "    - 58.0\n",
      "    - 34.0\n",
      "    - 24.0\n",
      "    - 63.0\n",
      "    - 20.0\n",
      "    - 81.0\n",
      "    - 63.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 89.0\n",
      "    - 16.0\n",
      "    - 77.0\n",
      "    - 81.0\n",
      "    - 84.0\n",
      "    - 73.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 103.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142194444965338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02030490674665795\n",
      "    mean_inference_ms: 0.23312838007061706\n",
      "    mean_raw_obs_processing_ms: 0.03172219766243932\n",
      "time_since_restore: 208.98831868171692\n",
      "time_this_iter_s: 2.6486947536468506\n",
      "time_total_s: 208.98831868171692\n",
      "timers:\n",
      "  learn_throughput: 2838.445\n",
      "  learn_time_ms: 1409.222\n",
      "  load_throughput: 21617337.972\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2640.072\n",
      "  update_time_ms: 0.713\n",
      "timestamp: 1656299592\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 316000\n",
      "training_iteration: 79\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 320000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 320000\n",
      "  num_agent_steps_trained: 320000\n",
      "  num_env_steps_sampled: 320000\n",
      "  num_env_steps_trained: 320000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-15\n",
      "done: false\n",
      "episode_len_mean: 45.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 103.0\n",
      "episode_reward_mean: 48.12\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 7566\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0360663465915187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013949833092810487\n",
      "        policy_loss: 3.471803520956347e-05\n",
      "        total_loss: 8.823148128550539\n",
      "        vf_explained_var: 0.0024977950639622185\n",
      "        vf_loss: 8.820599567249257\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 320000\n",
      "  num_agent_steps_trained: 320000\n",
      "  num_env_steps_sampled: 320000\n",
      "  num_env_steps_trained: 320000\n",
      "iterations_since_restore: 80\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 320000\n",
      "num_agent_steps_trained: 320000\n",
      "num_env_steps_sampled: 320000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 320000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.024999999999999\n",
      "  ram_util_percent: 48.625\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141687211217749\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020302322408102622\n",
      "  mean_inference_ms: 0.23312180035224003\n",
      "  mean_raw_obs_processing_ms: 0.031719390204084374\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.0\n",
      "  episode_reward_mean: 48.12\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 42\n",
      "    - 50\n",
      "    - 24\n",
      "    - 26\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 6\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 89.0\n",
      "    - 16.0\n",
      "    - 77.0\n",
      "    - 81.0\n",
      "    - 84.0\n",
      "    - 73.0\n",
      "    - 3.0\n",
      "    - 0.0\n",
      "    - 103.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 78.0\n",
      "    - 75.0\n",
      "    - 62.0\n",
      "    - 69.0\n",
      "    - 66.0\n",
      "    - 31.0\n",
      "    - 16.0\n",
      "    - 42.0\n",
      "    - 29.0\n",
      "    - 5.0\n",
      "    - 54.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 57.0\n",
      "    - 47.0\n",
      "    - 27.0\n",
      "    - 42.0\n",
      "    - 50.0\n",
      "    - 65.0\n",
      "    - 54.0\n",
      "    - 78.0\n",
      "    - 46.0\n",
      "    - 35.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 52.0\n",
      "    - 62.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 50.0\n",
      "    - 52.0\n",
      "    - 38.0\n",
      "    - 31.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 87.0\n",
      "    - 64.0\n",
      "    - 67.0\n",
      "    - 97.0\n",
      "    - 80.0\n",
      "    - 45.0\n",
      "    - 30.0\n",
      "    - 99.0\n",
      "    - 40.0\n",
      "    - 42.0\n",
      "    - 24.0\n",
      "    - 59.0\n",
      "    - 49.0\n",
      "    - 28.0\n",
      "    - 53.0\n",
      "    - 86.0\n",
      "    - 59.0\n",
      "    - 42.0\n",
      "    - 53.0\n",
      "    - 46.0\n",
      "    - 11.0\n",
      "    - 48.0\n",
      "    - 82.0\n",
      "    - 85.0\n",
      "    - 40.0\n",
      "    - 58.0\n",
      "    - 75.0\n",
      "    - 53.0\n",
      "    - 38.0\n",
      "    - 66.0\n",
      "    - 55.0\n",
      "    - 50.0\n",
      "    - 50.0\n",
      "    - 38.0\n",
      "    - 75.0\n",
      "    - 73.0\n",
      "    - 44.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 78.0\n",
      "    - 61.0\n",
      "    - 69.0\n",
      "    - 26.0\n",
      "    - 40.0\n",
      "    - 71.0\n",
      "    - 72.0\n",
      "    - 13.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141687211217749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020302322408102622\n",
      "    mean_inference_ms: 0.23312180035224003\n",
      "    mean_raw_obs_processing_ms: 0.031719390204084374\n",
      "time_since_restore: 211.63367676734924\n",
      "time_this_iter_s: 2.645358085632324\n",
      "time_total_s: 211.63367676734924\n",
      "timers:\n",
      "  learn_throughput: 2840.755\n",
      "  learn_time_ms: 1408.076\n",
      "  load_throughput: 21570089.997\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2638.909\n",
      "  update_time_ms: 0.706\n",
      "timestamp: 1656299595\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 320000\n",
      "training_iteration: 80\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 324000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 324000\n",
      "  num_agent_steps_trained: 324000\n",
      "  num_env_steps_sampled: 324000\n",
      "  num_env_steps_trained: 324000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-18\n",
      "done: false\n",
      "episode_len_mean: 45.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 94.0\n",
      "episode_reward_mean: 46.35\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 88\n",
      "episodes_total: 7654\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0388663012494324\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012459847701573244\n",
      "        policy_loss: -0.010537739891198373\n",
      "        total_loss: 8.07679956395139\n",
      "        vf_explained_var: 7.487009930354292e-05\n",
      "        vf_loss: 8.085091978503812\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 324000\n",
      "  num_agent_steps_trained: 324000\n",
      "  num_env_steps_sampled: 324000\n",
      "  num_env_steps_trained: 324000\n",
      "iterations_since_restore: 81\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 324000\n",
      "num_agent_steps_trained: 324000\n",
      "num_env_steps_sampled: 324000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 324000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.799999999999999\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142169485765598\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020301287200716497\n",
      "  mean_inference_ms: 0.23312634518011727\n",
      "  mean_raw_obs_processing_ms: 0.03171819100166989\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 94.0\n",
      "  episode_reward_mean: 46.35\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 88\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 35\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 18\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 73.0\n",
      "    - 44.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 78.0\n",
      "    - 61.0\n",
      "    - 69.0\n",
      "    - 26.0\n",
      "    - 40.0\n",
      "    - 71.0\n",
      "    - 72.0\n",
      "    - 13.0\n",
      "    - 57.0\n",
      "    - 35.0\n",
      "    - 58.0\n",
      "    - 60.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 6.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 28.0\n",
      "    - 63.0\n",
      "    - 94.0\n",
      "    - 57.0\n",
      "    - 63.0\n",
      "    - 44.0\n",
      "    - 33.0\n",
      "    - 14.0\n",
      "    - 71.0\n",
      "    - 39.0\n",
      "    - 32.0\n",
      "    - 78.0\n",
      "    - 64.0\n",
      "    - 27.0\n",
      "    - 61.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 69.0\n",
      "    - 34.0\n",
      "    - 40.0\n",
      "    - 80.0\n",
      "    - 47.0\n",
      "    - 49.0\n",
      "    - 57.0\n",
      "    - 76.0\n",
      "    - 76.0\n",
      "    - 36.0\n",
      "    - 81.0\n",
      "    - 19.0\n",
      "    - 2.0\n",
      "    - 77.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 56.0\n",
      "    - 79.0\n",
      "    - 57.0\n",
      "    - 67.0\n",
      "    - 67.0\n",
      "    - 44.0\n",
      "    - 39.0\n",
      "    - 49.0\n",
      "    - 36.0\n",
      "    - 76.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 27.0\n",
      "    - 61.0\n",
      "    - 44.0\n",
      "    - 35.0\n",
      "    - 56.0\n",
      "    - 48.0\n",
      "    - 39.0\n",
      "    - 87.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 62.0\n",
      "    - 81.0\n",
      "    - 77.0\n",
      "    - 71.0\n",
      "    - 59.0\n",
      "    - 19.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 42.0\n",
      "    - 25.0\n",
      "    - 26.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142169485765598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020301287200716497\n",
      "    mean_inference_ms: 0.23312634518011727\n",
      "    mean_raw_obs_processing_ms: 0.03171819100166989\n",
      "time_since_restore: 214.28476476669312\n",
      "time_this_iter_s: 2.651087999343872\n",
      "time_total_s: 214.28476476669312\n",
      "timers:\n",
      "  learn_throughput: 2839.799\n",
      "  learn_time_ms: 1408.55\n",
      "  load_throughput: 21432314.768\n",
      "  load_time_ms: 0.187\n",
      "  training_iteration_time_ms: 2638.5\n",
      "  update_time_ms: 0.702\n",
      "timestamp: 1656299598\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 324000\n",
      "training_iteration: 81\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 328000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 328000\n",
      "  num_agent_steps_trained: 328000\n",
      "  num_env_steps_sampled: 328000\n",
      "  num_env_steps_trained: 328000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-20\n",
      "done: false\n",
      "episode_len_mean: 46.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 117.0\n",
      "episode_reward_mean: 48.67\n",
      "episode_reward_min: -9.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 7741\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.018423105439832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014697068266230084\n",
      "        policy_loss: -0.007427851859760541\n",
      "        total_loss: 8.702788245293402\n",
      "        vf_explained_var: 0.004173500499417705\n",
      "        vf_loss: 8.707567664628387\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 328000\n",
      "  num_agent_steps_trained: 328000\n",
      "  num_env_steps_sampled: 328000\n",
      "  num_env_steps_trained: 328000\n",
      "iterations_since_restore: 82\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 328000\n",
      "num_agent_steps_trained: 328000\n",
      "num_env_steps_sampled: 328000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 328000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.75\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142924145840748\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02030013820908184\n",
      "  mean_inference_ms: 0.23313316142589358\n",
      "  mean_raw_obs_processing_ms: 0.03171726052046187\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 117.0\n",
      "  episode_reward_mean: 48.67\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 55.0\n",
      "    - 62.0\n",
      "    - 81.0\n",
      "    - 77.0\n",
      "    - 71.0\n",
      "    - 59.0\n",
      "    - 19.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 42.0\n",
      "    - 25.0\n",
      "    - 26.0\n",
      "    - 81.0\n",
      "    - 86.0\n",
      "    - 33.0\n",
      "    - 57.0\n",
      "    - 81.0\n",
      "    - 85.0\n",
      "    - 91.0\n",
      "    - 37.0\n",
      "    - 31.0\n",
      "    - 35.0\n",
      "    - 54.0\n",
      "    - 39.0\n",
      "    - 64.0\n",
      "    - 46.0\n",
      "    - 44.0\n",
      "    - 44.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 65.0\n",
      "    - 48.0\n",
      "    - 67.0\n",
      "    - 70.0\n",
      "    - 46.0\n",
      "    - 82.0\n",
      "    - 68.0\n",
      "    - 49.0\n",
      "    - 26.0\n",
      "    - 60.0\n",
      "    - 3.0\n",
      "    - 54.0\n",
      "    - 75.0\n",
      "    - 89.0\n",
      "    - 60.0\n",
      "    - 45.0\n",
      "    - 28.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 34.0\n",
      "    - 77.0\n",
      "    - 74.0\n",
      "    - 89.0\n",
      "    - 26.0\n",
      "    - 34.0\n",
      "    - 20.0\n",
      "    - 33.0\n",
      "    - 9.0\n",
      "    - 74.0\n",
      "    - 82.0\n",
      "    - 46.0\n",
      "    - 63.0\n",
      "    - 63.0\n",
      "    - 49.0\n",
      "    - 22.0\n",
      "    - 43.0\n",
      "    - 46.0\n",
      "    - 39.0\n",
      "    - 38.0\n",
      "    - 25.0\n",
      "    - 7.0\n",
      "    - 57.0\n",
      "    - 72.0\n",
      "    - 48.0\n",
      "    - 54.0\n",
      "    - 88.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 117.0\n",
      "    - 67.0\n",
      "    - 26.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 60.0\n",
      "    - -9.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 79.0\n",
      "    - 84.0\n",
      "    - 39.0\n",
      "    - 27.0\n",
      "    - 69.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142924145840748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02030013820908184\n",
      "    mean_inference_ms: 0.23313316142589358\n",
      "    mean_raw_obs_processing_ms: 0.03171726052046187\n",
      "time_since_restore: 216.9201579093933\n",
      "time_this_iter_s: 2.6353931427001953\n",
      "time_total_s: 216.9201579093933\n",
      "timers:\n",
      "  learn_throughput: 2841.335\n",
      "  learn_time_ms: 1407.789\n",
      "  load_throughput: 21410433.895\n",
      "  load_time_ms: 0.187\n",
      "  training_iteration_time_ms: 2638.302\n",
      "  update_time_ms: 0.702\n",
      "timestamp: 1656299600\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 328000\n",
      "training_iteration: 82\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 332000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 332000\n",
      "  num_agent_steps_trained: 332000\n",
      "  num_env_steps_sampled: 332000\n",
      "  num_env_steps_trained: 332000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-23\n",
      "done: false\n",
      "episode_len_mean: 46.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 117.0\n",
      "episode_reward_mean: 47.82\n",
      "episode_reward_min: -9.0\n",
      "episodes_this_iter: 84\n",
      "episodes_total: 7825\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.032028841523714\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01187443779881603\n",
      "        policy_loss: -0.014911866582609633\n",
      "        total_loss: 8.547096917962515\n",
      "        vf_explained_var: 0.001572088336431852\n",
      "        vf_loss: 8.559868999194073\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 332000\n",
      "  num_agent_steps_trained: 332000\n",
      "  num_env_steps_sampled: 332000\n",
      "  num_env_steps_trained: 332000\n",
      "iterations_since_restore: 83\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 332000\n",
      "num_agent_steps_trained: 332000\n",
      "num_env_steps_sampled: 332000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 332000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.125\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.01514322233088756\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020298745550286813\n",
      "  mean_inference_ms: 0.2331354988459559\n",
      "  mean_raw_obs_processing_ms: 0.031714589985188286\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 117.0\n",
      "  episode_reward_mean: 47.82\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 84\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 48\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 117.0\n",
      "    - 67.0\n",
      "    - 26.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 60.0\n",
      "    - -9.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 0.0\n",
      "    - 79.0\n",
      "    - 84.0\n",
      "    - 39.0\n",
      "    - 27.0\n",
      "    - 69.0\n",
      "    - 56.0\n",
      "    - 49.0\n",
      "    - 44.0\n",
      "    - 52.0\n",
      "    - 83.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 56.0\n",
      "    - 40.0\n",
      "    - 76.0\n",
      "    - 65.0\n",
      "    - 33.0\n",
      "    - 50.0\n",
      "    - 76.0\n",
      "    - 62.0\n",
      "    - 27.0\n",
      "    - 46.0\n",
      "    - 25.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 11.0\n",
      "    - 35.0\n",
      "    - 68.0\n",
      "    - 71.0\n",
      "    - 80.0\n",
      "    - 59.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 44.0\n",
      "    - 70.0\n",
      "    - 45.0\n",
      "    - 33.0\n",
      "    - 43.0\n",
      "    - 65.0\n",
      "    - 35.0\n",
      "    - 2.0\n",
      "    - 95.0\n",
      "    - 18.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 8.0\n",
      "    - 66.0\n",
      "    - 53.0\n",
      "    - 44.0\n",
      "    - 83.0\n",
      "    - 52.0\n",
      "    - 69.0\n",
      "    - 64.0\n",
      "    - 24.0\n",
      "    - 42.0\n",
      "    - 65.0\n",
      "    - 29.0\n",
      "    - 66.0\n",
      "    - 26.0\n",
      "    - 62.0\n",
      "    - 47.0\n",
      "    - 5.0\n",
      "    - 23.0\n",
      "    - 102.0\n",
      "    - 69.0\n",
      "    - 35.0\n",
      "    - 73.0\n",
      "    - 5.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 51.0\n",
      "    - 25.0\n",
      "    - 32.0\n",
      "    - 61.0\n",
      "    - 39.0\n",
      "    - 80.0\n",
      "    - 75.0\n",
      "    - 40.0\n",
      "    - 85.0\n",
      "    - 96.0\n",
      "    - 92.0\n",
      "    - 13.0\n",
      "    - 61.0\n",
      "    - 51.0\n",
      "    - 60.0\n",
      "    - 36.0\n",
      "    - 53.0\n",
      "    - 55.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.01514322233088756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020298745550286813\n",
      "    mean_inference_ms: 0.2331354988459559\n",
      "    mean_raw_obs_processing_ms: 0.031714589985188286\n",
      "time_since_restore: 219.56879687309265\n",
      "time_this_iter_s: 2.648638963699341\n",
      "time_total_s: 219.56879687309265\n",
      "timers:\n",
      "  learn_throughput: 2841.284\n",
      "  learn_time_ms: 1407.814\n",
      "  load_throughput: 21545159.882\n",
      "  load_time_ms: 0.186\n",
      "  training_iteration_time_ms: 2638.823\n",
      "  update_time_ms: 0.71\n",
      "timestamp: 1656299603\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 332000\n",
      "training_iteration: 83\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 336000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 336000\n",
      "  num_agent_steps_trained: 336000\n",
      "  num_env_steps_sampled: 336000\n",
      "  num_env_steps_trained: 336000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-26\n",
      "done: false\n",
      "episode_len_mean: 46.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 96.0\n",
      "episode_reward_mean: 50.41\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 7912\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0243527565592079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014585039873297016\n",
      "        policy_loss: -0.016533778908271943\n",
      "        total_loss: 8.704580572087277\n",
      "        vf_explained_var: 0.003890680241328414\n",
      "        vf_loss: 8.718486086527507\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 336000\n",
      "  num_agent_steps_trained: 336000\n",
      "  num_env_steps_sampled: 336000\n",
      "  num_env_steps_trained: 336000\n",
      "iterations_since_restore: 84\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 336000\n",
      "num_agent_steps_trained: 336000\n",
      "num_env_steps_sampled: 336000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 336000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.775000000000002\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142553124988349\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020295649311285247\n",
      "  mean_inference_ms: 0.2331259088178301\n",
      "  mean_raw_obs_processing_ms: 0.03171035234930519\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 96.0\n",
      "  episode_reward_mean: 50.41\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 80.0\n",
      "    - 75.0\n",
      "    - 40.0\n",
      "    - 85.0\n",
      "    - 96.0\n",
      "    - 92.0\n",
      "    - 13.0\n",
      "    - 61.0\n",
      "    - 51.0\n",
      "    - 60.0\n",
      "    - 36.0\n",
      "    - 53.0\n",
      "    - 55.0\n",
      "    - 40.0\n",
      "    - 90.0\n",
      "    - 60.0\n",
      "    - 36.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 64.0\n",
      "    - 39.0\n",
      "    - 83.0\n",
      "    - 73.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 28.0\n",
      "    - 68.0\n",
      "    - 84.0\n",
      "    - 32.0\n",
      "    - 49.0\n",
      "    - 84.0\n",
      "    - 58.0\n",
      "    - 49.0\n",
      "    - 11.0\n",
      "    - 52.0\n",
      "    - 18.0\n",
      "    - 56.0\n",
      "    - 94.0\n",
      "    - 80.0\n",
      "    - 73.0\n",
      "    - 52.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 23.0\n",
      "    - 60.0\n",
      "    - 28.0\n",
      "    - 55.0\n",
      "    - 51.0\n",
      "    - 32.0\n",
      "    - 63.0\n",
      "    - 68.0\n",
      "    - 42.0\n",
      "    - 72.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 65.0\n",
      "    - 16.0\n",
      "    - 47.0\n",
      "    - 38.0\n",
      "    - 74.0\n",
      "    - 52.0\n",
      "    - 88.0\n",
      "    - 19.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 1.0\n",
      "    - 58.0\n",
      "    - 42.0\n",
      "    - 21.0\n",
      "    - 53.0\n",
      "    - 82.0\n",
      "    - 76.0\n",
      "    - 64.0\n",
      "    - 26.0\n",
      "    - 40.0\n",
      "    - 88.0\n",
      "    - 37.0\n",
      "    - 78.0\n",
      "    - 91.0\n",
      "    - 23.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 59.0\n",
      "    - 92.0\n",
      "    - 61.0\n",
      "    - 68.0\n",
      "    - 52.0\n",
      "    - 49.0\n",
      "    - 45.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 71.0\n",
      "    - 42.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142553124988349\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020295649311285247\n",
      "    mean_inference_ms: 0.2331259088178301\n",
      "    mean_raw_obs_processing_ms: 0.03171035234930519\n",
      "time_since_restore: 222.20794773101807\n",
      "time_this_iter_s: 2.639150857925415\n",
      "time_total_s: 222.20794773101807\n",
      "timers:\n",
      "  learn_throughput: 2842.125\n",
      "  learn_time_ms: 1407.397\n",
      "  load_throughput: 21611768.646\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2637.95\n",
      "  update_time_ms: 0.712\n",
      "timestamp: 1656299606\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 336000\n",
      "training_iteration: 84\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 340000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 340000\n",
      "  num_agent_steps_trained: 340000\n",
      "  num_env_steps_sampled: 340000\n",
      "  num_env_steps_trained: 340000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-28\n",
      "done: false\n",
      "episode_len_mean: 46.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 101.0\n",
      "episode_reward_mean: 50.62\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 84\n",
      "episodes_total: 7996\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.01248937396593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01401564182453258\n",
      "        policy_loss: -0.014697685151771512\n",
      "        total_loss: 8.899268330297163\n",
      "        vf_explained_var: 0.0018810260680414016\n",
      "        vf_loss: 8.911440388874341\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 340000\n",
      "  num_agent_steps_trained: 340000\n",
      "  num_env_steps_sampled: 340000\n",
      "  num_env_steps_trained: 340000\n",
      "iterations_since_restore: 85\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 340000\n",
      "num_agent_steps_trained: 340000\n",
      "num_env_steps_sampled: 340000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 340000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.433333333333332\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141870210076453\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020292891990881606\n",
      "  mean_inference_ms: 0.23311468424975046\n",
      "  mean_raw_obs_processing_ms: 0.03170552409480743\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 50.62\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 84\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 22\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 23.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 59.0\n",
      "    - 92.0\n",
      "    - 61.0\n",
      "    - 68.0\n",
      "    - 52.0\n",
      "    - 49.0\n",
      "    - 45.0\n",
      "    - 53.0\n",
      "    - 0.0\n",
      "    - 48.0\n",
      "    - 71.0\n",
      "    - 42.0\n",
      "    - 57.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 5.0\n",
      "    - 33.0\n",
      "    - 96.0\n",
      "    - 14.0\n",
      "    - 52.0\n",
      "    - 56.0\n",
      "    - 42.0\n",
      "    - 43.0\n",
      "    - 82.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 86.0\n",
      "    - 76.0\n",
      "    - 62.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 17.0\n",
      "    - 88.0\n",
      "    - 60.0\n",
      "    - 56.0\n",
      "    - 35.0\n",
      "    - 40.0\n",
      "    - 44.0\n",
      "    - 101.0\n",
      "    - 41.0\n",
      "    - 39.0\n",
      "    - 70.0\n",
      "    - 61.0\n",
      "    - 51.0\n",
      "    - 70.0\n",
      "    - 42.0\n",
      "    - 59.0\n",
      "    - 39.0\n",
      "    - 51.0\n",
      "    - 58.0\n",
      "    - 66.0\n",
      "    - 50.0\n",
      "    - 42.0\n",
      "    - 66.0\n",
      "    - 22.0\n",
      "    - 88.0\n",
      "    - 65.0\n",
      "    - 5.0\n",
      "    - 78.0\n",
      "    - 53.0\n",
      "    - 47.0\n",
      "    - 44.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 82.0\n",
      "    - 68.0\n",
      "    - 75.0\n",
      "    - 60.0\n",
      "    - 86.0\n",
      "    - 52.0\n",
      "    - 62.0\n",
      "    - 24.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 55.0\n",
      "    - 74.0\n",
      "    - 45.0\n",
      "    - 87.0\n",
      "    - 74.0\n",
      "    - 67.0\n",
      "    - 53.0\n",
      "    - 71.0\n",
      "    - 30.0\n",
      "    - 72.0\n",
      "    - 34.0\n",
      "    - 52.0\n",
      "    - 30.0\n",
      "    - 60.0\n",
      "    - 43.0\n",
      "    - 33.0\n",
      "    - 40.0\n",
      "    - 43.0\n",
      "    - 47.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141870210076453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020292891990881606\n",
      "    mean_inference_ms: 0.23311468424975046\n",
      "    mean_raw_obs_processing_ms: 0.03170552409480743\n",
      "time_since_restore: 224.85915160179138\n",
      "time_this_iter_s: 2.6512038707733154\n",
      "time_total_s: 224.85915160179138\n",
      "timers:\n",
      "  learn_throughput: 2834.877\n",
      "  learn_time_ms: 1410.996\n",
      "  load_throughput: 21542393.426\n",
      "  load_time_ms: 0.186\n",
      "  training_iteration_time_ms: 2640.883\n",
      "  update_time_ms: 0.711\n",
      "timestamp: 1656299608\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 340000\n",
      "training_iteration: 85\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 344000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 344000\n",
      "  num_agent_steps_trained: 344000\n",
      "  num_env_steps_sampled: 344000\n",
      "  num_env_steps_trained: 344000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-31\n",
      "done: false\n",
      "episode_len_mean: 48.09\n",
      "episode_media: {}\n",
      "episode_reward_max: 109.0\n",
      "episode_reward_mean: 50.0\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 84\n",
      "episodes_total: 8080\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.031498505351364\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012658204098973143\n",
      "        policy_loss: -0.011655955530342555\n",
      "        total_loss: 8.678767249404743\n",
      "        vf_explained_var: 0.0027833946289554717\n",
      "        vf_loss: 8.688142125324536\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 344000\n",
      "  num_agent_steps_trained: 344000\n",
      "  num_env_steps_sampled: 344000\n",
      "  num_env_steps_trained: 344000\n",
      "iterations_since_restore: 86\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 344000\n",
      "num_agent_steps_trained: 344000\n",
      "num_env_steps_sampled: 344000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 344000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.1\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141049060015215\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02029059001249121\n",
      "  mean_inference_ms: 0.2331982581113803\n",
      "  mean_raw_obs_processing_ms: 0.0317013399275929\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 48.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 109.0\n",
      "  episode_reward_mean: 50.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 84\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 87.0\n",
      "    - 74.0\n",
      "    - 67.0\n",
      "    - 53.0\n",
      "    - 71.0\n",
      "    - 30.0\n",
      "    - 72.0\n",
      "    - 34.0\n",
      "    - 52.0\n",
      "    - 30.0\n",
      "    - 60.0\n",
      "    - 43.0\n",
      "    - 33.0\n",
      "    - 40.0\n",
      "    - 43.0\n",
      "    - 47.0\n",
      "    - 34.0\n",
      "    - 74.0\n",
      "    - 67.0\n",
      "    - 38.0\n",
      "    - 31.0\n",
      "    - 47.0\n",
      "    - 24.0\n",
      "    - 0.0\n",
      "    - 32.0\n",
      "    - 82.0\n",
      "    - 0.0\n",
      "    - 65.0\n",
      "    - 34.0\n",
      "    - 62.0\n",
      "    - 60.0\n",
      "    - 73.0\n",
      "    - 73.0\n",
      "    - 71.0\n",
      "    - 28.0\n",
      "    - 58.0\n",
      "    - 56.0\n",
      "    - 59.0\n",
      "    - 38.0\n",
      "    - 52.0\n",
      "    - 35.0\n",
      "    - 50.0\n",
      "    - 36.0\n",
      "    - 51.0\n",
      "    - 40.0\n",
      "    - 58.0\n",
      "    - 63.0\n",
      "    - 46.0\n",
      "    - 16.0\n",
      "    - 73.0\n",
      "    - 65.0\n",
      "    - 53.0\n",
      "    - 109.0\n",
      "    - 62.0\n",
      "    - 62.0\n",
      "    - 24.0\n",
      "    - 67.0\n",
      "    - 72.0\n",
      "    - 33.0\n",
      "    - 53.0\n",
      "    - 69.0\n",
      "    - 97.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 22.0\n",
      "    - 59.0\n",
      "    - 59.0\n",
      "    - 26.0\n",
      "    - 73.0\n",
      "    - 93.0\n",
      "    - 31.0\n",
      "    - 46.0\n",
      "    - 58.0\n",
      "    - 52.0\n",
      "    - 57.0\n",
      "    - 19.0\n",
      "    - 38.0\n",
      "    - 51.0\n",
      "    - 46.0\n",
      "    - 32.0\n",
      "    - 35.0\n",
      "    - 41.0\n",
      "    - 40.0\n",
      "    - 65.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 81.0\n",
      "    - 48.0\n",
      "    - 55.0\n",
      "    - 38.0\n",
      "    - 71.0\n",
      "    - 25.0\n",
      "    - 77.0\n",
      "    - 57.0\n",
      "    - 48.0\n",
      "    - 63.0\n",
      "    - 40.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141049060015215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02029059001249121\n",
      "    mean_inference_ms: 0.2331982581113803\n",
      "    mean_raw_obs_processing_ms: 0.0317013399275929\n",
      "time_since_restore: 227.55118775367737\n",
      "time_this_iter_s: 2.6920361518859863\n",
      "time_total_s: 227.55118775367737\n",
      "timers:\n",
      "  learn_throughput: 2832.718\n",
      "  learn_time_ms: 1412.071\n",
      "  load_throughput: 21611768.646\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2645.462\n",
      "  update_time_ms: 0.707\n",
      "timestamp: 1656299611\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 344000\n",
      "training_iteration: 86\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 348000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 348000\n",
      "  num_agent_steps_trained: 348000\n",
      "  num_env_steps_sampled: 348000\n",
      "  num_env_steps_trained: 348000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-34\n",
      "done: false\n",
      "episode_len_mean: 46.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 94.0\n",
      "episode_reward_mean: 46.98\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 88\n",
      "episodes_total: 8168\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0214803969347348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01568627074531089\n",
      "        policy_loss: -0.020124304665112367\n",
      "        total_loss: 8.305921109517415\n",
      "        vf_explained_var: 0.004405385896723757\n",
      "        vf_loss: 8.323218697886313\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 348000\n",
      "  num_agent_steps_trained: 348000\n",
      "  num_env_steps_sampled: 348000\n",
      "  num_env_steps_trained: 348000\n",
      "iterations_since_restore: 87\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 348000\n",
      "num_agent_steps_trained: 348000\n",
      "num_env_steps_sampled: 348000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 348000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.350000000000001\n",
      "  ram_util_percent: 48.625\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141262445150704\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020289852221106387\n",
      "  mean_inference_ms: 0.23321232136869122\n",
      "  mean_raw_obs_processing_ms: 0.03170000839164827\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 94.0\n",
      "  episode_reward_mean: 46.98\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 88\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 52.0\n",
      "    - 81.0\n",
      "    - 48.0\n",
      "    - 55.0\n",
      "    - 38.0\n",
      "    - 71.0\n",
      "    - 25.0\n",
      "    - 77.0\n",
      "    - 57.0\n",
      "    - 48.0\n",
      "    - 63.0\n",
      "    - 40.0\n",
      "    - 49.0\n",
      "    - 44.0\n",
      "    - 57.0\n",
      "    - 65.0\n",
      "    - 60.0\n",
      "    - 74.0\n",
      "    - 35.0\n",
      "    - 15.0\n",
      "    - 49.0\n",
      "    - 76.0\n",
      "    - 85.0\n",
      "    - 80.0\n",
      "    - 82.0\n",
      "    - 91.0\n",
      "    - 59.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 43.0\n",
      "    - 86.0\n",
      "    - 70.0\n",
      "    - 3.0\n",
      "    - 59.0\n",
      "    - 8.0\n",
      "    - 70.0\n",
      "    - 51.0\n",
      "    - 63.0\n",
      "    - 67.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 42.0\n",
      "    - 26.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 34.0\n",
      "    - 69.0\n",
      "    - 37.0\n",
      "    - 39.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 52.0\n",
      "    - 12.0\n",
      "    - 45.0\n",
      "    - 54.0\n",
      "    - 76.0\n",
      "    - 54.0\n",
      "    - 58.0\n",
      "    - 29.0\n",
      "    - 39.0\n",
      "    - 51.0\n",
      "    - 24.0\n",
      "    - 53.0\n",
      "    - 91.0\n",
      "    - 73.0\n",
      "    - 64.0\n",
      "    - 48.0\n",
      "    - 11.0\n",
      "    - 15.0\n",
      "    - 29.0\n",
      "    - 65.0\n",
      "    - 90.0\n",
      "    - 62.0\n",
      "    - 48.0\n",
      "    - 65.0\n",
      "    - 60.0\n",
      "    - 54.0\n",
      "    - 42.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 22.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 43.0\n",
      "    - 6.0\n",
      "    - 27.0\n",
      "    - 64.0\n",
      "    - 4.0\n",
      "    - 94.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 60.0\n",
      "    - 64.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141262445150704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020289852221106387\n",
      "    mean_inference_ms: 0.23321232136869122\n",
      "    mean_raw_obs_processing_ms: 0.03170000839164827\n",
      "time_since_restore: 230.1852617263794\n",
      "time_this_iter_s: 2.6340739727020264\n",
      "time_total_s: 230.1852617263794\n",
      "timers:\n",
      "  learn_throughput: 2832.795\n",
      "  learn_time_ms: 1412.033\n",
      "  load_throughput: 21765978.204\n",
      "  load_time_ms: 0.184\n",
      "  training_iteration_time_ms: 2646.074\n",
      "  update_time_ms: 0.736\n",
      "timestamp: 1656299614\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 348000\n",
      "training_iteration: 87\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 352000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 352000\n",
      "  num_agent_steps_trained: 352000\n",
      "  num_env_steps_sampled: 352000\n",
      "  num_env_steps_trained: 352000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-36\n",
      "done: false\n",
      "episode_len_mean: 46.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 98.0\n",
      "episode_reward_mean: 49.23\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 85\n",
      "episodes_total: 8253\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0118021154275505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014402158436695895\n",
      "        policy_loss: -0.018566398969000225\n",
      "        total_loss: 8.22710473921991\n",
      "        vf_explained_var: 0.001321670445062781\n",
      "        vf_loss: 8.243075773792881\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 352000\n",
      "  num_agent_steps_trained: 352000\n",
      "  num_env_steps_sampled: 352000\n",
      "  num_env_steps_trained: 352000\n",
      "iterations_since_restore: 88\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 352000\n",
      "num_agent_steps_trained: 352000\n",
      "num_env_steps_sampled: 352000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 352000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.125\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141281788429795\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020287937203250794\n",
      "  mean_inference_ms: 0.23320570011005962\n",
      "  mean_raw_obs_processing_ms: 0.03169724111445728\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 98.0\n",
      "  episode_reward_mean: 49.23\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 85\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 24\n",
      "    - 50\n",
      "    - 28\n",
      "    - 48\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 43.0\n",
      "    - 6.0\n",
      "    - 27.0\n",
      "    - 64.0\n",
      "    - 4.0\n",
      "    - 94.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 60.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 25.0\n",
      "    - 53.0\n",
      "    - 50.0\n",
      "    - 79.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 74.0\n",
      "    - 42.0\n",
      "    - 89.0\n",
      "    - 82.0\n",
      "    - 8.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 40.0\n",
      "    - 61.0\n",
      "    - 34.0\n",
      "    - 84.0\n",
      "    - 71.0\n",
      "    - 45.0\n",
      "    - 51.0\n",
      "    - 75.0\n",
      "    - 77.0\n",
      "    - 63.0\n",
      "    - 84.0\n",
      "    - 67.0\n",
      "    - 86.0\n",
      "    - 54.0\n",
      "    - 87.0\n",
      "    - 14.0\n",
      "    - 74.0\n",
      "    - 54.0\n",
      "    - 72.0\n",
      "    - 92.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 58.0\n",
      "    - 50.0\n",
      "    - 47.0\n",
      "    - 68.0\n",
      "    - 87.0\n",
      "    - 71.0\n",
      "    - 70.0\n",
      "    - 52.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 89.0\n",
      "    - 58.0\n",
      "    - 40.0\n",
      "    - 44.0\n",
      "    - 50.0\n",
      "    - 74.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 66.0\n",
      "    - 31.0\n",
      "    - 50.0\n",
      "    - 81.0\n",
      "    - 51.0\n",
      "    - 16.0\n",
      "    - 5.0\n",
      "    - 77.0\n",
      "    - 53.0\n",
      "    - 54.0\n",
      "    - 34.0\n",
      "    - 70.0\n",
      "    - 54.0\n",
      "    - 98.0\n",
      "    - 36.0\n",
      "    - 79.0\n",
      "    - 70.0\n",
      "    - 29.0\n",
      "    - 53.0\n",
      "    - 33.0\n",
      "    - 2.0\n",
      "    - 63.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 61.0\n",
      "    - 98.0\n",
      "    - 38.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141281788429795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020287937203250794\n",
      "    mean_inference_ms: 0.23320570011005962\n",
      "    mean_raw_obs_processing_ms: 0.03169724111445728\n",
      "time_since_restore: 232.83749675750732\n",
      "time_this_iter_s: 2.6522350311279297\n",
      "time_total_s: 232.83749675750732\n",
      "timers:\n",
      "  learn_throughput: 2830.257\n",
      "  learn_time_ms: 1413.299\n",
      "  load_throughput: 21595077.874\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2647.379\n",
      "  update_time_ms: 0.736\n",
      "timestamp: 1656299616\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 352000\n",
      "training_iteration: 88\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 356000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 356000\n",
      "  num_agent_steps_trained: 356000\n",
      "  num_env_steps_sampled: 356000\n",
      "  num_env_steps_trained: 356000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-39\n",
      "done: false\n",
      "episode_len_mean: 43.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 104.0\n",
      "episode_reward_mean: 43.29\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 93\n",
      "episodes_total: 8346\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 1.0053375771609685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013222993032566162\n",
      "        policy_loss: -0.016546265571628527\n",
      "        total_loss: 8.272951213518779\n",
      "        vf_explained_var: 0.0033495562691842357\n",
      "        vf_loss: 8.287114652254248\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 356000\n",
      "  num_agent_steps_trained: 356000\n",
      "  num_env_steps_sampled: 356000\n",
      "  num_env_steps_trained: 356000\n",
      "iterations_since_restore: 89\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 356000\n",
      "num_agent_steps_trained: 356000\n",
      "num_env_steps_sampled: 356000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 356000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.375\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141840800490447\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020285729663273643\n",
      "  mean_inference_ms: 0.23320707322979145\n",
      "  mean_raw_obs_processing_ms: 0.03169959917528525\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 43.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 104.0\n",
      "  episode_reward_mean: 43.29\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 93\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 28\n",
      "    - 48\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 41\n",
      "    - 36\n",
      "    - 20\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 21\n",
      "    - 6\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 49\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 14\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    episode_reward:\n",
      "    - 63.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 9.0\n",
      "    - 61.0\n",
      "    - 98.0\n",
      "    - 38.0\n",
      "    - 7.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 33.0\n",
      "    - 46.0\n",
      "    - 74.0\n",
      "    - 49.0\n",
      "    - 27.0\n",
      "    - 55.0\n",
      "    - 49.0\n",
      "    - 104.0\n",
      "    - 94.0\n",
      "    - 50.0\n",
      "    - 46.0\n",
      "    - 53.0\n",
      "    - 59.0\n",
      "    - 52.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 103.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 35.0\n",
      "    - 3.0\n",
      "    - 64.0\n",
      "    - 57.0\n",
      "    - 26.0\n",
      "    - 23.0\n",
      "    - 31.0\n",
      "    - 57.0\n",
      "    - 68.0\n",
      "    - 2.0\n",
      "    - 70.0\n",
      "    - 64.0\n",
      "    - 9.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 29.0\n",
      "    - 40.0\n",
      "    - 86.0\n",
      "    - 0.0\n",
      "    - 45.0\n",
      "    - 91.0\n",
      "    - 40.0\n",
      "    - 88.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 38.0\n",
      "    - 61.0\n",
      "    - 73.0\n",
      "    - 79.0\n",
      "    - 70.0\n",
      "    - 89.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 3.0\n",
      "    - 74.0\n",
      "    - 49.0\n",
      "    - 91.0\n",
      "    - 38.0\n",
      "    - 48.0\n",
      "    - 75.0\n",
      "    - 48.0\n",
      "    - 25.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 28.0\n",
      "    - 47.0\n",
      "    - 67.0\n",
      "    - 41.0\n",
      "    - 83.0\n",
      "    - 69.0\n",
      "    - 34.0\n",
      "    - 47.0\n",
      "    - 3.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 83.0\n",
      "    - 29.0\n",
      "    - 75.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141840800490447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020285729663273643\n",
      "    mean_inference_ms: 0.23320707322979145\n",
      "    mean_raw_obs_processing_ms: 0.03169959917528525\n",
      "time_since_restore: 235.47719979286194\n",
      "time_this_iter_s: 2.6397030353546143\n",
      "time_total_s: 235.47719979286194\n",
      "timers:\n",
      "  learn_throughput: 2832.97\n",
      "  learn_time_ms: 1411.946\n",
      "  load_throughput: 21454240.409\n",
      "  load_time_ms: 0.186\n",
      "  training_iteration_time_ms: 2646.473\n",
      "  update_time_ms: 0.739\n",
      "timestamp: 1656299619\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 356000\n",
      "training_iteration: 89\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 360000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 360000\n",
      "  num_agent_steps_trained: 360000\n",
      "  num_env_steps_sampled: 360000\n",
      "  num_env_steps_trained: 360000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-42\n",
      "done: false\n",
      "episode_len_mean: 45.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 103.0\n",
      "episode_reward_mean: 47.21\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 8432\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9943208335548319\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01300123383801791\n",
      "        policy_loss: -0.010407867132415694\n",
      "        total_loss: 8.503296614718694\n",
      "        vf_explained_var: 0.0028210901444958103\n",
      "        vf_loss: 8.511361654855872\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 360000\n",
      "  num_agent_steps_trained: 360000\n",
      "  num_env_steps_sampled: 360000\n",
      "  num_env_steps_trained: 360000\n",
      "iterations_since_restore: 90\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 360000\n",
      "num_agent_steps_trained: 360000\n",
      "num_env_steps_sampled: 360000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 360000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.333333333333332\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142224046484208\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02028350214310613\n",
      "  mean_inference_ms: 0.2332061034336896\n",
      "  mean_raw_obs_processing_ms: 0.031697661197058816\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.0\n",
      "  episode_reward_mean: 47.21\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 14\n",
      "    - 21\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 24\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 3.0\n",
      "    - 48.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 50.0\n",
      "    - 83.0\n",
      "    - 29.0\n",
      "    - 75.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 76.0\n",
      "    - 94.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 79.0\n",
      "    - 31.0\n",
      "    - 61.0\n",
      "    - 52.0\n",
      "    - 75.0\n",
      "    - 39.0\n",
      "    - 72.0\n",
      "    - 67.0\n",
      "    - 29.0\n",
      "    - 74.0\n",
      "    - 71.0\n",
      "    - 52.0\n",
      "    - 71.0\n",
      "    - 9.0\n",
      "    - 32.0\n",
      "    - 18.0\n",
      "    - 50.0\n",
      "    - 56.0\n",
      "    - 54.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 37.0\n",
      "    - 45.0\n",
      "    - 26.0\n",
      "    - 9.0\n",
      "    - 47.0\n",
      "    - 0.0\n",
      "    - 69.0\n",
      "    - 53.0\n",
      "    - 82.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 86.0\n",
      "    - 83.0\n",
      "    - 20.0\n",
      "    - 59.0\n",
      "    - 69.0\n",
      "    - 80.0\n",
      "    - 81.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 62.0\n",
      "    - 75.0\n",
      "    - 45.0\n",
      "    - 77.0\n",
      "    - 35.0\n",
      "    - 75.0\n",
      "    - 12.0\n",
      "    - 73.0\n",
      "    - 0.0\n",
      "    - 4.0\n",
      "    - 0.0\n",
      "    - 35.0\n",
      "    - 55.0\n",
      "    - 83.0\n",
      "    - 58.0\n",
      "    - 64.0\n",
      "    - 60.0\n",
      "    - 66.0\n",
      "    - 103.0\n",
      "    - 66.0\n",
      "    - 71.0\n",
      "    - 18.0\n",
      "    - 63.0\n",
      "    - 47.0\n",
      "    - 47.0\n",
      "    - 32.0\n",
      "    - 20.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 64.0\n",
      "    - 56.0\n",
      "    - 69.0\n",
      "    - 36.0\n",
      "    - 47.0\n",
      "    - 44.0\n",
      "    - 54.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142224046484208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02028350214310613\n",
      "    mean_inference_ms: 0.2332061034336896\n",
      "    mean_raw_obs_processing_ms: 0.031697661197058816\n",
      "time_since_restore: 238.1353018283844\n",
      "time_this_iter_s: 2.658102035522461\n",
      "time_total_s: 238.1353018283844\n",
      "timers:\n",
      "  learn_throughput: 2830.96\n",
      "  learn_time_ms: 1412.948\n",
      "  load_throughput: 21534098.319\n",
      "  load_time_ms: 0.186\n",
      "  training_iteration_time_ms: 2647.747\n",
      "  update_time_ms: 0.753\n",
      "timestamp: 1656299622\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 360000\n",
      "training_iteration: 90\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 364000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 364000\n",
      "  num_agent_steps_trained: 364000\n",
      "  num_env_steps_sampled: 364000\n",
      "  num_env_steps_trained: 364000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-44\n",
      "done: false\n",
      "episode_len_mean: 44.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 106.0\n",
      "episode_reward_mean: 46.0\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 89\n",
      "episodes_total: 8521\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9794582554730036\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015337983576921366\n",
      "        policy_loss: -0.014443758464047825\n",
      "        total_loss: 8.527455704186552\n",
      "        vf_explained_var: 0.003952406747366792\n",
      "        vf_loss: 8.539135507870746\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 364000\n",
      "  num_agent_steps_trained: 364000\n",
      "  num_env_steps_sampled: 364000\n",
      "  num_env_steps_trained: 364000\n",
      "iterations_since_restore: 91\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 364000\n",
      "num_agent_steps_trained: 364000\n",
      "num_env_steps_sampled: 364000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 364000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.575000000000001\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142433833554713\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02028066346058609\n",
      "  mean_inference_ms: 0.2332058462761533\n",
      "  mean_raw_obs_processing_ms: 0.031698396112251045\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 44.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.0\n",
      "  episode_reward_mean: 46.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 89\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 26\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 20.0\n",
      "    - 80.0\n",
      "    - 0.0\n",
      "    - 80.0\n",
      "    - 64.0\n",
      "    - 56.0\n",
      "    - 69.0\n",
      "    - 36.0\n",
      "    - 47.0\n",
      "    - 44.0\n",
      "    - 54.0\n",
      "    - 53.0\n",
      "    - 38.0\n",
      "    - 37.0\n",
      "    - 31.0\n",
      "    - 51.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 41.0\n",
      "    - 77.0\n",
      "    - 47.0\n",
      "    - 42.0\n",
      "    - 72.0\n",
      "    - 72.0\n",
      "    - 22.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 16.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 49.0\n",
      "    - 45.0\n",
      "    - 69.0\n",
      "    - 51.0\n",
      "    - 45.0\n",
      "    - 58.0\n",
      "    - 34.0\n",
      "    - 61.0\n",
      "    - 22.0\n",
      "    - 79.0\n",
      "    - 98.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 50.0\n",
      "    - 5.0\n",
      "    - 53.0\n",
      "    - 25.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 63.0\n",
      "    - 90.0\n",
      "    - 53.0\n",
      "    - 63.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 64.0\n",
      "    - 81.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 46.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 64.0\n",
      "    - 21.0\n",
      "    - 57.0\n",
      "    - 30.0\n",
      "    - 87.0\n",
      "    - 29.0\n",
      "    - 106.0\n",
      "    - 32.0\n",
      "    - 20.0\n",
      "    - 31.0\n",
      "    - 31.0\n",
      "    - 78.0\n",
      "    - 36.0\n",
      "    - 45.0\n",
      "    - 49.0\n",
      "    - 61.0\n",
      "    - 25.0\n",
      "    - 36.0\n",
      "    - 76.0\n",
      "    - 74.0\n",
      "    - 81.0\n",
      "    - 79.0\n",
      "    - 26.0\n",
      "    - 69.0\n",
      "    - 76.0\n",
      "    - 33.0\n",
      "    - 90.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 52.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142433833554713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02028066346058609\n",
      "    mean_inference_ms: 0.2332058462761533\n",
      "    mean_raw_obs_processing_ms: 0.031698396112251045\n",
      "time_since_restore: 240.78523182868958\n",
      "time_this_iter_s: 2.649930000305176\n",
      "time_total_s: 240.78523182868958\n",
      "timers:\n",
      "  learn_throughput: 2830.869\n",
      "  learn_time_ms: 1412.994\n",
      "  load_throughput: 21709648.033\n",
      "  load_time_ms: 0.184\n",
      "  training_iteration_time_ms: 2647.611\n",
      "  update_time_ms: 0.741\n",
      "timestamp: 1656299624\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 364000\n",
      "training_iteration: 91\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 368000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 368000\n",
      "  num_agent_steps_trained: 368000\n",
      "  num_env_steps_sampled: 368000\n",
      "  num_env_steps_trained: 368000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-47\n",
      "done: false\n",
      "episode_len_mean: 46.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 103.0\n",
      "episode_reward_mean: 49.75\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 8607\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9707524225276004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01706638142556673\n",
      "        policy_loss: -0.005291709896697793\n",
      "        total_loss: 8.443918258400373\n",
      "        vf_explained_var: 0.0021325031916300456\n",
      "        vf_loss: 8.44613459238442\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 368000\n",
      "  num_agent_steps_trained: 368000\n",
      "  num_env_steps_sampled: 368000\n",
      "  num_env_steps_trained: 368000\n",
      "iterations_since_restore: 92\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 368000\n",
      "num_agent_steps_trained: 368000\n",
      "num_env_steps_sampled: 368000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 368000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.125\n",
      "  ram_util_percent: 48.625\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142302355703878\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020278583952897437\n",
      "  mean_inference_ms: 0.2331994365806559\n",
      "  mean_raw_obs_processing_ms: 0.03169524619947417\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.0\n",
      "  episode_reward_mean: 49.75\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 38\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 24\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 9\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 36.0\n",
      "    - 76.0\n",
      "    - 74.0\n",
      "    - 81.0\n",
      "    - 79.0\n",
      "    - 26.0\n",
      "    - 69.0\n",
      "    - 76.0\n",
      "    - 33.0\n",
      "    - 90.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 52.0\n",
      "    - 63.0\n",
      "    - 51.0\n",
      "    - 94.0\n",
      "    - 84.0\n",
      "    - 52.0\n",
      "    - 45.0\n",
      "    - 48.0\n",
      "    - 82.0\n",
      "    - 47.0\n",
      "    - 60.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 62.0\n",
      "    - 55.0\n",
      "    - 28.0\n",
      "    - 41.0\n",
      "    - 65.0\n",
      "    - 70.0\n",
      "    - 62.0\n",
      "    - 39.0\n",
      "    - 100.0\n",
      "    - 61.0\n",
      "    - 103.0\n",
      "    - 46.0\n",
      "    - 48.0\n",
      "    - 74.0\n",
      "    - 46.0\n",
      "    - 32.0\n",
      "    - 53.0\n",
      "    - 22.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 73.0\n",
      "    - 46.0\n",
      "    - 74.0\n",
      "    - 44.0\n",
      "    - 76.0\n",
      "    - 31.0\n",
      "    - 21.0\n",
      "    - 77.0\n",
      "    - 0.0\n",
      "    - 96.0\n",
      "    - 61.0\n",
      "    - 86.0\n",
      "    - 76.0\n",
      "    - 91.0\n",
      "    - 52.0\n",
      "    - 32.0\n",
      "    - 50.0\n",
      "    - 51.0\n",
      "    - 0.0\n",
      "    - 47.0\n",
      "    - 45.0\n",
      "    - 79.0\n",
      "    - 0.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 55.0\n",
      "    - 63.0\n",
      "    - 18.0\n",
      "    - 46.0\n",
      "    - 48.0\n",
      "    - 45.0\n",
      "    - 38.0\n",
      "    - 69.0\n",
      "    - 30.0\n",
      "    - 0.0\n",
      "    - 30.0\n",
      "    - 17.0\n",
      "    - 39.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 89.0\n",
      "    - 12.0\n",
      "    - 64.0\n",
      "    - 35.0\n",
      "    - 58.0\n",
      "    - 20.0\n",
      "    - 31.0\n",
      "    - 77.0\n",
      "    - 62.0\n",
      "    - 84.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142302355703878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020278583952897437\n",
      "    mean_inference_ms: 0.2331994365806559\n",
      "    mean_raw_obs_processing_ms: 0.03169524619947417\n",
      "time_since_restore: 243.42204785346985\n",
      "time_this_iter_s: 2.6368160247802734\n",
      "time_total_s: 243.42204785346985\n",
      "timers:\n",
      "  learn_throughput: 2829.048\n",
      "  learn_time_ms: 1413.903\n",
      "  load_throughput: 21690001.293\n",
      "  load_time_ms: 0.184\n",
      "  training_iteration_time_ms: 2647.765\n",
      "  update_time_ms: 0.725\n",
      "timestamp: 1656299627\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 368000\n",
      "training_iteration: 92\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 372000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 372000\n",
      "  num_agent_steps_trained: 372000\n",
      "  num_env_steps_sampled: 372000\n",
      "  num_env_steps_trained: 372000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-50\n",
      "done: false\n",
      "episode_len_mean: 46.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 89.0\n",
      "episode_reward_mean: 47.47\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 87\n",
      "episodes_total: 8694\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.973163116106423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015673674742430848\n",
      "        policy_loss: -0.005600669412242789\n",
      "        total_loss: 8.635436349786739\n",
      "        vf_explained_var: 0.002492828458868047\n",
      "        vf_loss: 8.638212594678325\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 372000\n",
      "  num_agent_steps_trained: 372000\n",
      "  num_env_steps_sampled: 372000\n",
      "  num_env_steps_trained: 372000\n",
      "iterations_since_restore: 93\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 372000\n",
      "num_agent_steps_trained: 372000\n",
      "num_env_steps_sampled: 372000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 372000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.4\n",
      "  ram_util_percent: 48.675000000000004\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142187927143662\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020274981757456855\n",
      "  mean_inference_ms: 0.23319176486839774\n",
      "  mean_raw_obs_processing_ms: 0.03169193868181369\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 89.0\n",
      "  episode_reward_mean: 47.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 87\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 28\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 45\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 20\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 10\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 41.0\n",
      "    - 89.0\n",
      "    - 12.0\n",
      "    - 64.0\n",
      "    - 35.0\n",
      "    - 58.0\n",
      "    - 20.0\n",
      "    - 31.0\n",
      "    - 77.0\n",
      "    - 62.0\n",
      "    - 84.0\n",
      "    - 86.0\n",
      "    - 78.0\n",
      "    - 4.0\n",
      "    - 58.0\n",
      "    - 35.0\n",
      "    - 41.0\n",
      "    - 64.0\n",
      "    - 33.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 62.0\n",
      "    - 65.0\n",
      "    - 77.0\n",
      "    - 2.0\n",
      "    - 37.0\n",
      "    - 69.0\n",
      "    - 73.0\n",
      "    - 36.0\n",
      "    - 45.0\n",
      "    - 80.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 67.0\n",
      "    - 46.0\n",
      "    - 37.0\n",
      "    - 36.0\n",
      "    - 39.0\n",
      "    - 73.0\n",
      "    - 62.0\n",
      "    - 28.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 50.0\n",
      "    - 66.0\n",
      "    - 45.0\n",
      "    - 70.0\n",
      "    - 19.0\n",
      "    - 2.0\n",
      "    - 84.0\n",
      "    - 66.0\n",
      "    - 84.0\n",
      "    - 31.0\n",
      "    - 79.0\n",
      "    - 16.0\n",
      "    - 44.0\n",
      "    - 56.0\n",
      "    - 71.0\n",
      "    - 0.0\n",
      "    - 40.0\n",
      "    - 52.0\n",
      "    - 88.0\n",
      "    - 72.0\n",
      "    - 57.0\n",
      "    - 77.0\n",
      "    - 30.0\n",
      "    - 58.0\n",
      "    - 73.0\n",
      "    - 34.0\n",
      "    - 74.0\n",
      "    - 0.0\n",
      "    - 84.0\n",
      "    - 30.0\n",
      "    - 72.0\n",
      "    - 17.0\n",
      "    - 41.0\n",
      "    - 77.0\n",
      "    - 75.0\n",
      "    - 70.0\n",
      "    - 72.0\n",
      "    - 50.0\n",
      "    - 20.0\n",
      "    - 73.0\n",
      "    - 34.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 60.0\n",
      "    - 48.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142187927143662\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020274981757456855\n",
      "    mean_inference_ms: 0.23319176486839774\n",
      "    mean_raw_obs_processing_ms: 0.03169193868181369\n",
      "time_since_restore: 246.04086446762085\n",
      "time_this_iter_s: 2.618816614151001\n",
      "time_total_s: 246.04086446762085\n",
      "timers:\n",
      "  learn_throughput: 2834.093\n",
      "  learn_time_ms: 1411.386\n",
      "  load_throughput: 21564544.987\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2644.778\n",
      "  update_time_ms: 0.711\n",
      "timestamp: 1656299630\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 372000\n",
      "training_iteration: 93\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 376000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 376000\n",
      "  num_agent_steps_trained: 376000\n",
      "  num_env_steps_sampled: 376000\n",
      "  num_env_steps_trained: 376000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-52\n",
      "done: false\n",
      "episode_len_mean: 46.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 99.0\n",
      "episode_reward_mean: 47.94\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 85\n",
      "episodes_total: 8779\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9783934038172486\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013510079537167825\n",
      "        policy_loss: 0.004131890869429035\n",
      "        total_loss: 8.427773752520162\n",
      "        vf_explained_var: 0.006270652496686546\n",
      "        vf_loss: 8.421207307487405\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 376000\n",
      "  num_agent_steps_trained: 376000\n",
      "  num_env_steps_sampled: 376000\n",
      "  num_env_steps_trained: 376000\n",
      "iterations_since_restore: 94\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 376000\n",
      "num_agent_steps_trained: 376000\n",
      "num_env_steps_sampled: 376000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 376000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.575\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141925196688062\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0202716987439937\n",
      "  mean_inference_ms: 0.2331819808306558\n",
      "  mean_raw_obs_processing_ms: 0.03168834030745404\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 99.0\n",
      "  episode_reward_mean: 47.94\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 85\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 17\n",
      "    - 50\n",
      "    - 11\n",
      "    - 50\n",
      "    - 50\n",
      "    - 23\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 48\n",
      "    - 12\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 42\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 14\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 26\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 72.0\n",
      "    - 50.0\n",
      "    - 20.0\n",
      "    - 73.0\n",
      "    - 34.0\n",
      "    - 36.0\n",
      "    - 0.0\n",
      "    - 61.0\n",
      "    - 0.0\n",
      "    - 26.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 60.0\n",
      "    - 48.0\n",
      "    - 73.0\n",
      "    - 19.0\n",
      "    - 65.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 40.0\n",
      "    - 0.0\n",
      "    - 31.0\n",
      "    - 58.0\n",
      "    - 49.0\n",
      "    - 50.0\n",
      "    - 31.0\n",
      "    - 85.0\n",
      "    - 72.0\n",
      "    - 13.0\n",
      "    - 69.0\n",
      "    - 81.0\n",
      "    - 38.0\n",
      "    - 99.0\n",
      "    - 90.0\n",
      "    - 94.0\n",
      "    - 58.0\n",
      "    - 64.0\n",
      "    - 60.0\n",
      "    - 68.0\n",
      "    - 47.0\n",
      "    - 50.0\n",
      "    - 74.0\n",
      "    - 34.0\n",
      "    - 44.0\n",
      "    - 58.0\n",
      "    - 39.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 39.0\n",
      "    - 67.0\n",
      "    - 33.0\n",
      "    - 50.0\n",
      "    - 57.0\n",
      "    - 64.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 18.0\n",
      "    - 58.0\n",
      "    - 35.0\n",
      "    - 67.0\n",
      "    - 61.0\n",
      "    - 19.0\n",
      "    - 58.0\n",
      "    - 41.0\n",
      "    - 60.0\n",
      "    - 55.0\n",
      "    - 41.0\n",
      "    - 51.0\n",
      "    - 84.0\n",
      "    - 66.0\n",
      "    - 77.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 72.0\n",
      "    - 68.0\n",
      "    - 24.0\n",
      "    - 73.0\n",
      "    - 48.0\n",
      "    - 35.0\n",
      "    - 34.0\n",
      "    - 59.0\n",
      "    - 7.0\n",
      "    - 66.0\n",
      "    - 32.0\n",
      "    - 46.0\n",
      "    - 56.0\n",
      "    - 60.0\n",
      "    - 68.0\n",
      "    - 85.0\n",
      "    - 62.0\n",
      "    - 62.0\n",
      "    - 9.0\n",
      "    - 45.0\n",
      "    - 42.0\n",
      "    - 64.0\n",
      "    - 56.0\n",
      "    - 90.0\n",
      "    - 43.0\n",
      "    - 65.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141925196688062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0202716987439937\n",
      "    mean_inference_ms: 0.2331819808306558\n",
      "    mean_raw_obs_processing_ms: 0.03168834030745404\n",
      "time_since_restore: 248.68213653564453\n",
      "time_this_iter_s: 2.6412720680236816\n",
      "time_total_s: 248.68213653564453\n",
      "timers:\n",
      "  learn_throughput: 2833.651\n",
      "  learn_time_ms: 1411.607\n",
      "  load_throughput: 21614552.95\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2644.994\n",
      "  update_time_ms: 0.707\n",
      "timestamp: 1656299632\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 376000\n",
      "training_iteration: 94\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 380000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 380000\n",
      "  num_agent_steps_trained: 380000\n",
      "  num_env_steps_sampled: 380000\n",
      "  num_env_steps_trained: 380000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-55\n",
      "done: false\n",
      "episode_len_mean: 47.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 108.0\n",
      "episode_reward_mean: 54.81\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 8865\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9766578938371392\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015835364419285968\n",
      "        policy_loss: -0.014615366481725246\n",
      "        total_loss: 8.63818941936698\n",
      "        vf_explained_var: 0.003974952684935703\n",
      "        vf_loss: 8.64995121289325\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 380000\n",
      "  num_agent_steps_trained: 380000\n",
      "  num_env_steps_sampled: 380000\n",
      "  num_env_steps_trained: 380000\n",
      "iterations_since_restore: 95\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 380000\n",
      "num_agent_steps_trained: 380000\n",
      "num_env_steps_sampled: 380000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 380000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.733333333333333\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141706050512458\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02026976426055346\n",
      "  mean_inference_ms: 0.2331749467177885\n",
      "  mean_raw_obs_processing_ms: 0.0316856186511125\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 47.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 108.0\n",
      "  episode_reward_mean: 54.81\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 27\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    episode_reward:\n",
      "    - 56.0\n",
      "    - 60.0\n",
      "    - 68.0\n",
      "    - 85.0\n",
      "    - 62.0\n",
      "    - 62.0\n",
      "    - 9.0\n",
      "    - 45.0\n",
      "    - 42.0\n",
      "    - 64.0\n",
      "    - 56.0\n",
      "    - 90.0\n",
      "    - 43.0\n",
      "    - 65.0\n",
      "    - 87.0\n",
      "    - 52.0\n",
      "    - 43.0\n",
      "    - 17.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 74.0\n",
      "    - 70.0\n",
      "    - 52.0\n",
      "    - 97.0\n",
      "    - 67.0\n",
      "    - 26.0\n",
      "    - 61.0\n",
      "    - 78.0\n",
      "    - 1.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 95.0\n",
      "    - 36.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 64.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 75.0\n",
      "    - 84.0\n",
      "    - 49.0\n",
      "    - 75.0\n",
      "    - 23.0\n",
      "    - 79.0\n",
      "    - 39.0\n",
      "    - 56.0\n",
      "    - 79.0\n",
      "    - 46.0\n",
      "    - 56.0\n",
      "    - 34.0\n",
      "    - 19.0\n",
      "    - 63.0\n",
      "    - 52.0\n",
      "    - 71.0\n",
      "    - 45.0\n",
      "    - 87.0\n",
      "    - 73.0\n",
      "    - 48.0\n",
      "    - 53.0\n",
      "    - 51.0\n",
      "    - 43.0\n",
      "    - 79.0\n",
      "    - 63.0\n",
      "    - 62.0\n",
      "    - 48.0\n",
      "    - 49.0\n",
      "    - 41.0\n",
      "    - 23.0\n",
      "    - 71.0\n",
      "    - 13.0\n",
      "    - 54.0\n",
      "    - 87.0\n",
      "    - 19.0\n",
      "    - 68.0\n",
      "    - 40.0\n",
      "    - 82.0\n",
      "    - 21.0\n",
      "    - 60.0\n",
      "    - 43.0\n",
      "    - 57.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 65.0\n",
      "    - 78.0\n",
      "    - 29.0\n",
      "    - 68.0\n",
      "    - 46.0\n",
      "    - 85.0\n",
      "    - 45.0\n",
      "    - 92.0\n",
      "    - 46.0\n",
      "    - 55.0\n",
      "    - 64.0\n",
      "    - 73.0\n",
      "    - 108.0\n",
      "    - 93.0\n",
      "    - 45.0\n",
      "    - 58.0\n",
      "    - 54.0\n",
      "    - 47.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141706050512458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02026976426055346\n",
      "    mean_inference_ms: 0.2331749467177885\n",
      "    mean_raw_obs_processing_ms: 0.0316856186511125\n",
      "time_since_restore: 251.329758644104\n",
      "time_this_iter_s: 2.6476221084594727\n",
      "time_total_s: 251.329758644104\n",
      "timers:\n",
      "  learn_throughput: 2834.853\n",
      "  learn_time_ms: 1411.008\n",
      "  load_throughput: 21636853.237\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 2644.656\n",
      "  update_time_ms: 0.704\n",
      "timestamp: 1656299635\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 380000\n",
      "training_iteration: 95\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 384000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 384000\n",
      "  num_agent_steps_trained: 384000\n",
      "  num_env_steps_sampled: 384000\n",
      "  num_env_steps_trained: 384000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-13-58\n",
      "done: false\n",
      "episode_len_mean: 48.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 108.0\n",
      "episode_reward_mean: 54.76\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 83\n",
      "episodes_total: 8948\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9713544413607608\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015412951248788064\n",
      "        policy_loss: -0.012889406097031408\n",
      "        total_loss: 8.490082706430908\n",
      "        vf_explained_var: 0.0028385909654760873\n",
      "        vf_loss: 8.500194645953435\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 384000\n",
      "  num_agent_steps_trained: 384000\n",
      "  num_env_steps_sampled: 384000\n",
      "  num_env_steps_trained: 384000\n",
      "iterations_since_restore: 96\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 384000\n",
      "num_agent_steps_trained: 384000\n",
      "num_env_steps_sampled: 384000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 384000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.575\n",
      "  ram_util_percent: 48.650000000000006\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015141416679685915\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020267665856669777\n",
      "  mean_inference_ms: 0.23316642092520326\n",
      "  mean_raw_obs_processing_ms: 0.03168161565095651\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 48.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 108.0\n",
      "  episode_reward_mean: 54.76\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 83\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 25\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    episode_reward:\n",
      "    - 78.0\n",
      "    - 29.0\n",
      "    - 68.0\n",
      "    - 46.0\n",
      "    - 85.0\n",
      "    - 45.0\n",
      "    - 92.0\n",
      "    - 46.0\n",
      "    - 55.0\n",
      "    - 64.0\n",
      "    - 73.0\n",
      "    - 108.0\n",
      "    - 93.0\n",
      "    - 45.0\n",
      "    - 58.0\n",
      "    - 54.0\n",
      "    - 47.0\n",
      "    - 78.0\n",
      "    - 79.0\n",
      "    - 42.0\n",
      "    - 67.0\n",
      "    - 68.0\n",
      "    - 58.0\n",
      "    - 16.0\n",
      "    - 84.0\n",
      "    - 75.0\n",
      "    - 46.0\n",
      "    - 58.0\n",
      "    - 61.0\n",
      "    - 34.0\n",
      "    - 39.0\n",
      "    - 5.0\n",
      "    - 96.0\n",
      "    - 101.0\n",
      "    - 34.0\n",
      "    - 35.0\n",
      "    - 87.0\n",
      "    - 17.0\n",
      "    - 84.0\n",
      "    - 63.0\n",
      "    - 42.0\n",
      "    - 49.0\n",
      "    - 52.0\n",
      "    - 57.0\n",
      "    - 53.0\n",
      "    - 66.0\n",
      "    - 41.0\n",
      "    - 52.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 24.0\n",
      "    - 49.0\n",
      "    - 61.0\n",
      "    - 35.0\n",
      "    - 69.0\n",
      "    - 82.0\n",
      "    - 35.0\n",
      "    - 74.0\n",
      "    - 56.0\n",
      "    - 84.0\n",
      "    - 40.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 25.0\n",
      "    - 83.0\n",
      "    - 74.0\n",
      "    - 93.0\n",
      "    - 26.0\n",
      "    - 57.0\n",
      "    - 68.0\n",
      "    - 25.0\n",
      "    - 0.0\n",
      "    - 74.0\n",
      "    - 45.0\n",
      "    - 82.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 13.0\n",
      "    - 56.0\n",
      "    - 100.0\n",
      "    - 49.0\n",
      "    - 0.0\n",
      "    - 57.0\n",
      "    - 59.0\n",
      "    - 52.0\n",
      "    - 86.0\n",
      "    - 49.0\n",
      "    - 68.0\n",
      "    - 78.0\n",
      "    - 76.0\n",
      "    - 88.0\n",
      "    - 64.0\n",
      "    - 20.0\n",
      "    - 34.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 91.0\n",
      "    - 69.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015141416679685915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020267665856669777\n",
      "    mean_inference_ms: 0.23316642092520326\n",
      "    mean_raw_obs_processing_ms: 0.03168161565095651\n",
      "time_since_restore: 253.96218180656433\n",
      "time_this_iter_s: 2.632423162460327\n",
      "time_total_s: 253.96218180656433\n",
      "timers:\n",
      "  learn_throughput: 2839.021\n",
      "  learn_time_ms: 1408.937\n",
      "  load_throughput: 21404970.656\n",
      "  load_time_ms: 0.187\n",
      "  training_iteration_time_ms: 2638.729\n",
      "  update_time_ms: 0.702\n",
      "timestamp: 1656299638\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 384000\n",
      "training_iteration: 96\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 388000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 388000\n",
      "  num_agent_steps_trained: 388000\n",
      "  num_env_steps_sampled: 388000\n",
      "  num_env_steps_trained: 388000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-14-00\n",
      "done: false\n",
      "episode_len_mean: 46.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 103.0\n",
      "episode_reward_mean: 50.49\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 9034\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9574449512907254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015002161823964168\n",
      "        policy_loss: -0.010714125142542906\n",
      "        total_loss: 8.435968349313223\n",
      "        vf_explained_var: 0.006171896893491027\n",
      "        vf_loss: 8.443979057188956\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 388000\n",
      "  num_agent_steps_trained: 388000\n",
      "  num_env_steps_sampled: 388000\n",
      "  num_env_steps_trained: 388000\n",
      "iterations_since_restore: 97\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 388000\n",
      "num_agent_steps_trained: 388000\n",
      "num_env_steps_sampled: 388000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 388000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.950000000000001\n",
      "  ram_util_percent: 48.675000000000004\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015140789393283702\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020264153818833147\n",
      "  mean_inference_ms: 0.23315991212491632\n",
      "  mean_raw_obs_processing_ms: 0.031677940801075256\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.0\n",
      "  episode_reward_mean: 50.49\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 22\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 33\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 49.0\n",
      "    - 68.0\n",
      "    - 78.0\n",
      "    - 76.0\n",
      "    - 88.0\n",
      "    - 64.0\n",
      "    - 20.0\n",
      "    - 34.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 91.0\n",
      "    - 69.0\n",
      "    - 56.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 31.0\n",
      "    - 34.0\n",
      "    - 44.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 48.0\n",
      "    - 62.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 0.0\n",
      "    - 67.0\n",
      "    - 26.0\n",
      "    - 53.0\n",
      "    - 58.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 62.0\n",
      "    - 12.0\n",
      "    - 37.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 59.0\n",
      "    - 76.0\n",
      "    - 63.0\n",
      "    - 53.0\n",
      "    - 74.0\n",
      "    - 15.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 13.0\n",
      "    - 44.0\n",
      "    - 90.0\n",
      "    - 71.0\n",
      "    - 92.0\n",
      "    - 103.0\n",
      "    - 26.0\n",
      "    - 79.0\n",
      "    - 58.0\n",
      "    - 44.0\n",
      "    - 89.0\n",
      "    - 69.0\n",
      "    - 81.0\n",
      "    - 38.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 45.0\n",
      "    - 67.0\n",
      "    - 37.0\n",
      "    - 53.0\n",
      "    - 41.0\n",
      "    - 31.0\n",
      "    - 60.0\n",
      "    - 68.0\n",
      "    - 71.0\n",
      "    - 53.0\n",
      "    - 93.0\n",
      "    - 79.0\n",
      "    - 41.0\n",
      "    - 33.0\n",
      "    - 100.0\n",
      "    - 45.0\n",
      "    - 67.0\n",
      "    - 48.0\n",
      "    - 20.0\n",
      "    - 59.0\n",
      "    - 58.0\n",
      "    - 74.0\n",
      "    - 64.0\n",
      "    - 64.0\n",
      "    - 41.0\n",
      "    - 80.0\n",
      "    - 84.0\n",
      "    - 69.0\n",
      "    - 74.0\n",
      "    - 91.0\n",
      "    - 24.0\n",
      "    - 10.0\n",
      "    - 59.0\n",
      "    - 42.0\n",
      "    - 50.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 50.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015140789393283702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020264153818833147\n",
      "    mean_inference_ms: 0.23315991212491632\n",
      "    mean_raw_obs_processing_ms: 0.031677940801075256\n",
      "time_since_restore: 256.5927119255066\n",
      "time_this_iter_s: 2.6305301189422607\n",
      "time_total_s: 256.5927119255066\n",
      "timers:\n",
      "  learn_throughput: 2838.671\n",
      "  learn_time_ms: 1409.11\n",
      "  load_throughput: 21280081.177\n",
      "  load_time_ms: 0.188\n",
      "  training_iteration_time_ms: 2638.374\n",
      "  update_time_ms: 0.658\n",
      "timestamp: 1656299640\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 388000\n",
      "training_iteration: 97\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 392000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 392000\n",
      "  num_agent_steps_trained: 392000\n",
      "  num_env_steps_sampled: 392000\n",
      "  num_env_steps_trained: 392000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-14-03\n",
      "done: false\n",
      "episode_len_mean: 46.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 95.0\n",
      "episode_reward_mean: 51.81\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 9120\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9715839281517972\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014012139630554024\n",
      "        policy_loss: -0.016857032201463177\n",
      "        total_loss: 8.626473961081556\n",
      "        vf_explained_var: 0.0034263480735081497\n",
      "        vf_loss: 8.640805943806965\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 392000\n",
      "  num_agent_steps_trained: 392000\n",
      "  num_env_steps_sampled: 392000\n",
      "  num_env_steps_trained: 392000\n",
      "iterations_since_restore: 98\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 392000\n",
      "num_agent_steps_trained: 392000\n",
      "num_env_steps_sampled: 392000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 392000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.25\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015140243147686689\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020261072062696527\n",
      "  mean_inference_ms: 0.23315864483920523\n",
      "  mean_raw_obs_processing_ms: 0.03167438890286248\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.0\n",
      "  episode_reward_mean: 51.81\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 16\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 18\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 29\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 32\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 80.0\n",
      "    - 84.0\n",
      "    - 69.0\n",
      "    - 74.0\n",
      "    - 91.0\n",
      "    - 24.0\n",
      "    - 10.0\n",
      "    - 59.0\n",
      "    - 42.0\n",
      "    - 50.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 71.0\n",
      "    - 50.0\n",
      "    - 76.0\n",
      "    - 69.0\n",
      "    - 33.0\n",
      "    - 87.0\n",
      "    - 57.0\n",
      "    - 73.0\n",
      "    - 88.0\n",
      "    - 58.0\n",
      "    - 35.0\n",
      "    - 78.0\n",
      "    - 59.0\n",
      "    - 72.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 75.0\n",
      "    - 35.0\n",
      "    - 0.0\n",
      "    - 3.0\n",
      "    - 43.0\n",
      "    - 81.0\n",
      "    - 50.0\n",
      "    - 29.0\n",
      "    - 51.0\n",
      "    - 72.0\n",
      "    - 89.0\n",
      "    - 61.0\n",
      "    - 69.0\n",
      "    - 4.0\n",
      "    - 86.0\n",
      "    - 56.0\n",
      "    - 65.0\n",
      "    - 47.0\n",
      "    - 59.0\n",
      "    - 0.0\n",
      "    - 23.0\n",
      "    - 50.0\n",
      "    - 73.0\n",
      "    - 95.0\n",
      "    - 45.0\n",
      "    - 84.0\n",
      "    - 81.0\n",
      "    - 64.0\n",
      "    - 1.0\n",
      "    - 84.0\n",
      "    - 55.0\n",
      "    - 0.0\n",
      "    - 46.0\n",
      "    - 87.0\n",
      "    - 78.0\n",
      "    - 44.0\n",
      "    - 42.0\n",
      "    - 54.0\n",
      "    - 0.0\n",
      "    - 52.0\n",
      "    - 62.0\n",
      "    - 46.0\n",
      "    - 57.0\n",
      "    - 59.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 45.0\n",
      "    - 48.0\n",
      "    - 27.0\n",
      "    - 24.0\n",
      "    - 67.0\n",
      "    - 85.0\n",
      "    - 1.0\n",
      "    - 0.0\n",
      "    - 58.0\n",
      "    - 58.0\n",
      "    - 60.0\n",
      "    - 78.0\n",
      "    - 35.0\n",
      "    - 47.0\n",
      "    - 80.0\n",
      "    - 67.0\n",
      "    - 38.0\n",
      "    - 17.0\n",
      "    - 73.0\n",
      "    - 67.0\n",
      "    - 71.0\n",
      "    - 60.0\n",
      "    - 71.0\n",
      "    - 20.0\n",
      "    - 35.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015140243147686689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020261072062696527\n",
      "    mean_inference_ms: 0.23315864483920523\n",
      "    mean_raw_obs_processing_ms: 0.03167438890286248\n",
      "time_since_restore: 259.22216796875\n",
      "time_this_iter_s: 2.629456043243408\n",
      "time_total_s: 259.22216796875\n",
      "timers:\n",
      "  learn_throughput: 2843.527\n",
      "  learn_time_ms: 1406.704\n",
      "  load_throughput: 23298452.993\n",
      "  load_time_ms: 0.172\n",
      "  training_iteration_time_ms: 2636.102\n",
      "  update_time_ms: 0.669\n",
      "timestamp: 1656299643\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 392000\n",
      "training_iteration: 98\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 396000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 396000\n",
      "  num_agent_steps_trained: 396000\n",
      "  num_env_steps_sampled: 396000\n",
      "  num_env_steps_trained: 396000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-14-06\n",
      "done: false\n",
      "episode_len_mean: 45.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 113.0\n",
      "episode_reward_mean: 52.08\n",
      "episode_reward_min: -32.0\n",
      "episodes_this_iter: 88\n",
      "episodes_total: 9208\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9605522856917432\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015124890407527326\n",
      "        policy_loss: -0.01822771763388989\n",
      "        total_loss: 8.809308243823308\n",
      "        vf_explained_var: 0.003945902406528432\n",
      "        vf_loss: 8.82481039775315\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 396000\n",
      "  num_agent_steps_trained: 396000\n",
      "  num_env_steps_sampled: 396000\n",
      "  num_env_steps_trained: 396000\n",
      "iterations_since_restore: 99\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 396000\n",
      "num_agent_steps_trained: 396000\n",
      "num_env_steps_sampled: 396000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 396000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.599999999999998\n",
      "  ram_util_percent: 48.70000000000001\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142451413409452\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02026130959288674\n",
      "  mean_inference_ms: 0.23318956409692482\n",
      "  mean_raw_obs_processing_ms: 0.03167844857274406\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 113.0\n",
      "  episode_reward_mean: 52.08\n",
      "  episode_reward_min: -32.0\n",
      "  episodes_this_iter: 88\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 36\n",
      "    - 30\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 35\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 19\n",
      "    - 43\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 50\n",
      "    - 50\n",
      "    - 47\n",
      "    - 39\n",
      "    - 13\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 48\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 8\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 26\n",
      "    - 27\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 37\n",
      "    - 50\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 19\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 47.0\n",
      "    - 80.0\n",
      "    - 67.0\n",
      "    - 38.0\n",
      "    - 17.0\n",
      "    - 73.0\n",
      "    - 67.0\n",
      "    - 71.0\n",
      "    - 60.0\n",
      "    - 71.0\n",
      "    - 20.0\n",
      "    - 35.0\n",
      "    - 19.0\n",
      "    - 60.0\n",
      "    - 85.0\n",
      "    - 73.0\n",
      "    - 21.0\n",
      "    - 9.0\n",
      "    - 63.0\n",
      "    - 48.0\n",
      "    - 46.0\n",
      "    - 90.0\n",
      "    - 67.0\n",
      "    - 73.0\n",
      "    - 45.0\n",
      "    - 71.0\n",
      "    - 63.0\n",
      "    - 48.0\n",
      "    - 42.0\n",
      "    - 0.0\n",
      "    - 77.0\n",
      "    - 67.0\n",
      "    - 33.0\n",
      "    - 41.0\n",
      "    - 0.0\n",
      "    - 62.0\n",
      "    - 74.0\n",
      "    - 75.0\n",
      "    - 86.0\n",
      "    - 14.0\n",
      "    - 53.0\n",
      "    - 31.0\n",
      "    - 50.0\n",
      "    - 0.0\n",
      "    - 70.0\n",
      "    - 60.0\n",
      "    - 43.0\n",
      "    - 42.0\n",
      "    - 58.0\n",
      "    - 67.0\n",
      "    - 0.0\n",
      "    - 60.0\n",
      "    - 0.0\n",
      "    - 68.0\n",
      "    - 34.0\n",
      "    - 0.0\n",
      "    - 81.0\n",
      "    - 47.0\n",
      "    - 56.0\n",
      "    - 65.0\n",
      "    - 88.0\n",
      "    - 34.0\n",
      "    - 113.0\n",
      "    - 41.0\n",
      "    - 34.0\n",
      "    - 88.0\n",
      "    - 68.0\n",
      "    - 22.0\n",
      "    - 109.0\n",
      "    - 93.0\n",
      "    - 0.0\n",
      "    - 36.0\n",
      "    - 60.0\n",
      "    - 72.0\n",
      "    - 58.0\n",
      "    - 82.0\n",
      "    - 75.0\n",
      "    - -32.0\n",
      "    - 69.0\n",
      "    - 63.0\n",
      "    - 77.0\n",
      "    - 74.0\n",
      "    - 69.0\n",
      "    - 66.0\n",
      "    - 28.0\n",
      "    - 52.0\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 19.0\n",
      "    - 59.0\n",
      "    - 83.0\n",
      "    - 35.0\n",
      "    - 55.0\n",
      "    - 76.0\n",
      "    - 55.0\n",
      "    - 37.0\n",
      "    - 39.0\n",
      "    - 24.0\n",
      "    - 61.0\n",
      "    - 56.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142451413409452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02026130959288674\n",
      "    mean_inference_ms: 0.23318956409692482\n",
      "    mean_raw_obs_processing_ms: 0.03167844857274406\n",
      "time_since_restore: 261.9145348072052\n",
      "time_this_iter_s: 2.6923668384552\n",
      "time_total_s: 261.9145348072052\n",
      "timers:\n",
      "  learn_throughput: 2836.032\n",
      "  learn_time_ms: 1410.421\n",
      "  load_throughput: 23438412.965\n",
      "  load_time_ms: 0.171\n",
      "  training_iteration_time_ms: 2641.348\n",
      "  update_time_ms: 0.655\n",
      "timestamp: 1656299646\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 396000\n",
      "training_iteration: 99\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n",
      "agent_timesteps_total: 400000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 400000\n",
      "  num_agent_steps_trained: 400000\n",
      "  num_env_steps_sampled: 400000\n",
      "  num_env_steps_trained: 400000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-26_22-14-08\n",
      "done: false\n",
      "episode_len_mean: 46.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 102.0\n",
      "episode_reward_mean: 51.03\n",
      "episode_reward_min: 0.0\n",
      "episodes_this_iter: 86\n",
      "episodes_total: 9294\n",
      "experiment_id: 049eaa19908543d188202c947a809c98\n",
      "hostname: Tunas-MBP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.18020324707031246\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.9585850990587665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01472253852463778\n",
      "        policy_loss: -0.012160447245884326\n",
      "        total_loss: 8.694781667442731\n",
      "        vf_explained_var: 0.0032140985611946353\n",
      "        vf_loss: 8.704289097939768\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 400000\n",
      "  num_agent_steps_trained: 400000\n",
      "  num_env_steps_sampled: 400000\n",
      "  num_env_steps_trained: 400000\n",
      "iterations_since_restore: 100\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 400000\n",
      "num_agent_steps_trained: 400000\n",
      "num_env_steps_sampled: 400000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 400000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.35\n",
      "  ram_util_percent: 48.6\n",
      "pid: 25686\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.015142974091531576\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.020259389250042198\n",
      "  mean_inference_ms: 0.23319644865185968\n",
      "  mean_raw_obs_processing_ms: 0.03167691618347987\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 46.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 102.0\n",
      "  episode_reward_mean: 51.03\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 86\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 15\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 26\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 46\n",
      "    - 50\n",
      "    - 40\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 34\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 5\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 41\n",
      "    - 38\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 31\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 6\n",
      "    - 50\n",
      "    - 44\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 7\n",
      "    - 6\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    - 50\n",
      "    episode_reward:\n",
      "    - 0.0\n",
      "    - 87.0\n",
      "    - 19.0\n",
      "    - 59.0\n",
      "    - 83.0\n",
      "    - 35.0\n",
      "    - 55.0\n",
      "    - 76.0\n",
      "    - 55.0\n",
      "    - 37.0\n",
      "    - 39.0\n",
      "    - 24.0\n",
      "    - 61.0\n",
      "    - 56.0\n",
      "    - 61.0\n",
      "    - 64.0\n",
      "    - 54.0\n",
      "    - 1.0\n",
      "    - 40.0\n",
      "    - 59.0\n",
      "    - 76.0\n",
      "    - 59.0\n",
      "    - 73.0\n",
      "    - 76.0\n",
      "    - 36.0\n",
      "    - 3.0\n",
      "    - 65.0\n",
      "    - 56.0\n",
      "    - 72.0\n",
      "    - 63.0\n",
      "    - 68.0\n",
      "    - 69.0\n",
      "    - 53.0\n",
      "    - 41.0\n",
      "    - 24.0\n",
      "    - 68.0\n",
      "    - 55.0\n",
      "    - 58.0\n",
      "    - 33.0\n",
      "    - 102.0\n",
      "    - 57.0\n",
      "    - 38.0\n",
      "    - 39.0\n",
      "    - 23.0\n",
      "    - 78.0\n",
      "    - 76.0\n",
      "    - 62.0\n",
      "    - 52.0\n",
      "    - 2.0\n",
      "    - 58.0\n",
      "    - 78.0\n",
      "    - 78.0\n",
      "    - 66.0\n",
      "    - 0.0\n",
      "    - 38.0\n",
      "    - 60.0\n",
      "    - 58.0\n",
      "    - 62.0\n",
      "    - 24.0\n",
      "    - 18.0\n",
      "    - 47.0\n",
      "    - 83.0\n",
      "    - 53.0\n",
      "    - 65.0\n",
      "    - 57.0\n",
      "    - 41.0\n",
      "    - 70.0\n",
      "    - 46.0\n",
      "    - 41.0\n",
      "    - 73.0\n",
      "    - 68.0\n",
      "    - 52.0\n",
      "    - 20.0\n",
      "    - 56.0\n",
      "    - 82.0\n",
      "    - 62.0\n",
      "    - 43.0\n",
      "    - 56.0\n",
      "    - 62.0\n",
      "    - 80.0\n",
      "    - 25.0\n",
      "    - 41.0\n",
      "    - 67.0\n",
      "    - 61.0\n",
      "    - 55.0\n",
      "    - 76.0\n",
      "    - 45.0\n",
      "    - 0.0\n",
      "    - 54.0\n",
      "    - 25.0\n",
      "    - 54.0\n",
      "    - 79.0\n",
      "    - 27.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 100.0\n",
      "    - 38.0\n",
      "    - 43.0\n",
      "    - 25.0\n",
      "    - 49.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.015142974091531576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.020259389250042198\n",
      "    mean_inference_ms: 0.23319644865185968\n",
      "    mean_raw_obs_processing_ms: 0.03167691618347987\n",
      "time_since_restore: 264.5954518318176\n",
      "time_this_iter_s: 2.6809170246124268\n",
      "time_total_s: 264.5954518318176\n",
      "timers:\n",
      "  learn_throughput: 2831.742\n",
      "  learn_time_ms: 1412.558\n",
      "  load_throughput: 23481058.083\n",
      "  load_time_ms: 0.17\n",
      "  training_iteration_time_ms: 2643.644\n",
      "  update_time_ms: 0.641\n",
      "timestamp: 1656299648\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400000\n",
      "training_iteration: 100\n",
      "trial_id: default\n",
      "warmup_time: 1.2516570091247559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "import ray\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    \"num_workers\": 1,\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    \"model_path\": r\"data/2d_stacked.csv\",\n",
    "    \"available_pipe\": 50,\n",
    "    \"delim\": \",\",\n",
    "}\n",
    "\n",
    "config[\"env_config\"] = env_config\n",
    "\n",
    "try:\n",
    "    ray.init()\n",
    "except RuntimeError:\n",
    "    ray.shutdown()\n",
    "    ray.init()\n",
    "\n",
    "agent = PPOTrainer(config=config, env=SimpleDriller)\n",
    "\n",
    "for i in range(100):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        checkpoint = agent.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see tensorboard run this from CLI.\n",
    "\n",
    "Note Ray needs `grpcio`. On new Macs this has to be installed from conda or be built from scratch for M1 silicon.\n",
    "\n",
    "The pip version doesn't work, and ray crashes.\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir ~/ray_results/\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 22:14:08,886\tWARNING deprecation.py:46 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.0\n"
     ]
    }
   ],
   "source": [
    "# instantiate env class\n",
    "env = SimpleDriller(env_config)\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "trajectory = env.trajectory\n",
    "print(episode_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x15f7acee0>]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1440 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(env.trajectory, columns=[\"rows\", \"colums\"])\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.grid()\n",
    "plt.plot(df[\"colums\"], df[\"rows\"], \"-\", c=\"m\", linewidth=6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "plt.close(\"all\")\n",
    "\n",
    "fig = plt.figure(facecolor=\"white\")\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "plt.pcolormesh(\n",
    "    env.model, vmin=-10, vmax=2, antialiased=True, shading=\"auto\", edgecolors=\"darkgray\"\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "borehole = ax.plot(df[\"colums\"][0], df[\"rows\"][0], ls=\"-\", c=\"m\", lw=6, zorder=10)\n",
    "bit = ax.scatter(df[\"colums\"][0], df[\"rows\"][0], marker=\"s\", s=100, c=\"k\", zorder=20)\n",
    "\n",
    "\n",
    "def animate_drill(i):\n",
    "    borehole[0].set_data(df[\"colums\"][:i], df[\"rows\"][:i])\n",
    "    bit.set_offsets([df[\"colums\"][i], df[\"rows\"][i]])\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, frames=50, func=animate_drill)\n",
    "\n",
    "FFwriter = animation.FFMpegWriter(fps=10, bitrate=500)\n",
    "anim.save(\"animation.mp4\", writer=FFwriter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}